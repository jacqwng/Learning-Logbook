---
title: "Tree phenology Analysis with R"
author: "Jacqueline Wingen"
date: "`r format(Sys.Date(), '%d %B %Y')`"
site: bookdown::bookdown_site
---

# Introduction

Hi, my name is Jacqueline. I’m a master’s student in Crop Sciences at the [University of Bonn](https://www.uni-bonn.de/de). This is my learning logbook for the module “Tree phenology analysis with R”. This module provides an overview of methods to study the impact of climate and climate change on tree phenology. It is designed for those who may not yet be familiar with phenology or how to analyze climate change effects, but it also aims to offer new insights for those with existing knowledge in these areas. Initially developed for M.Sc. students in Crop Science and Agricultural Science and Resource Management in the Tropics and Subtropics (ARTS) at the University of Bonn, the material is accessible to anyone interested.

The content begins with an introduction to phenology (with a special emphasis on dormancy) as well as an overview of climate change. It then focuses heavily on the practical application of the `chillR` package for R. This tool has been continuously developed since 2013 by [Prof. Dr. Eike Lüdeling](https://www.gartenbauwissenschaft.uni-bonn.de/author/prof.-dr.-eike-luedeling/), head of the HortiBonn research group at the [Institute of Crop Science and Resource Conservation (INRES)](https://www.inres.uni-bonn.de/de) at the University of Bonn, to support this type of analysis.

## Learning goals

This course will offer the following skills and experiences:

-   Knowledge about phenology

-   Knowledge about tree dormancy

-   Understanding of climate change impact projection methods

-   Appreciation for the importance of risks and uncertainty in climate change projection

-   Understanding of how to use some staple tools of R code development

-   Ability to use `chillR` functions for climate change impact projection

-   Ability to use `chillR` functions for tree phenology analysis

-   Understanding and ability to use the PhenoFlex dormancy analysis framework

<!--chapter:end:index.Rmd-->

# Tools

This course is designed to provide knowledge about tree phenology, climate change, and related topics, along with hands-on exercises to demonstrate the functionalities of the `chillR` package. It is recommended to document everything learned in a `learning logbook`. To engage in these practical components effectively, various tools are required. Since `chillR` is an [R](https://www.r-project.org/) package, using R, preferably through the [RStudio](https://posit.co/download/rstudio-desktop/) interface, will be necessary.

Although it is possible to run RStudio on a local computer and save files directly on the hard drive, this approach differs from the methods commonly used by professional programmers. To align with standard programming practices, familiarity with certain code development tools is essential. This course will therefore cover the basics of using [Git](https://git-scm.com/) and [GitHub](https://github.com/), which are valuable tools for organizing, securing, and sharing code. Additionally, proper documentation techniques in R will be introduced, focusing on creating well-structured, professional reports using [RMarkdown](https://rmarkdown.rstudio.com/). While these tools may seem complex at first, their usefulness is likely to become clearer as they are used throughout the module.

[Dr. Cory Whitney](https://inresgb-lehre.iaas.uni-bonn.de/author/dr.-cory-whitney/), a researcher at HortiBonn, has volunteered to create tutorial videos to provide an introduction to these tools.

## R and RStudio

The first video *Using R and RStudio* demonstrates how to install and run [R](https://www.r-project.org/) and [RStudio](https://posit.co/download/rstudio-desktop/):

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/WT3tKlzCZgo")
```

## Git and Github

The next video Using *Git and Github* explores the programming version control environment [Git](https://git-scm.com/) and the interface [GitHub](https://github.com/), which is used to access these features:

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/S98XJTyIVaY")
```

## Rmarkdown

In the last video *Using R Mardown*, [R Markdown](https://rmarkdown.rstudio.com/) will be examined, a powerful tool that enables the creation of sophisticated reports, websites, and more from R code:

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/hh4wyP8tvkI")
```


<!--chapter:end:02-tools.Rmd-->

# Tree dormancy

This chapter is presented by [Dr. Erica Fadón](https://scholar.google.de/citations?hl=de&user=MTmTnnsAAAAJ), a researcher at [HortiBonn from 2018 to 2021](https://inresgb-lehre.iaas.uni-bonn.de/author/dr.-erica-fadon-adrian/), who explores dormancy in temperate fruit trees. This topic remains complex and has many unanswered questions. A central question for researchers is, “How do trees know when to flower?” Although it seems clear that trees bloom in spring, the reality is more complicated. This chapter provides a better understanding of dormancy and demonstrates how to use the `chillR` tool to predict flowering times, even in the face of challenges posed by global warming.

## Learning goals

-   Learn about dormancy in temperate fruit trees
-   Be able to identify the phenological stages of a fruit tree and understand phenology data sets
-   Describe and compare the two methodologies (empirical and statistical) to identify the chilling and forcing periods of a certain cultivar

## Introduction to dormancy

Tree dormancy is a state of reduced activity that occurs when environmental conditions are unfavorable, especially during winter. This state acts as a survival strategy, helping trees withstand extreme temperatures, water shortages, and other stress factors. During dormancy, trees slow down or stop their growth to conserve energy and avoid damage. Dormancy is a continuous process divided into three phases:

-   **Dormancy Establishment**
-   **Endo-Dormancy**
-   **Eco-Dormancy**

Dormancy establishment is the process where temperate trees transition from active growth in summer to a dormant state in autumn. This shift is mainly triggered by shorter daylight hours and decreasing temperatures, causing buds to form, growth to stop, and leaves to fall. The importance of these factors varies by species, with some trees responding more to day length and others to temperature.

Endo-dormancy is a phase of dormancy controlled by the plant's internal factors, where growth is suppressed even under favorable conditions. It requires a period of cold exposure (chilling) to release the buds from this state, preventing premature growth during temporary warm spells in winter. Low temperatures are the main trigger for breaking endo-dormancy, while the role of light (photoperiod) is still uncertain.

Eco-dormancy is the phase after endo-dormancy, where buds have regained their ability to grow but remain inactive due to unfavorable environmental conditions, mainly low temperatures. During this phase, growth is on hold until warmer temperatures trigger it. Heat accumulation is needed to resume growth. Eco-dormancy ends when enough heat has been accumulated, leading to visible growth changes, typically in late winter or early spring.

The below video *Introduction to dormancy* by [Dr. Erica Fadón](https://scholar.google.de/citations?hl=de&user=MTmTnnsAAAAJ) gives the basic knowledge of this dormancy phases and processes that regulate dormancy.

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/qh9AZDmOm3o")
```

## Dormancy physiology

Dormancy as a whole is the result of complex interactions between numerous physiological processes that occur in different parts of the tree, such as buds, twigs, meristems, and vascular tissues. We divide these processes into four main themes:

-   **Transport:** occurs at both the whole-plant and cellular levels
-   **Phytohormone Dynamics:** behavior and levels of plant hormones during dormancy
-   **Genetic and Epigenetic Regulation:** how genetic factors and their modifications influence dormancy
-   **Dynamics of Nonstructural Carbohydrates:** changes in carbohydrate levels that affect dormancy

The following figure from the study ["A conceptual framework for Winter Dormancy in Deciduous Trees"](https://www.mdpi.com/2073-4395/10/2/241#) by Fadón et al. (2015) presents a conceptual framework of winter dormancy in deciduous trees and summarizes the three dormancy phases along with their respective physiological processes.

![Conceptual framework of winter dormancy in deciduous trees. The dormancy framework (gray background) indicates three main phases: (a) dormancy establishment (light gray), (b) endo-dormancy (dark gray), and (c) eco-dormancy (medium gray). For each phase (a–c), the dormancy-related physiological processes are represented by colored shapes and numbers (0 to 4).](images/framework.png)

All the processes depicted are explained in detail in the video below, *Dormancy Physiology* by [Dr. Erica Fadón](https://scholar.google.de/citations?hl=de&user=MTmTnnsAAAAJ).

```{r,echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/HriLSz77QEQ")
```

## Experimental and statistical determination of chilling and forcing periods

Dormancy consists of two phases where temperatures have opposite effects on flowering. During endodormancy, higher chill accumulation leads to earlier flowering, whereas similar cool temperatures during ecodormancy can delay flowering. The challenge lies in differentiating between these two phases, as the tree buds appear to be in the same developmental stage throughout. To address this, there are two methods available:

-   **Experimental method:** collecting buds periodically during winter, exposing them to favorable growth conditions, and evaluating bud break to determine when dormancy is overcome

-   **Statistical method:** uses long-term phenological data and temperature records to estimate the dates of chilling fulfillment and heat accumulation through partial least squares (PLS) regression analysis

The video *Dormancy determination* covers the experimental and statistical methods to determine the chilling and forcing periods for temperate fruit trees to overcome dormancy and initiate growth. It explains the concept of dormancy, its phases (endodormancy and ecodormancy), and the temperature requirements for breaking dormancy.

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/hMM27ktlzBM")
```

## Phenology record and BBCH scale

Phenology is the study of periodic events in biological life cycles and how these are influenced by seasonal and interannual variations in climate. This module will involve working with phenology data sets, primarily focusing on a specific stage, usually budbreak, even though trees pass through various developmental stages during the year. These stages are typically identified by numerical codes.

To describe these growth stages systematically, the BBCH scale is employed. This internationally standardized system outlines the growth and developmental phases of plants. The BBCH scale consists of ten main stages, known as principal growth stages, which are numbered from 0 to 9. Each of these main stages is further divided into substages, enabling a more detailed description of a plant's development.

Principal growth stages:

| Stage | Description |
|----|----|
| 0 | Germination / sprouting / bud development |
| 1 | Leaf development (main shoot) |
| 2 | Formation of side shoots / tillering |
| 3 | Stem elongation or rosette growth / shoot development (main shoot) |
| 4 | Development of harvestable vegetative plant parts or vegetatively propagated organs / booting (main shoot) |
| 5 | Inflorescence emergence |
| 6 | Flowering (main shoot) |
| 7 | Development of fruit |
| 8 | Ripening or maturity of fruit and seed |
| 9 | Senescence, beginning of dormancy |

For a comprehensive overview of phenology and the BBCH scale, the video *Phenology* by [Dr. Erica Fadón](https://scholar.google.de/citations?hl=de&user=MTmTnnsAAAAJ) is recommended. In this video, Dr. Fadón explains the concept of phenology and how the BBCH scale uses numerical codes to represent the different developmental stages of trees, from budding and flowering to fruit ripening and leaf fall.

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/Ssoe6Ahv88Y")
```

## `Excercises` on tree dormancy

1.  *Put yourself in the place of a breeder who wants to calculate the temperature requirements of a newly released cultivar. Which method will you use to calculate the chilling and forcing periods? Please justify your answer.*

As a breeder aiming to calculate the temperature requirements for the chilling and forcing periods of a newly released cultivar, I would use the experimental method to determine the chilling and forcing periods. Here's my justification:

-   **Direct measurement of bud response:** The experimental method allows me to directly observe when buds break under controlled temperature conditions. By regularly collecting buds during winter and placing them in ideal growth conditions, I can determine exactly when dormancy ends. This practical approach gives me quick and useful information about the specific cultivar

-   **Specific to the cultivar:** Each cultivar has its own unique chilling and forcing needs. The experimental method looks at the specific traits of the new cultivar, making sure the results are relevant and applicable to that variety

-   **Immediate results for breeding decisions:** The experimental method provides quick evaluations of bud break, allowing me to make faster decisions about breeding and managing the new cultivar

2.  *Which are the advantages (2) of the BBCH scale compared with earlier scales?*

-   **Standardization:** The BBCH scale provides a standardized framework for describing plant growth stages, enabling consistent communication and comparisons across studies
-   **Detailed Staging**: It offers a more granular categorization of developmental stages using a two-digit system, allowing for a better understanding of plant development and environmental impacts.

3.  *Classify the following phenological stages of sweet cherry according to the BBCH scale:*

![](images/pheno_stages.png)

-   **left image:** BBCH stage 55 (single flower buds visible (still closed), green scales slightly open)
-   **middle image:** BBCH stage 65 (full flowering: at least 50% of flowers open, first petals falling)
-   **right image:** BBCH stage 89 (fruit ripe for harvesting)

<!--chapter:end:03-tree-dormancy.Rmd-->

# Climate change and impact projection

Before using `chillR`, there's a brief overview of climate change, because the upcoming work will mainly focus on predicting how global warming might affect phenology-related metrics.

Climate change refers to long-term changes in temperatures and weather patterns. While these changes can occur naturally — such as fluctuations in solar activity — since the 19th century, climate change has primarily been driven by human activities, particularly the burning of fossil fuels like coal, oil, and natural gas.

## The drivers of climate change

To understand what's happening to our planet, it's important to know the main causes of climate change. This helps us spot false claims that things like the sun, cities, or natural changes in the climate are the main reasons for global warming. The truth is simple: human-made greenhouse gas emissions are heating up our planet, and the only way to stop this is to greatly reduce these emissions.

The video below, titled *Climate Change 1 - Drivers of Climate Change*, is the first in a series of four videos on the topic of climate change presented by [Eike Lüdeling](https://www.gartenbauwissenschaft.uni-bonn.de/author/prof.-dr.-eike-luedeling/). It provides a comprehensive overview of the primary drivers of global climate change, such as greenhouse gases, aerosols, solar radiation, ozone, and others.

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/lFtc-Y5OYNs")
```

## What's already known

The next video, *Climate Change 2 - Recent Warming*, explores climatic changes that have already occurred or for which there is substantial evidence. It demonstrates that the planet has experienced significant warming for several decades, almost globally.

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/sLmfKcvsWow")
```

## Future scenarios

When it comes to climate change, the most severe impacts are still ahead. This is largely due to the significantly higher rate of greenhouse gas emissions observed over the past few decades, with no signs of a slowdown in the near future. As a result, the human-induced 'forcing' effect on our climate has reached unprecedented levels, making it likely that future changes will occur even more rapidly than those we have already witnessed. The next video *Climate Change 3 - Future scenarios* introduces the methods that climate scientists employ to forecast future conditions and presents climate scenarios developed by these scientists, which researchers in other fields can use to project the impacts of climate change on ecological and agricultural systems.

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/PX6fAxBEkCE")
```

## Impact projections approaches

Having robust climate scenarios is essential, but they only take us partway toward reliable assessments of climate change impacts. A potentially greater challenge lies in translating these climate scenarios into biological consequences. To achieve this, we need impact models or other methods to derive the impacts of climate change. The last video *Climate change 4 - impact projection approaches* introduces various methods for projecting climate impacts.

```{r, echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/3Q8HF4E7rkM")
```

## `Exercises` on climate change

1.  *List the main drivers of climate change at the decade to century scale, and briefly explain the mechanism through which the currently most important driver affects our climate.*

The main drivers of climate change on a decade-to-century scale include:

-   **Greenhouse Gases (GHGs):** GHGs like carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N₂O) trap heat in the atmosphere, leading to the greenhouse effect, which raises Earth's temperature. The increase in these gases is primarily due to human activities, such as burning fossil fuels, industrial processes, and deforestation

-   **Aerosols:** Particles in the atmosphere that can cool the climate by reflecting sunlight. They come from both natural sources (e.g. sea salt, dust, volcanic eruptions, fires) and human activities (e.g.power plants, cars, fires and cook stove). They are major climate driver in industrial centers (e.g. China)

-   **Sun:** Solar radiation heats the Earth, with minor fluctuations occurring over time due to cycles in solar activity, such as sunspots. Although these variations contribute only a small portion to the current climate changes, they play a significant role in driving climate change over geological timescales

-   **Ozone:** Ozone in the stratosphere protects Earth from UV-B radiation, while tropospheric ozone acts as a greenhouse gas and contributes to warming

-   **Surface albedo:** The reflectivity of the Earth's surface affects how much solar energy is absorbed. Light surfaces (like ice) reflect more energy, while dark surfaces (like forests or oceans) absorb more, influencing the planet's heat balance. Changes in surface reflectivity, such as melting ice and snow, decrease the albedo effect, leading to more heat absorption and further warming

The currently most important driver of climate change is greenhouse gases, particularly CO₂. The mechanism through which CO₂ affects the climate involves the greenhouse effect: CO₂ molecules in the atmosphere absorb long-wave radiation emitted from the Earth's surface and re-radiate it in all directions, including back toward the surface. This process traps heat and increases global temperatures, driving many of the changes we observe in climate patterns.

2.  *Explain briefly what is special about temperature dynamics of recent decades, and why we have good reasons to be concerned.*

In recent decades, global temperatures have been rising at a faster rate than at any other time in human history. This trend is evident from the fact that the hottest years on record have all occurred within the last few decades. One striking example is the extreme heat in Siberia in the spring of 2020, where temperatures were up to 8°C above the recent average. This trend is particularly concerning because it is mainly driven by human activities, especially the emission of greenhouse gases. Unlike previous climate changes, which took place slowly over long periods, today’s fast rise in temperatures increases the risk of triggering dangerous effects, like melting permafrost and losing ice cover, which could make global warming even worse. Even a small increase of 1.5°C could seriously upset the balance of our climate, showing how important it is to take action against these human-caused changes.

3.  *What does the abbreviation ‘RCP’ stand for, how are RCPs defined, and what is their role in projecting future climates?*

RCP stands for Representative Concentration Pathways, which are essential scenarios used in climate modeling to project potential future greenhouse gas emissions and their impacts on the climate. RCPs are defined by the level of radiative forcing — measured in watts per square meter (W/m²) — that is expected by the end of the 21st century. Each pathway corresponds to a specific amount of greenhouse gas concentrations, which can significantly influence global temperatures. The role of RCPs is to serve as inputs for climate models, helping to produce future climate scenarios, which are essential for understanding the potential impacts of climate change and planning appropriate mitigation and adaptation strategies.

4.  *Briefly describe the 4 climate impact projection methods described in the fourth video.*

The four climate impact projection methods described in the fourth video are:

-   **Statistical models:** These models establish relationships between climate parameters and impact measures, such as crop yield. They use historical data to explain past trends and project future climate impacts. Their primary limitation is that the statistical relationships may not remain valid under future climate conditions, and they may overlook important factors

-   **Species Distribution Modeling:** Also known as ecological niche modeling, this method predicts the future distribution of species by relating current presence or absence data to climatic parameters. However, these models may assume species are in equilibrium with the climate, which is often not the case

-   **Process based models:** These models aim to represent all major system processes using equations, capturing the scientific knowledge of processes like crop growth, phenology or hydrology. However, they are limited by the lack of complete understanding of complex systems, and often require extensive parameterization or assumptions

-   **Climate Analogue models:** This method identifies current locations with climates similar to those expected in the future at another site, offering real-world examples that can guide adaptation strategies. However, they may be limited by differences in non-climatic factors and lack of suitable data, making it difficult to draw clear conditions

<!--chapter:end:04-climate.Rmd-->

# Winter chill projections

This section provides an overview of how winter chill can be modeled. It summarizes past studies on this topic, aiming to clarify the methodological aspects that lead to the analyses conducted. By the end of this lesson, most of the analyses presented in the discussed papers should be understandable.

## Learning goals

-   Be aware of past studies that have projected climate change impacts on winter chill
-   Get a rough idea of how such studies are done
-   Get curious about how to do such studies

## Winter chill in Oman

During his doctoral studies at the University of Kassel, [Prof. Dr. Eike Lüdeling](https://www.gartenbauwissenschaft.uni-bonn.de/author/prof.-dr.-eike-luedeling/) became interested in winter chill while participating in research on mountain oases in Oman. Initially focused on calculating nutrient budgets for the oases, particularly in the "Hanging Gardens" of Ash Sharayjah, the study shifted when many fruit trees failed to bear fruit. This led to the hypothesis that insufficient winter chill might be the issue, especially since the oases hosted temperate species such as pomegranates (*Punica granatum*), walnuts (*Juglans regia*), and apricots (*Prunus armeniaca*).

To investigate this, temperature loggers were placed in three oases at different levels of elevation, allowing for the study of chill accumulation along an elevation gradient. A map of the study area illustrates the locations of the oases:

![***Map of oasis systems in Al Jabal Al Akhdar, Oman***](images/Fig_01_gradient_map.jpg)

A nearby long-term weather station provided valuable data, although its location - 1000 meters above the lowest oasis - limited its representativeness. Since records were available from the oases, transfer functions were defined to derive oasis temperatures from the long-term data. These transfer functions were set up using PLS regression, which, in hindsight, wasn’t a very good idea, to directly calculate hourly temperatures in the oases from the daily records of the official station at Saiq.

![**Regression between temperature at Saiq and temperature in three oases, Al Jabal Al Akhdar, Oman**](images/Luedeling_JPG_Figure_02_Hourly_regressions.jpg)

This approach facilitated the calculation of hourly temperatures, which were essential for assessing winter chill dynamics over several years.

![**Chill dynamics between 1983 and 2007, Al Jabal Al Akhdar, Oman**](images/Luedeling_JPG_Figure_09_chilling_hours.jpg)

The findings were submitted to the journal Climatic Change ([Luedeling et al., 2009b](https://link.springer.com/article/10.1007/s10584-009-9581-7)), where reviewers suggested incorporating future climate scenarios. To address this, the LARS-WG weather generator was employed to simulate plausible weather conditions for the oases under scenarios of 1°C and 2°C warming.

![**Chill prospects for 1°C and 2°C warming scenarios in Al Jabal Al Akhdar, Oman**](images/Luedeling_JPG_Figure_10_Future_chilling.jpg)

The results illustrated the potential impacts of climate change on winter chill, marking the beginning of a career focused on chill modeling.

## Chill model sensitivity

After completing a PhD at the University of Kassel, [Prof. Dr. Eike Lüdeling](https://www.gartenbauwissenschaft.uni-bonn.de/author/prof.-dr.-eike-luedeling/) became a Postdoctoral Scholar at the University of California at Davis, where his research focused on climate change impacts on winter chill in California's Central Valley, a key region for temperate fruit tree production.

Upon arriving in California, it became evident that the choice of chill model significantly impacts winter chill quantification. Initially, the simplest model was chosen due to a lack of programming skills, but further investigation highlighted the importance of model selection. Extensive library research revealed the need for a thorough examination of various chill models. Knowledge gained in Oman was utilized to create temperature scenarios for multiple locations, allowing for the analysis of how chill accumulation would likely change in the future.

The analysis focused on changes predicted by various models for the same locations and future scenarios. Here are the locations examined:

![**Weather station locations in California**](images/Luedeling_Figure_1.jpg)

The results revealed considerable variation in chill projections for these locations. The analysis illustrated significant differences in estimates of chill losses by 2050, indicating that not all models could accurately represent winter chill dynamics. Ultimately, the Dynamic Model emerged as the most reliable option, prompting its primary use in subsequent research.

![**Sensitivity of chill projections to model choice (CH - Chilling Hours; Utah - Utah Model; Utah+ - Positive Utah Model; Port. - Dynamic Model)**](images/Luedeling_Figure_2.jpg)

However, challenges arose with the complexity of the Dynamic Model, which required outdated Excel software for calculations. Additionally, the data processing steps necessary to generate credible temperature scenarios proved cumbersome and error-prone, highlighting the need to develop programming skills for more efficient analysis.

## Winter chill in California

The primary goal during the time in California was to create a winter chill projection for the Central Valley, an important region for fruit and nut production. Utilizing California's extensive network of weather stations, the plan involved using data from over 100 stations and generating multiple climate scenarios. To manage this complex task efficiently, a decision was made to automate most processes, leading to an exploration of programming.

The automation was implemented using JSL, a programming language associated with the statistics software JMP, which facilitated the handling of the data. Despite some challenges, the automation was largely successful, though running the weather generator manually for each station remained tedious. Ultimately, projections were generated for all stations, illustrating chill accumulation over 100 plausible winter seasons for each climate scenario.

To present the results effectively, a metric called 'Safe Winter Chill' was developed, defined as the 10th percentile of the chill distribution, indicating the minimum chill amount that would be exceeded in 90% of the years. Here’s an illustration of the Safe Winter Chill metric:

![**Illustration of the Safe Winter Chill concept**](images/Figure_2_Boxplots_Davis_chilling_hours_a2.jpg)

A method for spatially interpolating the station results was also established, leading to the creation of maps that depicted winter chill prospects for the Central Valley. Here’s one of the maps that resulted from this:

![**Winter chill prospects for California’s Central Valley**](images/Figure_4_Chill_Portions_Central_Valley_absolute.jpg)

This analysis was published in the journal PLOS ONE ([Luedeling et al., 2009d](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0006166)).

## Winter chill ratios

Following the automation of processing steps in JSL, attention turned to creating a global winter chill projection. The Global Summary of the Day database was identified as a valuable data source, featuring records from thousands of weather stations. The project proved challenging due to limited programming skills. Data processing was carried out on six computers operating around the clock for several weeks, likely a result of initial setup difficulties rather than the complexity of the analyses. In the end, data for about 5,000 weather stations were processed, generating multiple chill metrics.

This extensive dataset allowed for a comparison of chill models by calculating the ratios between various chill metrics at each station. If these ratios had been consistent worldwide (e.g., one Chill Portion always equating to ten Chilling Hours), any chill model could have been reliably used. However, significant variations in chill metric ratios were observed globally.

![**Chill metric ratios around the world**](images/Luedeling_Fig_4_all_ratios_map.jpg)

This study was published in the International Journal of Biometeorology ([Luedeling & Brown, 2011a](https://link.springer.com/article/10.1007/s00484-010-0352-y)).

## A global projection of future winter chill

Using the same analytical methods, a global projection of the potential impacts of climate change on winter chill was also generated:

![**Projected decline in available winter chill around the world**](images/Figure_5_Time_Comp.jpg)

The regions marked in red and orange on the lower two maps may experience significant impacts on fruit and nut production due to decreasing winter chill. With substantial chill losses, it is unlikely that growers will be able to sustain their current tree cultivars. Notably, the Mediterranean region is expected to be particularly affected.

![**Winter chill projection for the Mediterranean region**](images/Figure_6_regional_grids_Mediterranean.jpg)

This prompted collaboration with partners in the Mediterranean region and other countries with similar climates, such as South Africa and Chile.

## Winter chill in Germany

Germany is not highlighted as particularly vulnerable to chill losses, and an analysis of historical chilling trends from 1950 supports this observation:

![**Winter chill in Germany in 2010, and changes between 1950 and 2010**](images/Chilling_Germany.png)

## Winter chill in Tunisia

Prospects for orchards in Tunisia are particularly challenging due to the region being close to the warmest limits for many fruit and nut tree species. An assessment published in 2018 examined past and future trends in winter chill for an orchard in Central Tunisia, following a seven-year gap from earlier studies. This delay stemmed from other professional commitments and the difficulty of obtaining suitable future climate data for chill modeling.

While climate change data is widely available, much of it is presented as spatial grids, making it cumbersome to work with. Each climate scenario requires numerous grids for temperature and rainfall, leading to substantial data storage needs, sometimes exceeding 700 GB. Soon after establishing a processing structure for these datasets, the IPCC introduced the Representative Concentration Pathways (RCPs), rendering earlier scenarios outdated and complicating the analysis further, especially given the limited data transfer capabilities while based in Kenya.

Collaboration with colleagues in Tunisia, particularly [Haifa Benmoussa](https://scholar.google.de/citations?hl=de&user=DdV9jsAAAAAJ), revealed that tree crops like almonds and pistachios are highly vulnerable to climate change impacts. Fortunately, a new climate database specifically for Africa, called AFRICLIM, was developed, facilitating the acquisition and processing of relevant climate scenarios. This allowed for the incorporation of new functions in `chillR` to sample from AFRICLIM grids and produce the necessary climate projections.

![**Winter chill analysis for an orchard near Sfax in Central Tunisia (blue bars indicate approximate chilling requirements of pistachios and two types of almonds)**](images/Tunisia_2018_Fig_1.jpg)

The figure, which is to be created by the end of the semester, illustrates the historical development of chill accumulation at a specific location, with observed values represented by red dots and typical chill distributions shown as boxplots. These data were generated using a weather generator that is calibrated with historical weather data and produces artificial annual weather records. The generator was also used to create future scenarios based on the AFRICLIM database.

The analysis indicates that in none of the future scenarios does the cultivation of pistachios or high-chill almonds remain viable. This conclusion is supported by observations in Tunisia, where, after the warm winter of 2015/16, many pistachio trees barely developed any flowers, leading to crop failures.

![**Pistachio tree near Sfax, Central Tunisia, in April of 2016**](images/Tunisia_pistachios.png)

## Winter chill in Chile

AFRICLIM addressed the challenge of obtaining future climate data for Africa but did not fully meet the needs for integrating climate change projections into `chillR`. It was limited to African data, and users seeking information for single locations had to download large datasets, which was inefficient. A more effective solution was needed to access data quickly for individual weather stations globally.

An early resource was ClimateWizard, developed by [Evan Girvetz](https://scholar.google.de/citations?user=Yh2sQY4AAAAJ&hl=de), which initially provided gridded data but later included a script for extracting information for specific locations. This functionality was eventually made available through an API at CIAT, allowing access to outputs from 15 climate models for the latest RCP scenarios. This advancement enabled [Eduardo Fernández](https://scholar.google.de/citations?hl=de&user=ibSma_AAAAAJ) to analyze past and future chill development across nine locations in Chile, expanding the geographic scope of the research.

![**Map of fruit growing locations in Chile**](images/Chile_chill_map.png)

The following diagram illustrates the assessment of past and future winter chill across nine locations in Chile:

![**Assessment of past and future winter chill for 9 locations across Chile**](images/Chile_chill_all.png)

Eduardo preferred a different plot design and utilized the `ggplot2` package, a robust plotting tool for R, to redesign it. The complexity of having data from multiple sites made interpretation challenging, prompting Eduardo to creatively summarize key information for each scenario. He presented this information as a heat map, simplifying the visualization.

![**Heatmap showing Safe Winter Chill (10% quantile of chill distribution) for nine locations in Chile**](images/Chile_chill_heatmap.png)

## Chill projection for Patagonia

Certain regions may become more suitable for agriculture as the climate changes. An analysis was conducted to assess the climatic suitability for fruit and nut trees in Patagonia, southern Argentina, which is located at the southern frontier of agriculture:

![**Map of parts of Patagonia in Argentina, showing locations that were analyzed in this study** ](images/Patagonia_map.png)

Weather station records for all locations on the map were obtained, enabling the calibration of a weather generator and the download of climate projections from the ClimateWizard database. This facilitated the creation of past and future temperature scenarios for all stations, as well as the computation of winter chill and other agroclimatic metrics. However, the results of the winter chill calculations were not particularly noteworthy, as minimal changes were projected.

![**Heatmap showing Safe Winter Chill (10% quantile of chill distribution) for eleven locations in Patagonia**](images/Patagonia_heatmap.png)

Climate change could potentially enhance land suitability for fruit trees by providing increased summer heat:

![**Past and projected future heat availability for four exemplary locations in Patagonia**](images/Patagonia_heat.png)

A further beneficial development is a likely reduction in the number of frost hours:

![**Past and projected frost risk for four exemplary locations in Patagonia**](images/Patagonia_frost.png)

While the changes observed may appear minor, they are likely to shift many locations from a climate that is too cool for agriculture, particularly for fruit trees, to a more optimal situation. This presents a rare instance of potentially positive news related to climate change, though it is important to acknowledge that these changes could have negative consequences for natural ecosystems and other agricultural systems.

## Chill model comparison

Eduardo Fernandez recently utilized the climate change analysis framework to enhance previous chill model comparisons, significantly building on earlier work. He compiled a collection of 13 methods for quantifying chill accumulation from existing literature and applied these models to datasets from several locations in Germany, Tunisia, and Chile, which are part of the PASIT project. A map illustrates the locations included in this analysis.

![**Locations used for comparing predictions by a total of 13 chill models across different climates**](images/Model_comp_map.png)

The expectation was that the models would show significant differences in the extent of changes they predicted, and this anticipation was indeed fulfilled:

![**Chill change projections by a total of 13 chill models across different climate scenarios**](images/Model_comp_results.png)

The figure illustrates the changes predicted by 13 different models across various sites and climate scenarios, categorized into three groups: warm, moderate, and cool. Eduardo’s analysis reveals significant discrepancies among the models, highlighting the risks of selecting the most convenient model for predictions. The variation in predictions is evident in the color distribution across the rows of the panels, with a uniform color indicating consistency among models—something that is not observed here.

For locations in Tunisia and Chile, the predictions mainly concern the extent of chill losses, ranging from mild to alarming. In Germany, the situation is even less clear, with some models predicting increases in chill and others predicting decreases.

These findings underscore the importance of model choice, as many models may be arbitrary and can be disregarded, yet uncertainties remain regarding which models accurately represent future conditions. This area of research offers opportunities for further exploration and innovation.

## Chill projection for all of Tunisia

The study projected climate change impacts on winter chill for an orchard near Sfax in Central Tunisia, but the region is not the most favorable for temperate fruit and nut tree cultivation. Tunisia is climatically diverse, featuring mountains, plains, coastal areas, and interior deserts, leading to significant variation in historical and future chill availability across the country.

Under the leadership of [Haifa Benmoussa](https://scholar.google.de/citations?hl=de&user=DdV9jsAAAAAJ), the team mapped chill accumulation throughout Tunisia using a framework previously developed. This analysis utilized data from 20 weather stations in Tunisia and neighboring countries. By applying the established analytical framework to each location, they were able to interpolate results and create chill maps that illustrate the trends in chill availability in Tunisia over the past few decades:

![**Chill availability maps for Tunisia for several past scenarios**](images/Figure_1_Tunisia_historic.png)

The process of interpolating site-specific results into a comprehensive map for Tunisia involves some areas for improvement. Currently, the methodology uses site-specific predictions of Safe Winter Chill, defined as the 10th percentile of the chill distribution derived from annual temperature dynamics generated by the weather model. This information is then interpolated using the Kriging technique.

In addition, the elevations of the locations where chill was modeled are also considered. A linear model is fitted to establish a relationship between chill accumulation and elevation. Using a Digital Elevation Model (DEM), the differences between the model-derived elevations from weather stations and the actual elevations of each location are calculated. This difference, not accounted for in the initial chill surface derived from weather station data, is corrected using the established elevation-chill relationship.

While this method seems reasonable for Tunisia, it may not be suitable for cooler regions like Germany, where the relationship between elevation and chill availability may not be linear. The resulting projection of future chill for Tunisia is displayed in the following map:

![**Chill availability for Tunisia for various plausible scenarios of future climate**](images/Figure_2_Tunisia_all_scenarios.png)

The projections reveal significant concern regarding winter chill in Tunisia. The Dynamic Model, which is regarded as a reliable predictor, indicates substantial decreases in Chill Portions, the units used by the model. This trend poses serious challenges for much of the country. Even in areas where some winter chill is expected to persist, farmers will need to adapt their practices, as the tree species currently cultivated are suited to past climate conditions. Adaptation strategies may include shifting to tree cultivars with lower chilling requirements, provided such options are available.

## Revisiting chill accumulation in Oman

After a decade of exploration in other regions, the analysis turned back to Oman, where there was a desire to enhance the initial study of chill accumulation. The first assessment had limitations, particularly concerning model selection and a lack of adequate future climate data. With encouragement from [Prof. Dr. Andreas Bürkert](https://scholar.google.de/citations?user=ZNvcJJ8AAAAJ&hl=de), a more robust evaluation became possible using the climate change analysis framework. This involved incorporating new methods to convert daily temperatures into hourly data. Updated assessments of past winter chill and future forecasts for the oases of Al Jabal Al Akhdar were produced, with the findings published in *Climatic Change* ([Buerkert et al., 2020](https://link.springer.com/article/10.1007/s10584-020-02862-8)).

## `Exercises` on past chill projections

1.  *Sketch out three data access and processing challenges that had to be overcome in order to produce chill projections with state-of-the-art methodology.*

-   **Accessing Climate Data for Specific Locations**:\
    Previous climate datasets like AFRICLIM and ClimateWizard only provided large-scale data. To get weather data for specific locations without downloading too much extra information, an API was created to quickly access data for single sites

-   **Converting Daily to Hourly Temperature Data**:\
    Chill models need hourly temperature data, but many databases only give daily averages. Early methods for converting daily to hourly data weren't very good, especially in areas with unique temperatures. Improved algorithms were developed to estimate hourly temperatures more accurately from daily data

-   **Handling Large Volumes of Climate Model Outputs**:\
    Studying different climate futures involves dealing with a lot of data from many climate models, which can be hard to manage. To handle this large amount of data effectively, workflows were streamlined and selective processing techniques were used

2.  *Outline, in your understanding, the basic steps that are necessary to make such projections.*

To make climate-based chill projections for specific regions, here are the essential steps typically involved:

-   **Data Collection and Calibration:** collect historical weather data and use it to calibrate a weather generator for realistic temperature simulations

-   **Model Selection and Scenario Setup:** choose relevant climate models and emission scenarios to explore various future climates

-   **Generate Temperature Projections:** downscale climate data, converting it to daily or hourly temperatures as needed for chill calculations

-   **Chill Calculation:** apply chill models to estimate chill accumulation across different climate scenarios

-   **Analysis and Visualization:** compare chill projections across models and scenarios and visualize the findings

-   **Interpretation:** validate projections with observed data where possible and assess agricultural impacts and adaptation needs

<!--chapter:end:05-winter-chill.Rmd-->

# Manual chill

This chapter explains how to calculate Chilling Hours using R and the `chillR` package. Chilling Hours measure the number of hours where temperatures are between 0°C and 7.2°C, which is important for certain plants to meet their cold requirements during dormancy and grow properly.

## Learning goals

-   Learn about some basic R operations we need for calculating Chilling Hours
-   Be able to calculate Chilling Hours
-   Understand what an R function is
-   Be able to write your own basic function

## Chilling hours calculation using chillR

Basic models like the Chilling Hours Model are simple and can be calculated manually, especially if familiar with R or spreadsheet software. This example will show how to understand and use the functions in the `chillR` package to calculate these chill hours.

### **Data Requirements**

The Chilling Hours Model is relatively simple but requires hourly temperature data. A common challenge is the unavailability of such data, though approximations can be made from daily records using `chillR` tools. For this example, a sample dataset, `Winters_hours_gaps`, provided within `chillR`, was used. It contains hourly temperature data recorded in 2008 from a walnut orchard in Winters, California, and is structured with columns for year, month, day, hour, and temperature.

### Loading and Preparing Data

To work with `chillR`, the package was loaded using `library(chillR)`. The data can also be imported via `read.table()` or `read.csv()` for external datasets. The `Winters_hours_gaps` dataset was filtered to retain only the essential columns: year, month, day, hour, and temperature. This cleaned version, stored in a new dataframe called `hourtemps`, ensures the data is in the correct format for calculating Chilling Hours:

```{r, include = FALSE}
library(chillR)
```

```{r}
hourtemps <- Winters_hours_gaps[,c("Year",
                                   "Month",
                                   "Day",
                                   "Hour",
                                   "Temp")]
```

```{r, include = FALSE}
library(kableExtra)
library(tidyverse)
```

```{r, echo = FALSE}
library(kableExtra)

kable(hourtemps[1:10,], align = rep("c", ncol(hourtemps))) %>% 
  kable_styling(
    bootstrap_options = c("striped", "hover"), 
    position = "center", 
    font_size = 15,   
    full_width = TRUE
  )

```

### **Manual Calculation of Chilling Hours**

Chilling Hours are defined as any hour where the temperature falls between 0°C and 7.2°C. A logical comparison was used in R to identify whether each hour met this criterion:

```{r}
hourtemps[, "Chilling_Hour"] <- hourtemps$Temp >= 0 & hourtemps$Temp <= 7.2
```

```{r, echo = FALSE}
kable(hourtemps[1:10, ], align = rep("c", ncol(hourtemps))) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"), 
    position = "center", 
    font_size = 15,   
    full_width = TRUE,
  )

```

A new column, `Chilling_Hour`, was created, indicating whether a given hour was a valid Chilling Hour (TRUE or FALSE). These values can then be summed to calculate the total number of Chilling Hours for any period using the `sum()` function.

### Automation with Functions

A function is a tool that automates a particular procedure. It consists of a name, some arguments that are passed to the function, and some code that should be executed. To avoid repeating manual calculations, a reusable function called `CH` was created to automate the addition of a `Chilling_Hour` column:

```{r}
CH <- function(hourtemps) {
  hourtemps[, "Chilling_Hour"] <- hourtemps$Temp >= 0 & hourtemps$Temp <= 7.2
  return(hourtemps)
}
```

This function applies to any appropriately structured dataset. Additionally, a more complex function, `sum_CH`, was developed to calculate the total Chilling Hours between two specific dates:

```{r}
sum_CH <- function(hourtemps, Start_Year,
                              Start_Month,
                              Start_Day, 
                              Start_Hour, 
                              End_Year, 
                              End_Month, 
                              End_Day, 
                              End_Hour) 
  
{hourtemps[,"Chilling_Hour"] <- hourtemps$Temp > 0 &
                                hourtemps$Temp  <= 7.2

  Start_Date <- which(hourtemps$Year == Start_Year & 
                      hourtemps$Month == Start_Month &
                      hourtemps$Day == Start_Day &
                      hourtemps$Hour == Start_Hour)
  
  End_Date <- which(hourtemps$Year == End_Year &
                    hourtemps$Month == End_Month &
                    hourtemps$Day == End_Day & 
                    hourtemps$Hour == End_Hour)
  
  CHs <- sum(hourtemps$Chilling_Hour[Start_Date:End_Date])
  return(CHs)
}

```

This function calculates Chilling Hours for a user-defined date range, using the `which()` function to identify the relevant rows in the dataset corresponding to the start and end dates.

To simplify the parameter passing, compact strings in the format `YEARMODAHO` (year, month, day, and hour as consecutive numbers) can be used instead of individual parameters for year, month, day, and hour. The start and end times are now passed as strings, from which the required values are extracted using the `substr()` function and converted to numeric values with `as.numeric()`.

```{r}
sum_CH <- function(hourtemps, 
                   startYEARMODAHO,
                   endYEARMODAHO)
{hourtemps[,"Chilling_Hour"] <- hourtemps$Temp > 0 &
  hourtemps$Temp <= 7.2

startYear <- as.numeric(substr(startYEARMODAHO, 1, 4))
startMonth <- as.numeric(substr(startYEARMODAHO, 5, 6))
startDay <- as.numeric(substr(startYEARMODAHO, 7, 8))
startHour <- as.numeric(substr(startYEARMODAHO, 9, 10))

endYear <- as.numeric(substr(endYEARMODAHO, 1, 4))
endMonth <- as.numeric(substr(endYEARMODAHO, 5, 6))
endDay <- as.numeric(substr(endYEARMODAHO, 7, 8))
endHour <- as.numeric(substr(endYEARMODAHO, 9, 10))


Start_row <- which(hourtemps$Year == startYear &
                   hourtemps$Month == startMonth &
                   hourtemps$Day == startDay &
                   hourtemps$Hour == startHour
)
End_row <- which(hourtemps$Year == endYear &
                 hourtemps$Month == endMonth &
                 hourtemps$Day == endDay &
                 hourtemps$Hour == endHour
)

CHs <- sum(hourtemps$Chilling_Hour[Start_row:End_row])
return(CHs)

}

```

### Application Example

The functions created allow for efficient calculation of Chilling Hours. For instance, using the `sum_CH()` function, it was calculated that between April 1st and October 11th, 2008, the walnut orchard experienced 77 Chilling Hours:

```{r}
sum_CH(hourtemps, startYEARMODAHO =2008040100,
                  endYEARMODAHO = 2008101100)
```

## `Exercises` on basic chill modeling

1.  *Write a basic function that calculates warm hours (\>25°C).*

```{r}
WH <- function(data)
  {data[, "Warm_Hour"] <- data$Temp > 25
  return(data)
}
```

2.  *Apply this function to the* `Winters_hours_gaps` *dataset.*

```{r, eval = FALSE}
WH(Winters_hours_gaps)
```

```{r, echo = FALSE}
library(chillR)
result <- WH(Winters_hours_gaps)
kable(result[1:10, ], align = rep("c", ncol(hourtemps))) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"), 
    position = "center", 
    font_size = 15,   
    full_width = TRUE 
  )
```

3.  *Extend this function, so that it can take start and end dates as inputs and sums up warm hours between these dates.*

```{r}
sum_WH <- function(data, 
                   startYEARMODAHO,
                   endYEARMODAHO)
  
{data[,"Warm_Hour"] <- data$Temp > 25

startYear <- as.numeric(substr(startYEARMODAHO, 1, 4))
startMonth <- as.numeric(substr(startYEARMODAHO, 5, 6))
startDay <- as.numeric(substr(startYEARMODAHO, 7, 8))
startHour <- as.numeric(substr(startYEARMODAHO, 9, 10))

endYear <- as.numeric(substr(endYEARMODAHO, 1, 4))
endMonth <- as.numeric(substr(endYEARMODAHO, 5, 6))
endDay <- as.numeric(substr(endYEARMODAHO, 7, 8))
endHour <- as.numeric(substr(endYEARMODAHO, 9, 10))


Start_Date <- which(data$Year == startYear &
                    data$Month == startMonth &
                    data$Day == startDay &
                    data$Hour == startHour)

End_Date <- which(data$Year == endYear &
                  data$Month == endMonth &
                  data$Day == endDay &
                  data$Hour == endHour)

WHs <- sum(data$Warm_Hour[Start_Date:End_Date])
return(WHs)
}
```

Application Example:

```{r}
sum_WH(Winters_hours_gaps, startYEARMODAHO = 2008080100, 
                           endYEARMODAHO = 2008083100)


```

During the month of August 2008, from the 1st to the 31st, the walnut orchard experienced a total of 283 warm hours (defined as hours when the temperature exceeded 25°C).

<!--chapter:end:06-manual-chill.Rmd-->

# Chill

In this chapter, various chill models will be explored using the `chillR` package in R, which simplifies the calculation of chilling hours and other dormancy-related metrics based on temperature data.

## Learning goals

-   Learn how to run chill models using `chillR`
-   Learn how to produce your own temperature response model in `chillR`

## `Chilling_Hours()` function

The `Chilling_Hours()` function calculates the time during which temperatures fall within a key range for chill accumulation. It takes hourly temperature data as input and, by default, provides the cumulative amount of chilling accumulated over time.

```{r, include=FALSE}
library(chillR)
```

```{r}
Chilling_Hours(Winters_hours_gaps$Temp)[1:100]
```

The result will show the first 100 values, where the cumulative chilling hours increase as the temperature falls within the specified range.

## Utah Model

The Utah Model assigns different weights to various temperature ranges, reflecting their impact on chill accumulation. The `Utah_Model()` function in `chillR` calculates these weighted chilling contributions for each hour of temperature data. The output will show the Utah model values for the first 100 hours, where positive, zero, and negative weights are applied based on the temperature:

```{r}
Utah_Model(Winters_hours_gaps$Temp)[1:100]
```

## Creating Custom Chill Models with `step_model()`

The `step_model()` function, part of the `chillR` package, enables the creation of custom chill models based on temperature thresholds and weights. This process involves defining a data frame that specifies temperature ranges and their corresponding weights. Here’s an example of a data frame that defines temperature ranges and their corresponding weights:

```{r}
df <- data.frame(
  lower = c(-1000, 1, 2, 3, 4, 5,    6),
  upper = c(    1, 2, 3, 4, 5, 6, 1000),
  weight = c(   0, 1, 2, 3, 2, 1,    0))
```

```{r, echo = FALSE}
library(kableExtra)
kable(df, align = rep("c", ncol(df))) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                position = "center",
                font_size = 14,
                full_width = TRUE)
```

A function called `custom()` implements a chill model based on this data frame. This function is then applied to the `Winters_hours_gaps` dataset to calculate the chilling contributions:

```{r}
custom <- function(x) step_model(x, df)
custom(Winters_hours_gaps$Temp)[1:100]
```

## Dynamic model

The Dynamic Model provides a more complex and reliable approach to calculating chill, with the `Dynamic_Model()` function handling the intricate equations involved. This function can be easily applied to the `Winters_hours_gaps` dataset, producing output that displays dynamic chill values for the first 100 hours, reflecting the underlying physiological processes:

```{r}
Dynamic_Model(Winters_hours_gaps$Temp)[1:100]
```

## `Chilling` and `tempResponse` functions

The `chillR` package offers several functions for analyzing hourly temperature data, including wrapper functions that enable the computation of chill between specific start and end dates. The `chilling()` function automatically calculates various basic metrics, including Chilling Hours, Utah Model, Dynamic Model, and Growing Degree Hours. It is important to use the `make_JDay()` function to add Julian dates (which count the days of the year) to the dataset, ensuring proper functionality.

```{r}
chill_output <- chilling(make_JDay(Winters_hours_gaps), Start_JDay = 90, End_JDay = 100)
```

```{r, echo = FALSE}
kable(chill_output, align = rep("c", ncol(Winters_hours_gaps))) %>%
  kable_styling("striped",
                position = "center",
                font_size = 14,
                full_width = TRUE)%>%
  scroll_box(width = "100%")
```

However, there may be instances where not all metrics are desired, or there is a need for different metrics altogether. In such cases, the `tempResponse` function can be employed. This function is similar to `chilling()` but offers the flexibility to take a list of specific temperature models to be computed as input.

```{r}
chill_output <- tempResponse(make_JDay(Winters_hours_gaps), 
                       Start_JDay = 90, 
                       End_JDay = 100, 
                       models = list(Chill_Portions = Dynamic_Model, GDH = GDH))
```

```{r, echo = FALSE}
kable(chill_output, align = rep("c", ncol(chill_output))) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14, 
                full_width = TRUE,  
                bootstrap_options = c("striped", "hover", "responsive"))
```

This will return only the Dynamic Model and Growing Degree Hours (GDH**)** values for the specified period.

## `Exercises` on chill models

1.  *Run the* `chilling()` *function on the* `Winters_hours_gap` *dataset.*

```{r}
august <- chilling(make_JDay(Winters_hours_gaps), Start_JDay = 214, End_JDay = 244)
```

```{r, echo = FALSE}
kable(august, align = rep("c", ncol(august))) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14, 
                full_width = FALSE,  
                bootstrap_options = c("striped", "hover", "responsive")) %>%
  scroll_box(width = "100%")
```

2.  *Create your own temperature-weighting chill model using the* `step_model()` *function.*

```{r}
df <- data.frame(
  lower = c(-1000, 0,  5, 10, 15, 20,   25),  
  upper = c(    0, 5, 10, 15, 20, 25, 1000), 
  weight = c(   0, 1,  2,  3,  2,  1,    0))

custom <- function(x) step_model(x, df)
```

```{r, echo = FALSE}
library(kableExtra)
kable(df, align = rep("c", ncol(df))) %>%
  kable_styling(bootstrap_options = c("striped", "hover"),
                position = "center",
                font_size = 14,
                full_width = TRUE)
```

3.  *Run this model on the* `Winters_hours_gaps` *dataset using the* `tempResponse()` *function.*

```{r}
models <- list(
  Chilling_Hours = Chilling_Hours,
  Utah_Chill_Units = Utah_Model,
  Chill_Portions = Dynamic_Model,
  GDH = GDH,
  custom = custom)

result <- tempResponse(make_JDay(Winters_hours_gaps), 
                       Start_JDay = 214, 
                       End_JDay = 244, 
                       models)
```

```{r, echo = FALSE}
library(kableExtra)

kable(result, align = rep("c", ncol(result))) %>% 
  kable_styling(
    bootstrap_options = c("striped", "hover", "responsive"), 
    position = "center", 
    font_size = 14,   
    full_width = TRUE) %>%
   scroll_box(width = "100%")
```

<!--chapter:end:07-chill.Rmd-->

# Making hourly temperatures

## Learning goals

-   Understand why we often need hourly temperature data and why we need ways of making them out of daily data
-   Understand some basic algorithms for making hourly data from daily minimum and maximum temperatures
-   Understand how we can make use of observed hourly temperatures to produce our own empirical transfer function that can make hourly from daily data
-   Be able to use the respective `chillR` functions that implement these steps

## Generating hourly temperatures

With the Chilling Hours function developed, the next challenge arises from the limited availability of hourly temperature data, as most sources provide only daily minimum and maximum temperatures. This limitation complicates the direct calculation of Chilling Hours. Various methods have been employed to address this issue, including relating Chilling Hours to minimum temperatures (Crossa-Raynaud, 1955) or using complex equations.

With better computing tools, some researchers started to assume that daily minimum and maximum temperatures occur at specific times and used linear interpolation for the hours in between, creating a triangular shape for daily temperature patterns ([Baldocchi & Wong, 2008](https://link.springer.com/article/10.1007/s10584-007-9367-8)).

```{r triangular, out.width="80%", out.height="50%", echo=FALSE}
library(chillR)
library(ggplot2)
library(tidyr)
library(kableExtra)

KA_hours <- KA_weather[10:20, ]
KA_hours[, "Hour"] <- 0
KA_hours$Hour[nrow(KA_hours)] <- 23
KA_hours[, "Temp"] <- 0
KA_hours <- make_all_day_table(KA_hours, timestep = "hour")

for (i in 2:nrow(KA_hours))
{
  if (is.na(KA_hours$Tmin[i]))
    KA_hours$Tmin[i] <- KA_hours$Tmin[i - 1]
  if (is.na(KA_hours$Tmax[i]))
    KA_hours$Tmax[i] <- KA_hours$Tmax[i - 1]
}
KA_hours$Temp <- NA

KA_hours$Temp[which(KA_hours$Hour == 6)] <-
  KA_hours$Tmin[which(KA_hours$Hour == 6)]
KA_hours$Temp[which(KA_hours$Hour == 18)] <-
  KA_hours$Tmax[which(KA_hours$Hour == 18)]
KA_hours$Temp <- interpolate_gaps(KA_hours$Temp)$interp

ggplot(KA_hours[20:100, ], aes(DATE, Temp)) +
  geom_line(lwd = 1.5) +
  xlab("Date") +
  ylab("Temperature (°C)") +
  theme_bw(base_size = 15)
```

While assuming a triangular temperature pattern may serve as a rough approximation, it is not entirely realistic. The rate of temperature increase in the morning differs from the rate of decrease in the evening. Additionally, the timing of the lowest daily temperature varies significantly throughout the year, particularly outside of equatorial regions. Therefore, it is important to take these variations into account.

## Idealized daily temperature curves

A major breakthrough in modeling daily temperature curves was made when [Dale E. Linvill](https://journals.ashs.org/hortsci/view/journals/hortsci/25/1/article-p14.xml) from Clemson University published a paper in 1990. He combined two mathematical equations: a sine curve to represent warming during the day and a logarithmic decay function for cooling at night. The times for sunrise and sunset defined the transition between these phases, and the length of each phase was linked to the amount of daylight. This method allowed for more accurate daily temperature curves than previous approaches, but not all researchers adopted these equations due to a lack of awareness or data processing skills.

One challenge with Linvill's equations was their dependence on local sunrise and sunset times. While these can be calculated from observations, having a general method would help researchers. Fortunately, for areas without major geographical features, sunrise and sunset times can be calculated based on solar system geometry. Although agricultural scientists may not be familiar with this, they can use insights from other fields. The `chillR` package uses equations from [Spencer (1971)](https://www.mail-archive.com/sundial@uni-koeln.de/msg01050.html) and [Almorox et al. (2005)](https://www.sciencedirect.com/science/article/abs/pii/S0196890404001992). [Prof. Dr. Eike Lüdeling](https://inresgb-lehre.iaas.uni-bonn.de/author/prof.-dr.-eike-luedeling/) only needed to understand these equations once to code them into an R function for future use.

Bringing together these functions is similar to how the `CH()` function was developed and used in the `sum_CH` function, though the components were more complex. The result is a function that can create realistic daily temperature curves based on the latitude of a location. The provided code illustrates how to use the `daylength` function to create plots showing sunrise time, sunset time, and daylength for Klein-Altendorf (Latitude: 50.4°N) throughout the year:

```{r, include = FALSE}
library(chillR)
library(ggplot2)
library(tidyr)
```

```{r}
Days <- daylength(latitude = 50.4, JDay = 1:365)
Days_df <-
  data.frame(
    JDay = 1:365,
    Sunrise = Days$Sunrise,
    Sunset = Days$Sunset,
    Daylength = Days$Daylength
  )
Days_df <- pivot_longer(Days_df, cols = c(Sunrise:Daylength))

ggplot(Days_df, aes(JDay, value)) +
  geom_line(lwd = 1.5) +
  facet_grid(cols = vars(name)) +
  ylab("Time of Day / Daylength (Hours)") +
  theme_bw(base_size = 15)
```

In this context, JDay refers to the Julian Date, which represents the Day of the Year. For example, January 1st is JDay 1, while December 31st is JDay 365 in regular years and JDay 366 in leap years. The `ggplot2` package is used for creating attractive plots, and the ideal input for it is a data frame. Therefore, the outputs from the `daylength()` function were first converted into a data frame. Additionally, the three time series - Sunrise, Sunset, and Daylength - were organized into a stacked format using the `pivot_longer` command from the `tidyr` package.

The `stack_hourly_temps()` function in the `chillR` package integrates these daily dynamics. This function requires a dataset containing daily minimum and maximum temperatures, specifically with columns labeled `Tmin`, `Tmax`, `Year`, `Month`, and `Day`. The latitude of the location must also be provided. Using these inputs, the function applies the previously discussed equations to calculate hourly temperatures, and it can also output sunrise and sunset times if desired.

To demonstrate this process, another dataset included with `chillR`, called `KA_weather`, will be used. This data frame contains temperature data from the University of Bonn’s experimental station at Klein-Altendorf. The first 10 rows of the `KA_weather` dataset will be shown for illustration:

```{r, echo = FALSE}
library(kableExtra)

kable(KA_weather[1:10,], align = rep("c", ncol(KA_weather))) %>% 
  kable_styling(
    bootstrap_options = c("striped", "hover"), 
    position = "center", 
    font_size = 14,   
    full_width = TRUE
  )
```

The following process describes how hourly temperatures can be calculated based on the idealized daily temperature curve:

```{r, echo=TRUE, eval=FALSE}
stack_hourly_temps(KA_weather, latitude = 50.4)
```

```{r, echo = FALSE}

KA_hourly <- stack_hourly_temps(KA_weather, latitude = 50.4)

kable(KA_hourly$hourtemps[100:120, ], row.names = FALSE, align = c("c", "c","c","c","c","c","c","c"))  %>%
  kable_styling("striped", position = "center", font_size = 14)
```

And here’s a plot of the data:

```{r idealized, out.width="80%", out.height="50%", echo=FALSE}

KA_hourly$hourtemps[, "DATE"] <-
  ISOdate(
    KA_hourly$hourtemps$Year,
    KA_hourly$hourtemps$Month,
    KA_hourly$hourtemps$Day,
    KA_hourly$hourtemps$Hour
  )

ggplot(KA_hourly$hourtemps[20:100, ], aes(DATE, Temp)) +
  geom_line(lwd = 1.5) +
  xlab("Date") +
  ylab("Temperature (°C)") +
  theme_bw(base_size = 15)

```

## Empirical daily temperature curves

In some locations, idealized daily temperature curves are ineffective, particularly in areas with rugged topography where temperate fruit trees may be shaded for part of the day. For example, in the Jabal Al Akhdar region of Oman, where [Prof. Dr. Eike Lüdeling](https://inresgb-lehre.iaas.uni-bonn.de/author/prof.-dr.-eike-luedeling/) initially studied winter chill, various oases in the deeply incised Wadi Muaydin canyon were investigated. Trees near the top of the canyon receive significantly more sunlight than those at the bottom, which is about 1000 meters lower.

![**Terraced oasis fields at Ash Sharayjah**](images/IMG_8487.jpg)

Even in the absence of mountains, the temperature curve in an orchard may not closely resemble the curve proposed by [Linvill (1990)](https://journals.ashs.org/hortsci/view/journals/hortsci/25/1/article-p14.xml) due to its unique microclimate, featuring shaded and sunny spots, dew-covered grass, and bare ground.

In the initial study on Omani oases ([Luedeling et al., 2009b](https://link.springer.com/article/10.1007/s10584-009-9581-7)), this issue was not adequately addressed. However, a recent revisit to the location aimed to improve this aspect ([Buerkert et al., 2020](https://link.springer.com/article/10.1007/s10584-020-02862-8)).

To analyze the temperature patterns, a dataset of hourly temperature data for the relevant location is needed, ideally covering an entire year or multiple years. For this exercise, the `Winters_hours_gaps` dataset from a walnut orchard near Winters, California, will be used, as the temperature logger was directly attached to a tree branch, making it unlikely for the data to exactly match the standard daily temperature curve.

The `Empirical_daily_temperature_curve()` function will be employed to determine the typical hourly temperature patterns for each month of the year, although this method could potentially be enhanced by allowing for continuous analysis instead of monthly breakdowns.

```{r}
empi_curve <- Empirical_daily_temperature_curve(Winters_hours_gaps)
```

```{r, echo = FALSE}
kable(empi_curve[1:48, ], align = rep("c", ncol(empi_curve)))  %>%
  kable_styling("striped", position = "center", font_size = 14)
```

```{r}
ggplot(data = empi_curve[1:96, ], aes(Hour, Prediction_coefficient)) +
  geom_line(lwd = 1.3, 
            col = "red") + 
  facet_grid(rows = vars(Month)) + 
  xlab("Hour of the day") +
  ylab("Prediction coefficient") +
  theme_bw(base_size = 15)
```

The set of coefficients can now be applied to a daily dataset from the same location, allowing for the creation of a reasonable hourly temperature record for the orchard. This is accomplished using the `Empirical_hourly_temperatures` function, which requires a set of hourly coefficients and a daily temperature record that includes `Tmin` and `Tmax` columns.

Additionally, the `?` operator can be used to access help on how to use any function, such as `?Empirical_hourly_temperatures`.

The process also involves using the `make_all_day_table` function, which fills gaps in daily or hourly temperature records and summarizes hourly data into daily minimum and maximum temperatures.

```{r}
coeffs <- Empirical_daily_temperature_curve(Winters_hours_gaps)
Winters_daily <-
  make_all_day_table(Winters_hours_gaps, input_timestep = "hour")
Winters_hours <- Empirical_hourly_temperatures(Winters_daily, coeffs)
```

The next step is to plot the results to visualize the hourly temperature data. This allows for a comparison between the results from the empirical method, the triangular function, and the idealized temperature curve. Additionally, actual observed temperatures will be used to validate the results. To facilitate this process, the data will first be simplified for easier handling:

```{r}
Winters_hours <- Winters_hours[, c("Year", "Month", "Day", "Hour", "Temp")]
colnames(Winters_hours)[ncol(Winters_hours)] <- "Temp_empirical"
Winters_ideal <-
  stack_hourly_temps(Winters_daily, latitude = 38.5)$hourtemps
Winters_ideal <- Winters_ideal[, c("Year", "Month", "Day", "Hour", "Temp")]
colnames(Winters_ideal)[ncol(Winters_ideal)] <- "Temp_ideal"
```

The next step involves creating the 'triangular' dataset. Understanding the process behind this construction is essential.

```{r}
Winters_triangle <- Winters_daily
Winters_triangle[, "Hour"] <- 0
Winters_triangle$Hour[nrow(Winters_triangle)] <- 23
Winters_triangle[, "Temp"] <- 0
Winters_triangle <-
  make_all_day_table(Winters_triangle, timestep = "hour")
colnames(Winters_triangle)[ncol(Winters_triangle)] <-
  "Temp_triangular"

# with the following loop, we fill in the daily Tmin and Tmax values for every
# hour of the dataset

for (i in 2:nrow(Winters_triangle))
{
  if (is.na(Winters_triangle$Tmin[i]))
    Winters_triangle$Tmin[i] <- Winters_triangle$Tmin[i - 1]
  if (is.na(Winters_triangle$Tmax[i]))
    Winters_triangle$Tmax[i] <- Winters_triangle$Tmax[i - 1]
}
Winters_triangle$Temp_triangular <- NA

# now we assign the daily Tmin value to the 6th hour of every day

Winters_triangle$Temp_triangular[which(Winters_triangle$Hour == 6)] <-
  Winters_triangle$Tmin[which(Winters_triangle$Hour == 6)]

# we also assign the daily Tmax value to the 18th hour of every day

Winters_triangle$Temp_triangular[which(Winters_triangle$Hour == 18)] <-
  Winters_triangle$Tmax[which(Winters_triangle$Hour == 18)]

# in the following step, we use the chillR function "interpolate_gaps"
# to fill in all the gaps in the hourly record with straight lines

Winters_triangle$Temp_triangular <-
  interpolate_gaps(Winters_triangle$Temp_triangular)$interp
Winters_triangle <-
  Winters_triangle[, c("Year", "Month", "Day", "Hour", "Temp_triangular")]
```

The next step is to merge all the data frames to facilitate easier display and comparison of the datasets.

```{r}
Winters_temps <-
  merge(Winters_hours_gaps,
        Winters_hours,
        by = c("Year", "Month", "Day", "Hour"))
Winters_temps <-
  merge(Winters_temps,
        Winters_triangle,
        by = c("Year", "Month", "Day", "Hour"))
Winters_temps <-
  merge(Winters_temps,
        Winters_ideal,
        by = c("Year", "Month", "Day", "Hour"))
```

The dataset now includes observed temperatures along with the three approximations: triangular, idealized curve, and empirical curve. To plot this data effectively, the `Year`, `Month`, `Day`, and `Hour` columns will be converted into R's date format using `ISOdate`, and the data frame will be reorganized for better usability.

```{r}
Winters_temps[, "DATE"] <-
  ISOdate(Winters_temps$Year,
          Winters_temps$Month,
          Winters_temps$Day,
          Winters_temps$Hour)


Winters_temps_to_plot <-
  Winters_temps[, c("DATE",
                    "Temp",
                    "Temp_empirical",
                    "Temp_triangular",
                    "Temp_ideal")]
Winters_temps_to_plot <- Winters_temps_to_plot[100:200, ]
Winters_temps_to_plot <- pivot_longer(Winters_temps_to_plot, cols=Temp:Temp_ideal)
colnames(Winters_temps_to_plot) <- c("DATE", "Method", "Temperature")


ggplot(data = Winters_temps_to_plot, aes(DATE, Temperature, colour = Method)) +
  geom_line(lwd = 1.3) + ylab("Temperature (°C)") + xlab("Date")
```

The plot indicates that the triangular curve deviates significantly from the observed data, while the `Temp_empirical` and `Temp_ideal` curves appear quite similar and are difficult to differentiate.

To compare these curves more thoroughly, the Root Mean Square Error (RMSE) can be calculated, as it is useful for quantifying the alignment between predicted and observed values. The `chillR` package includes a function to perform this calculation.

```{r}
# here's the RMSE for the triangular method:
RMSEP(Winters_temps$Temp_triangular, Winters_temps$Temp)
```

```{r}
# here's the RMSE for the idealized-curve method:
RMSEP(Winters_temps$Temp_ideal, Winters_temps$Temp)
```

```{r}
# and here's the RMSE for the empirical-curve method:
RMSEP(Winters_temps$Temp_empirical, Winters_temps$Temp)
```

The results show an RMSE of 4.7 for the triangular method, 1.63 for the idealized curve method, and 1.41 for the empirical curve method. Since a lower RMSE indicates better accuracy, these results demonstrate that calibrating the prediction function with observed hourly data significantly improves accuracy, especially compared to the triangular method.

While it might be questioned how much this affects chill accumulation modeling, it is often found to make a considerable difference.

## `Exercises` on hourly temperatures

1.  *Choose a location of interest, find out its latitude and produce plots of daily sunrise, sunset and daylength.*

The Yakima Valley in Washington State, USA, is located at about 46.6° N latitude. This region has a continental climate with cold winters and hot, dry summers, creating ideal conditions for growing fruit trees. The valley is well known for producing a variety of fruits, including apples, cherries, pears, and grapes, which benefit from its distinct seasonal changes. Using the `daylength()` function, you could create plots showing daily sunrise, sunset, and day length times.

```{r, include = FALSE}
library(chillR)
library(tidyr)
library(ggplot2)
library(kableExtra)
```

```{r}
Yakima <- daylength(latitude = 46.6, JDay = 1:365)

Yakima_df <-
  data.frame(
    JDay = 1:365,
    Sunrise = Yakima$Sunrise,
    Sunset = Yakima$Sunset,
    Daylength = Yakima$Daylength
  )

Yakima_df_longer <- pivot_longer(Yakima_df, cols = c(Sunrise:Daylength))

ggplot(Yakima_df_longer, aes(JDay, value)) +
  geom_line(lwd = 1.5) +
  facet_grid(cols = vars(name)) +
  ylab("Time of Day / Daylength (Hours)") +
  theme_bw(base_size = 15)
```

2.  *Produce an hourly dataset, based on idealized daily curves, for the* `KA_weather` *dataset* *(included in* `chillR`*)*

```{r, echo = TRUE, eval = FALSE}
KA_hourly <- stack_hourly_temps(KA_weather, latitude = 50.4)
```

Based on idealized daily curves, the hourly dataset for Julian Day 6 (January 6th) is shown below:

```{r echo = FALSE}
KA_hourly <- stack_hourly_temps(KA_weather, latitude = 50.4)

kable(KA_hourly$hourtemps[121:144, ], row.names = FALSE, align = c("c"))  %>%  kable_styling("striped", position = "left", font_size = 15)
```

3.  *Produce empirical temperature curve parameters for the* `Winters_hours_gaps` *dataset, and use them to predict hourly values from daily temperatures (this is very similar to the example above, but please make sure you understand what’s going on).*

-   Generating empirical daily temperature curve from observed hourly data:

```{r}
empi_curve <- Empirical_daily_temperature_curve(Winters_hours_gaps)
```

-   Filling gaps in daily or hourly temperature data:

```{r}
Winters_daily <- make_all_day_table(Winters_hours_gaps, input_timestep = "hour")

```

-   Using empirical coefficients to predict hourly temperatures based on daily temperatures:

```{r}
Winters_hours <- Empirical_hourly_temperatures(Winters_daily, empi_curve)
```

```{r}
Winters_hours <- Winters_hours[, c("Year", "Month", "Day", "Hour", "Temp")]
colnames(Winters_hours)[ncol(Winters_hours)] <- "Temp_empirical"
```

```{r}
Winters_temps <-
  merge(Winters_hours_gaps,
        Winters_hours,
        by = c("Year", "Month", "Day", "Hour"))

```

```{r}
Winters_temps[, "DATE"] <-
  ISOdate(Winters_temps$Year,
          Winters_temps$Month,
          Winters_temps$Day,
          Winters_temps$Hour)

Winters_temps_to_plot <-
  Winters_temps[, c("DATE",
                    "Temp",
                    "Temp_empirical")]
Winters_temps_to_plot <- Winters_temps_to_plot[100:200, ]
Winters_temps_to_plot <- pivot_longer(Winters_temps_to_plot, cols=Temp:Temp_empirical)
colnames(Winters_temps_to_plot) <- c("DATE", "Method", "Temperature")

ggplot(data = Winters_temps_to_plot, aes(DATE, Temperature, colour = Method)) +
  geom_line(lwd = 1.3) + ylab("Temperature (°C)") + xlab("Date")
```

<!--chapter:end:08-making-temp.Rmd-->

# Some useful tools in R

## Learning goals

-   Get to know some neat tools in R that can make coding more elegant - and easier
-   Get introduced to the `tidyverse`
-   Learn about loops
-   Get to know the `apply` function family

## An evolving language - and a lifelong learning process

The R universe is constantly evolving, offering much more now than the original base functions. Over time, modern tools and more elegant programming styles have become integral. In the upcoming chapters, some of these new tools will be introduced, along with the basics needed to use them effectively.

## The `tidyverse`

Many of the tools introduced here come from the `tidyverse` - a collection of packages developed by [Hadley Wickham](https://en.wikipedia.org/wiki/Hadley_Wickham) and his team. This collection offers many ways to improve programming skills. In this book, only the functions that are directly used will be covered. A big advantage of the tidyverse is that, with just one command - `library(tidyverse)` - all functions in the package collection become available.

## The `ggplot2` package

The `ggplot2` package, first released by [Hadley Wickham](https://en.wikipedia.org/wiki/Hadley_Wickham) in 2007, has become one of the most popular R packages because it greatly simplifies the creation of attractive graphics. The history of the package can be found [here](https://en.wikipedia.org/wiki/Ggplot2), and an introduction along with links to various tutorials is available [here](https://ggplot2.tidyverse.org/).

## The `tibble` package

A `tibble` is an enhanced version of a `data.frame` that offers several improvements. The most notable improvement is that `tibbles` avoid the common `data.frame` behavior of unexpectedly converting strings into factors. Although `tibbles` are relatively new here, they will be used throughout the rest of the book.

To create a `tibble` from a regular `data.frame` (or a similar structure), the `as_tibble` command can be used:

```{r, include = FALSE}
library(tidyverse)
```

```{r}
dat <- data.frame(a = c(1, 2, 3), b = c(4, 5, 6))
d <- as_tibble(dat)
d
```

## The `magrittr` package - pipes

`Magrittr` helps organize steps applied to the same dataset by using the pipe operator `%>%`. This operator links multiple operations on a data structure, such as a `tlibbe`, making it easier to perform tasks like calculating the sum of all numbers in the dataset:

```{r}
d %>% sum()
```

After the pipe operator `%>%`, the next function automatically takes the piped-in data as its first input, so it’s unnecessary to specify it explicitly. Additional commands can be chained by adding more pipes. This approach allows for building more complex workflows, as shown in examples later.

## The `tidyr` package

The `tidyr` package offers helpful functions for organizing data. The `KA_weather` dataset from `chillR` will be used here to illustrate some of these functions:

```{r, include = FALSE}
library(chillR)
```

```{r}
KAw <- as_tibble(KA_weather[1:10,])
KAw
```

### `pivot_longer`

The `pivot_longer` function, introduced previously, is useful for reshaping data from separate columns (like `Tmin` and `Tmax`) into individual rows. In this setup, each day’s record will have a row for `Tmin` and another for `Tmax`. This transformation is often necessary for tasks like plotting data with the `ggplot2` package. The function can be combined with a pipe for a streamlined workflow:

```{r}
KAwlong <- KAw %>% pivot_longer(cols = Tmax:Tmin)
KAwlong
```

In this example, it was necessary to specify the columns to stack. The `pivot_longer` function serves a similar purpose to the `melt` function from the `reshape2` package, which was used previously and in earlier book editions. However, `pivot_longer` is more intuitive, so it will be used throughout the remaining chapters.

### `pivot_wider`

The `pivot_wider` function allows for the opposite transformation of `pivot_longer`, converting rows back into separate columns:

```{r}
KAwwide <- KAwlong %>% pivot_wider(names_from = name) 
KAwwide
```

The `names_from` argument in `pivot_wider` specifies the column that will provide the headers for the new columns. In some cases, `pivot_wider` might work without this argument, but it’s generally recommended to include it for clarity and to ensure that the function behaves as expected, especially with more complex datasets.

### `select`

The `select` function allows users to choose a subset of columns from a `data.frame` or `tibble`:

```{r}
KAw %>% select(c(Month, Day, Tmax))
```

### `filter`

The `filter` function reduces a `data.frame` or `tibble` to just the rows that fulfill certain conditions:

```{r}
KAw %>% filter(Tmax > 10)
```

### `mutate`

The `mutate` function is essential for creating, modifying, and deleting columns in a `data.frame` or `tibble`. For example, it can be used to add new columns, such as converting `Tmin` and `Tmax` to Kelvin:

```{r}
KAw_K <- KAw %>% mutate(Tmax_K = Tmax + 273.15, Tmin_K = Tmin + 273.15)
KAw_K
```

To delete the columns created with `mutate`, you can set them to `NULL`. This effectively removes the specified columns from the `data.frame` or `tibble`:

```{r}
KAw_K %>% mutate(Tmin_K = NULL, Tmax_K = NULL)
```

Next, the original temperature values will be replaced directly with their corresponding Kelvin values. The following code will make these modifications to the specified columns:

```{r}
KAw %>% mutate(Tmin = Tmin + 273.15, Tmax = Tmax + 273.15)
```

### `arrange`

`arrange` is a function to sort data in `data.frames` or `tibbles`:

```{r}
KAw %>% arrange(Tmax, Tmin)
```

It is also possible to sort a `data.frame` or `tibble` in descending order:

```{r}
KAw %>% arrange(desc(Tmax), Tmin)
```

## Loops

In addition to the `tidyverse` functions, understanding loops is essential for efficient coding. Loops enable the repetition of operations multiple times without needing to retype or copy and paste code. They also allow for modifications to be made with each iteration. While detailed explanations on loops can be found elsewhere, the basics will be covered here.

There are two primary types of loops: **for loops** and **while loops**. For both types, it is necessary to provide instructions that determine how many times the loop will run, as well as what actions to perform during each iteration.

### *For* loops

In a for loop, explicit instructions dictate how many times the code inside the loop should be executed. This is typically done by providing a vector or list of elements, directing R to run the code for each of these elements. As a result, the number of executions corresponds to the number of elements in the vector or list. A counter is needed to track the current iteration, commonly referred to as `i`, though it can be any variable name:

```{r}
for (i in 1:3) print("Hello")
```

This command executed the code three times, producing the same output with each iteration. The structure can be made more complex by including multiple lines of code within curly brackets:

```{r}
addition <- 1

for (i in 1:3)
{
  addition <- addition + 1
  print(addition)
}
```

The code in this loop incremented an initial value of 1 by 1 in each iteration and printed the resulting value. It is important to note that R may require explicit instructions to `print` these values when the operation is embedded within a loop.

By utilizing the index `i` within the code block, additional flexibility can be introduced to the operations performed in each iteration:

```{r}
addition <- 1

for (i in 1:3)
{
  addition <- addition + i
  print(addition)
}
```

In this iteration, the respective value of `i` was added to the initial element during each run. Additionally, `i` can be utilized in more creative ways within the loop to enhance the operations being performed:

```{r}
names <- c("Paul", "Mary", "John")

for (i in 1:3)
{
  print(paste("Hello", names[i]))
}
```

The counter in a loop does not have to be numeric; it can take on various forms, including strings. By using this flexibility, the same output as generated in the previous code block can be achieved with a different formulation:

```{r}
for (i in c("Paul", "Mary", "John"))
{
  print(paste("Hello", i))
}
```

### *While* loops

A loop can also be controlled using a `while` statement, which executes the code until a specified condition is no longer met. This approach is meaningful only if the condition can change based on the operations performed within the loop:

```{r}
cond <- 5

while (cond > 0)
{
  print(cond)
  cond <- cond - 1
}
```

Once `cond` reaches 0, the starting condition is no longer satisfied, and the code will not execute again. It is important to note that while loops can lead to issues if the condition remains true regardless of the code block's execution. In such cases, the code may become stuck and will need to be manually interrupted.

## `apply` functions

In addition to loops, R offers a more efficient way to perform operations on multiple elements simultaneously. This method, which is often faster, utilizes functions from the **apply** family: `apply`, `lapply`, and `sapply`. These functions require two key arguments: the list of items to which the operation will be applied and the operation itself.

### `sapply`

When the goal is to apply an operation to a vector of elements, the simplest function to use is `sapply`. It requires two arguments: the vector (`X`) and the function to be applied (`FUN`). For illustration, a simple function called `func` will be created, which adds 1 to an input object:

```{r}
func <- function(x)
  x + 1

sapply(1:5, func)
```

The output is a vector of numbers that are each 1 greater than the corresponding elements of the input vector. When this function is applied to a list of numbers, the output becomes a matrix, although the values remain the same:

```{r}
sapply(list(1:5), func)
```

### `lapply`

To obtain a list as the output, the `lapply` function can be used. This function treats the input element `X` as a list and returns a new list containing the same number of elements as the input. Each element in the output list corresponds to the result of applying `FUN` to the respective element of the input list:

```{r}
lapply(1:5, func)
```

If the input element `X` is a list, `lapply` treats the entire list as a single input element, applying `FUN` to the whole list and returning the result as one element in the output list. An example can help clarify this behavior:

```{r}
lapply(list(1:5), func)
```

### `apply`

The basic `apply` function is designed for applying functions to arrays, allowing operations to be performed either on the rows (`MARGIN = 1`) or on the columns (`MARGIN = 2`) of the array. While this function may not be used frequently, here are some simple examples of its functionality. For further information, it is advisable to consult the help file or explore online resources, as there are many helpful materials available.

```{r}
mat <- matrix(c(1, 1, 1, 2, 2, 2, 3, 3, 3), c(3, 3))
mat
```

```{r}
apply(mat, MARGIN = 1, sum) # adding up all the data in each row
```

```{r}
apply(mat, MARGIN = 2, sum) # adding up all the data in each column
```

## `Exercises` on useful R tools

1.  *Based on the* `Winters_hours_gaps` *dataset, use* `magrittr` *pipes and functions of the* `tidyverse` *to accomplish the following:*

-   

    a)  *Convert the dataset into a* `tibble`

-   

    b)  *Select only the top 10 rows of the dataset*

```{r, include = FALSE}
library(tidyverse)
library(chillR)
library(ggplot2)
```

```{r}
WHG <- as_tibble(Winters_hours_gaps[1:10, ])
WHG
```

-   

    c)  *Convert the* `tibble` *to a* `long` *format, with separate rows for* `Temp_gaps` *and* `Temp`

To see the difference between the columns `Temp_gaps` and `Temp`, rows 279 to 302 (Julian Day 15) are used below:

```{r}
WHG <- as_tibble(Winters_hours_gaps[279:302, ])
WHGlong <- WHG %>% pivot_longer(cols = Temp_gaps:Temp)
WHGlong
```

-   

    d)  *Use* `ggplot2` *to plot* `Temp_gaps` *and* `Temp` *as facets (point or line plot)*

```{r, warning = FALSE}
ggplot(WHGlong, aes(Hour, value)) +
  geom_line(lwd = 1.5) +
  facet_grid(cols = vars(name)) +
  ylab("Temperature (°C)") +
  theme_bw(base_size = 15)
```

-   

    e)  *Convert the dataset back to the* `wide` *format*

```{r}
WHGwide <- WHGlong %>% pivot_wider(names_from = name)
WHGwide
```

-   

    f)  *Select only the following columns:* `Year`, `Month`, `Day` *and* `Temp`

```{r}
WHG %>% select(c(Year, Month, Day, Temp))
```

-   

    g)  *Sort the dataset by the* `Temp` *column, in descending order*

```{r}
WHG %>% arrange(desc(Temp))
```

2.  *For the* `Winter_hours_gaps` *dataset, write a* `for` *loop to convert all temperatures* *(*`Temp` *column) to degrees Fahrenheit*

So that the execution of the following code does not take too long, only Julian Day 15 (rows 279 to 302) is used here. To convert the entire `Temp` column to Fahrenheit, just omit `[279:302]`

```{r}
Temp <- Winters_hours_gaps$Temp[279:302]

for (i in Temp)
{
  Fahrenheit <- i * 1.8 + 32 
  print(Fahrenheit)
}
```

3.  *Execute the same operation with a function from the* `apply` *family*

Here it is the same as in 2, just omit `[279:302]` to convert the entire `Temp` column

```{r}

x <- Winters_hours_gaps$Temp

fahrenheit <- function(x)
  x * 1.8 + 32

sapply(x[279:302], fahrenheit)
```

4.  *Now use the* `tidyverse` *function* `mutate` *to achieve the same outcome*

```{r}
WHG_F <- WHG %>% mutate(Temp_F = Temp * 1.8 + 32)
WHG_F
```

<!--chapter:end:09-tools-in-R.Rmd-->

# Getting temperature data

## Learning goals for this lesson

-   Appreciate the need for daily temperature data
-   Know how to get a list of promising weather stations contained in an international database
-   Be able to download weather data using `chillR` functions
-   Know how to convert downloaded data into `chillR` format

## Temperature data needs

Temperature data is essential for phenology and chill modeling, as it serves as a key input for these models. Although weather data might seem easily accessible, it is often challenging to obtain. Many countries have official weather stations that record the necessary information, but access to these data is frequently restricted and expensive, despite likely being funded by taxpayers. While such costs may be manageable for small-scale studies, they quickly become prohibitive for larger analyses.

Given the urgent need to understand climate change impacts, these access limitations are a barrier to effective climate research. Such restrictions may also reduce the quality of studies that rely on comprehensive data. Ideally, high-quality, localized datasets would be available, but in their absence, some alternative databases can be used. Currently, `chillR` connects to one global and one California-specific database, with the potential to expand as more data sources become accessible.

## The Global Summary of the Day database

The [National Centers for Environmental Information (NCEI)](https://www.ncei.noaa.gov/), formerly known as the National Climatic Data Center (NCDC), offers valuable temperature data through its [Global Summary of the Day (GSOD) database](https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-day). While accessing this data can be challenging, the bulk download section provides weather records for each station and year. However, finding specific stations can be time-consuming.

A list of weather stations is available on NOAA's website, and the process of downloading and organizing the data can be automated using the `chillR` tool. A previous user, [Adrian Fülle](https://www.gartenbauwissenschaft.uni-bonn.de/author/adrian-fulle/), developed a faster and more efficient method for this task. The `chillR` function `handle_gsod()` simplifies the process by guiding users through the necessary steps.

### `action=list_stations`

When used with this action, the `handle_gsod()` function retrieves a list of weather stations and sorts them by their proximity to specified coordinates. For example, stations near Bonn (Latitude: 50.73, Longitude: 7.10) can be identified. Additionally, a time interval, such as 1990-2020, can be set to limit the search to data from those specific years:

```{r list_stations, eval=FALSE}
library(chillR)
station_list<-handle_gsod(action="list_stations",
                          location=c(7.10,50.73),
                          time_interval=c(1990,2020))
```

```{r load_actual_weather_files, echo=FALSE, message=FALSE}
station_list<-read.csv("data/station_list.csv")
weather<-list(`KOLN BONN`=read.csv("data/Bonn_weather.csv"))
cleaned_weather<-list(`KOLN BONN`=read.csv("data/Bonn_chillR_weather.csv"))
```

```{r print_station_list}
require(kableExtra)

kable(station_list) %>%
  kable_styling("striped", position = "left", font_size = 8)

```

The list provided contains the 25 closest weather stations to the specified location, ordered by their distance to the target coordinates, which is shown in the `distance` column. The `Overlap_years` column indicates the number of years for which data is available, while the `Perc_interval_covered` column shows the percentage of the target time period that is covered by the data. It's important to note that this is based solely on the start and end dates in the table. The dataset may have gaps, and these gaps can sometimes cover almost the entire record.

### `action="download_weather"`

When used with this option, the `handle_gsod()` function downloads the weather data for a specific station, using its unique `chillR_code`, which is displayed in the respective column of the station list. Instead of manually entering the code, the function can reference the code from the station list. To download the data, the code for the 4th station in the list can be used, as it appears to cover most of the desired time period.

```{r download_weather, eval=FALSE}
weather<-handle_gsod(action="download_weather",
                     location=station_list$chillR_code[4],
                     time_interval=c(1990,2020))
```

The result of this operation is a list with two elements. The first element (`weather[[1]]`) indicates the source of the database from which the data was retrieved. The second element (`weather[[2]]`) contains the actual weather dataset, which can be viewed here.

```{r, eval=FALSE}
weather[[1]][1:20,]
```

```{r downloaded_weather, echo=FALSE}
kable(weather[[1]][1:20,]) %>%
  kable_styling("striped", position = "left", font_size = 14)

```

The data appears complicated and includes unnecessary information. To simplify this, `chillR` provides a function that streamlines the record. However, this process removes several variables, including quality flags, which may indicate unreliable data. While these have been overlooked so far, there is potential for improvement.

### downloaded weather as `action` argument

This method of calling `handle_gsod()` cleans the dataset and converts it into a format that `chillR` can easily process.

```{r clean_weather,eval=FALSE}
cleaned_weather<-handle_gsod(weather)
```

```{r, eval=FALSE}
cleaned_weather[[1]][1:20,]

```

```{r output_cleaned_weather, echo=FALSE}
kable(cleaned_weather[[1]][1:20,]) %>%
  kable_styling("striped", position = "left", font_size = 14)

```

The strange numbers in the dataset are due to the original database storing temperatures in degrees Fahrenheit. These values were then converted to degrees Celsius using the formula:

$Temperature[°C]=(Temperature[°F]-32)\cdot\frac{5}{9}$

This conversion process can result in awkward numbers, but it's straightforward to apply. After this conversion, the temperature records are now in a more usable format for working with `chillR`.

However, upon closer inspection, the dataset reveals significant gaps, including entire years with missing data. This issue will be addressed in the lesson on filling gaps in temperature records.

Additionally, `chillR` has a similar function for downloading data from the [California Irrigation Management Information System (CIMIS)](https://cimis.water.ca.gov/), and there is potential for more data sources to be integrated into `chillR`.

Finally, the files generated in this process will be saved for use in upcoming chapters:

```{r save_downloaded_weather, eval=FALSE}
write.csv(station_list,"data/station_list.csv",row.names=FALSE)
write.csv(weather[[1]],"data/Bonn_raw_weather.csv",row.names=FALSE)
write.csv(cleaned_weather[[1]],"data/Bonn_chillR_weather.csv",row.names=FALSE)

```

## `Exercises` on getting temperature data

Please document all results of the following assignments in your `learning logbook`.

1)  *Choose a location of interest and find the 25 closest weather stations using the* `handle_gsod` *function*

```{r, include = FALSE}
library(chillR)
```

```{r}
station_list_Yakima <- handle_gsod(action = "list_stations",
                                   location = c(long = -120.50, lat = 46.60), 
                                   time_interval = c(1990, 2020))
```

```{r , echo = FALSE}
library(kableExtra)
kable(station_list_Yakima, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)
```

2)  *Download weather data for the most promising station on the list*

```{r}
weather_Yakima <- handle_gsod(action = "download_weather",
                              location = station_list_Yakima$chillR_code[1],
                              time_interval = c(1990, 2020))
```

```{r, eval = FALSE}
weather_Yakima[[1]][1:20,]
```

```{r, echo = FALSE}
library(kableExtra)
kable(weather_Yakima[[1]][1:20,], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
   scroll_box(width = "100%", 
              height = "300px",
              fixed_thead = TRUE)
              
              

```

3)  *Convert the weather data into* `chillR` *format*

```{r}
cleaned_weather_Yakima <- handle_gsod(weather_Yakima) 
```

```{r, eval = FALSE}
cleaned_weather_Yakima[[1]][1:20,]
```

```{r, echo = FALSE}
library(kableExtra)
kable(cleaned_weather_Yakima[[1]][1:20,], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(height = "300px",
             fixed_thead = TRUE)

```

```{r, warning = FALSE}
dir.create("Yakima")
write.csv(station_list_Yakima,"Yakima/station_list.csv", row.names = FALSE)
write.csv(weather_Yakima[[1]],"Yakima/raw_weather.csv", row.names = FALSE)
write.csv(cleaned_weather_Yakima[[1]],"Yakima/chillR_weather.csv", row.names = FALSE)
```

<!--chapter:end:10-temp-data.Rmd-->

# Filling gaps in temperature records

## Learning goals

-   see why having gaps in records can be quite problematic
-   learn about (too?) simple ways to fill gaps in daily temperature records
-   learn how to use data from auxiliary weather stations to fill gaps in daily temperature records
-   learn about a creative way to close gaps in hourly temperature records

## Gaps

There is a wealth of weather data, but it often contains gaps due to issues like equipment malfunctions, power outages, and storage problems. These gaps pose challenges for modeling agroclimatic conditions, as many scientific methods struggle with missing data. Therefore, effective gap-filling methods are necessary.

## Filling short gaps in daily records

Weather records are mostly complete, though some daily maximum or minimum temperatures may be missing. For short gaps, simple linear interpolation can estimate values by averaging the last known and first known measurements around the gap. This approach can work for gaps of 2-3 days, though accuracy declines with longer gaps. For extended absences of several weeks or more, linear interpolation may miss significant temperature trends, and for gaps as long as a year, it could entirely overlook seasonal patterns, leading to large errors. Nonetheless, the `chillR` package provides a function, `interpolate_gaps()`, to perform this basic interpolation:

```{r, include = FALSE}
library(chillR)
library(tidyverse)
library(kableExtra)
library(magrittr)
```

```{r}
weather <- KA_weather %>% make_all_day_table()

Tmin_int <- interpolate_gaps(weather[,"Tmin"])

weather <- weather %>% mutate(Tmin = Tmin_int$interp,
                              Tmin_interpolated = Tmin_int$missing)

Tmax_int <- interpolate_gaps(weather[,"Tmax"])

weather <- weather %>% mutate(Tmax = Tmax_int$interp,
                              Tmax_interpolated = Tmax_int$missing)
```

The `fix_weather()` function in `chillR` uses linear interpolation to fill gaps in minimum and maximum temperature data by default. If any entire days are missing, it adds rows for these dates using `make_all_day_table()`. Users can also customize the function by setting the range of years (with `start_year` and `end_year`), specifying specific dates (using `start_date` and `end_date` in Julian days), and choosing column names if they differ from `Tmin` and `Tmax`.

```{r}
# add an extra day to the KA_weather dataset that is not connected to the days that are already there.
# this creates a large gap, which we can then interpolate
KA_weather_gap <- rbind(KA_weather, c(Year = 2011,
                                      Month = 3,
                                      Day = 3,
                                      Tmax = 26,
                                      Tmin = 14)) 

# fill in the gaps between Julian date 300 (late October) and 100 (early April), only returning data between 2000 and 2011
fixed_winter_days <- KA_weather_gap %>% fix_weather(start_year = 2000,
                                                    end_year = 2011,
                                                    start_date = 300,
                                                    end_date = 100)

# fill in all gaps
fixed_all_days <- KA_weather_gap %>% fix_weather()
```

The `fix_weather()` function returns a list with two components:

1.  `weather`: A `data.frame` containing the interpolated weather data. It includes two additional columns, `no_Tmin` and `no_Tmax`, which mark rows where minimum or maximum temperatures were originally missing (indicated as `TRUE`, otherwise `FALSE`).
2.  `QC:` A quality control object that summarizes the number of interpolated values for each season.

These QC elements provide an overview of the data quality and interpolation extent for each dataset processed:

```{r, eval = FALSE}
fixed_winter_days$QC
```

```{r, echo = FALSE}
kable(fixed_winter_days$QC, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)

```

```{r, eval = FALSE}
fixed_all_days$QC
```

```{r, echo = FALSE}
kable(fixed_all_days$QC, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)
```

As mentioned, linear interpolation works reasonably well for short gaps in data but becomes less reliable as the gaps lengthen. Below is a simple demonstration of this effect:

```{r}
gap_weather <- KA_weather[200:305, ]
gap_weather[ ,"Tmin_observed"] <- gap_weather$Tmin
gap_weather$Tmin[c(2, 4:5, 7:9, 11:14, 16:20, 22:27, 29:35, 
                   37:44, 46:54, 56:65, 67:77, 79:90, 92:104)] <- NA
fixed_gaps <- fix_weather(gap_weather)$weather

ggplot(data = fixed_gaps,
       aes(DATE, Tmin_observed)) +
  geom_line(lwd = 1.3) +
  xlab("Date") +
  ylab("Daily minimum temperature (°C)") +
  geom_line(data = fixed_gaps, aes(DATE, Tmin), col = "red", lwd = 1.3)
```

The plot illustrates original temperature measurements (in black) alongside interpolated values (in red). To simulate gaps, values were removed incrementally from the dataset - starting with single missing values on the left and increasing up to 13 consecutive missing values on the right. The results show that for shorter gaps (left side), interpolation closely follows the temperature trends. However, as gaps grow longer (right side), the interpolated values deviate more significantly, failing to capture the actual temperature dynamics accurately. This visual demonstrates the diminishing reliability of linear interpolation as gaps increase in size.

```{r}
fixed_gaps[,"error"] <- abs(fixed_gaps$Tmin - fixed_gaps$Tmin_observed)

ggplot(data = fixed_gaps,
       aes(DATE, error)) +
  geom_line(lwd = 1.3) +
  xlab("Date") +
  ylab("Error introduced by interpolation (°C)") +
  geom_point(data = fixed_gaps[which(!fixed_gaps$no_Tmin),],
             aes(DATE, error),col = "red", cex = 3)
```

The size of interpolation errors depends on temperature variation during the missing periods - high variation leads to larger errors. As gap sizes increase, errors tend to grow, especially in the middle of large gaps, where interpolated values can be far from actual conditions. While errors are zero at points where data exists, larger gaps clearly require a more advanced method for accurate estimation.

## Filling long gaps in daily records

Long gaps in temperature records are a persistent issue because actual temperatures at specific times and locations are unknown if no measurements were taken. While linear interpolation can provide reasonable estimates for short gaps, longer gaps require more reliable approaches. Instead of using complex interpolation algorithms, an effective solution can be to incorporate additional data from nearby weather stations in climatically similar settings.

Typically, if another station close to the target site has a similar climate, its data can be used to fill in gaps. For example, this method is used in the [CIMIS](https://cimis.water.ca.gov/) network in California, where data from nearby stations is often used to address missing values. However, this approach has limitations, as even small differences in elevation or unique landscape features (like lakes, forests, or coastlines) can lead to climate variations that make nearby stations less accurate proxies.

In `chillR`, the `patch_weather()` function addresses these limitations by using data from auxiliary stations to fill gaps. It can detect certain biases, adjust for mean temperature biases, and apply the corrected data to the location of interest, thus providing a more refined way to handle missing temperature records.

To fill gaps in the temperature dataset for Bonn, downloaded in the *Getting temperature data* lesson, the following steps should be taken:

```{r}
Bonn <- read.csv("data/Bonn_chillR_weather.csv")
Bonn_QC <- fix_weather(Bonn)$QC
```

```{r, eval = FALSE}
Bonn_QC
```

```{r echo = FALSE}
kable(Bonn_QC, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)
```

The dataset for Bonn has significant gaps between 1998 and 2004, as well as in 2008 (where almost all values are missing), along with shorter gaps in 2015, 2018, and 2020.

To address these gaps, data from nearby weather stations is required. The `handle_gsod()` function can be used to find relevant stations in the surrounding area.

```{r}
station_list <- handle_gsod(action = "list_stations",
                            location = c(7.10, 50.73),
                            time_interval = c(1990, 2020))
```

```{r, eval = FALSE}
station_list
```

```{r, echo = FALSE}

kable(station_list, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)
```

Many of the stations listed may not be useful, as they only overlap with the existing record for a few years, or not at all. As a result, it's unlikely that any single station will be able to fill all the gaps in Bonn’s temperature data. However, by combining data from multiple auxiliary stations, it may be possible to fill in the missing values. Promising stations include **BONN-HARDTHOEHE**, **BONN-ROLEBER**, and **NORVENICH**. These stations will be downloaded and stored in a list.

In `chillR` version 0.74 and later, the `handle_gsod()` function can download multiple station records at once, returning them as a named list. This function will be used to download the records for the selected stations (positions 2, 3, and 6 in the station list).

```{r message=FALSE, warning=FALSE}
patch_weather <-
      handle_gsod(action = "download_weather",
                  location = as.character(station_list$chillR_code[c(2, 3, 6)]),
                  time_interval = c(1990, 2020)) %>%
  handle_gsod()
```

With the list of potentially useful weather records now available, the next step is to use the `patch_daily_temperatures()` function. This function will integrate the data from the selected auxiliary stations to fill the missing temperature values in the Bonn dataset:

```{r}
patched <- patch_daily_temperatures(weather = Bonn,
                                    patch_weather = patch_weather)
```

The results can be reviewed by examining the statistics element of the patched object, accessed using `patched$statistics`. This will provide an overview of how the gaps were filled and the adjustments made using the auxiliary station data.

```{r, eval = FALSE}
# Patch statistics for NORVENICH
patched$statistics[[1]]
```

```{r, echo = FALSE}

kable(patched$statistics[[1]], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 

```

```{r, eval = FALSE}
# Patch statistics for BONN-HARDTHOEHE
patched$statistics[[2]]
```

```{r, echo = FALSE}

kable(patched$statistics[[2]], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

```{r, eval = FALSE}
# Patch statistics for BONN-ROLEBER
patched$statistics[[3]]
```

```{r, echo = FALSE}

kable(patched$statistics[[3]], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

The analysis provides insight into the similarity between the temperature records from the auxiliary stations and the target station in Bonn, specifically for both `Tmin` and `Tmax`. It includes the following statistics:

1.  `filled`: The number of days for which data was successfully transferred from each auxiliary station.
2.  `gaps_remain`: The number of gaps still present after the patching process.

Additionally, two quality statistics are provided:

-   **Mean Bias (**`mean_bias`**)**: The average temperature difference between the auxiliary station and the Bonn station. This can be easily addressed by adjusting daily temperature values when transferring data between stations, and the `patch_daily_temperatures()` function automatically corrects for this.

-   **Standard Deviation of Daily Differences (**`stdev_bias`**)**: This indicates the variability in the differences between the stations. A high stdev_bias suggests that the temperature differences are not systematic and may not be easily corrected. If the stdev_bias exceeds a certain threshold, it may be best to exclude that station from the data transfer.

To address these metrics, limits can be set for both. The `mean_bias` can be capped at 1°C, and the `stdev_bias` can be capped at 2°C. These thresholds will be passed as parameters (`max_mean_bias` and `max_stdev_bias`) to the `patch_daily_temperatures()` function. After setting these limits, the statistics should be reviewed again to ensure the quality of the patching process.

```{r}
patched <- patch_daily_temperatures(weather = Bonn,
                                    patch_weather = patch_weather,
                                    max_mean_bias = 1,
                                    max_stdev_bias = 2)
```

```{r, eval = FALSE}
# Patch statistics for NORVENICH
patched$statistics[[1]]
```

```{r, echo = FALSE}
kable(patched$statistics[[1]], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

```{r, eval = FALSE}
# Patch statistics for BONN-HARDTHOEHE
patched$statistics[[2]]
```

```{r, echo = FALSE}
kable(patched$statistics[[2]], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

```{r, eval = FALSE}
# Patch statistics for BONN-ROLEBER
patched$statistics[[3]]
```

```{r, echo = FALSE}
kable(patched$statistics[[3]], align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

After applying the filters, all records from **BONN-HARDTHOEHE** and the `Tmax` records from **BONN-ROLEBER** were rejected because they did not meet the mean_bias threshold. However, the data from **NORVENICH** were deemed suitable, allowing us to fill 2,146 gaps for both `Tmin` and `Tmax`, leaving just 1 gap remaining for each.

To identify where the remaining gaps are, the next step is to use the `fix_weather()` function. This will allow us to examine the dataset and pinpoint the exact locations of the remaining gaps in both `Tmin` and `Tmax`.

```{r}
post_patch_stats <- fix_weather(patched)$QC
```

```{r, eval = FALSE}
post_patch_stats
```

```{r, echo = FALSE}
kable(post_patch_stats, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)
```

After patching the dataset, only a single day's data remains missing, which is a very short gap. Given its small size, it is reasonable to use linear interpolation to fill this remaining gap. The `fix_weather()` function can be used to interpolate this final missing value and complete the dataset.

```{r}
Bonn_weather <- fix_weather(patched)
```

### Bias-correction for shorter intervals

In the `patch_daily_temperatures()` function, the bias correction is typically based on the average temperature difference between two weather stations over the entire year. However, this approach assumes that the bias between stations is constant throughout the year, which may not be true. In reality, the bias can vary across different seasons, meaning a station might be a good proxy for temperature data during certain times of the year but not others.

To address this, the `patch_daily_temps()` function (note the slight name difference) allows for seasonally adjusted bias correction. By default, this function evaluates temperature data on a monthly basis, making separate comparisons for each calendar month. It can then assess whether an auxiliary station is a reliable proxy for temperatures in each specific month and apply month-specific bias corrections, leading to more accurate estimations with smaller biases than if the entire year were used for comparison.

```{r}
patched_monthly <- patch_daily_temps(weather = Bonn,
                                     patch_weather = patch_weather,
                                     max_mean_bias = 1,
                                     max_stdev_bias = 2,
                                     time_interval = "month")
```

The findings for minimum temperatures from the **NORVENICH** station can be summarized as follows:

```{r, eval = FALSE}
patched_monthly$statistics$Tmin$NORVENICH
```

```{r, echo = FALSE}
kable(patched_monthly$statistics$Tmin$NORVENICH, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) %>% 
  scroll_box(height = "300px",
             fixed_thead = TRUE)

```

The analysis shows that the **mean bias** between the stations varies significantly throughout the year, indicating that a month-specific bias correction would likely improve the accuracy of the temperature data.

To implement this, the `time_interval` parameter in the `patch_daily_temps()` function allows for flexibility in the interval used for bias correction. Intervals can be set to `month`, `week`, or even custom durations like `10 days` or `2 weeks`. The function will start counting these intervals from January 1st of each year, which might result in shorter intervals toward the end of the year. This can generate warnings if the final interval is smaller than the selected period.

However, it's important to keep in mind that using smaller intervals reduces the amount of data available for bias determination. For short time series, very short intervals may not provide enough data, which could reduce the reliability of the bias correction. Therefore, selecting an appropriate interval size is crucial to balancing accuracy and data availability.

```{r, warning = FALSE}
patched_2weeks <- patch_daily_temps(weather = Bonn,
                                    patch_weather = patch_weather,
                                    max_mean_bias = 1,
                                    max_stdev_bias = 2,
                                    time_interval = "2 weeks")
```

To demonstrate the effects, 5000 gaps will be created in the Bonn weather record and filled with proxy data using annual, monthly, and bi-weekly intervals for bias evaluation. The resulting errors will be visualized with a violin plot using `ggplot2`.

```{r, warning = FALSE}
Gaps <- sample(seq(1:nrow(Bonn)), size = 5000, replace = FALSE)

Bonn_gaps <- Bonn %>% mutate(obs_Tmin=Tmin,
                             obs_Tmax=Tmax)
Bonn_gaps$Tmin[Gaps] <- NA
Bonn_gaps$Tmax[Gaps] <- NA

patch_annual <- patch_daily_temps(weather = Bonn_gaps,
                                  patch_weather = patch_weather,
                                  max_mean_bias = 1,
                                  max_stdev_bias = 2,
                                  time_interval = "year")
patch_month <- patch_daily_temps(weather = Bonn_gaps,
                                 patch_weather = patch_weather,
                                 max_mean_bias = 1,
                                 max_stdev_bias = 2,
                                 time_interval = "month")
patch_2weeks <- patch_daily_temps(weather = Bonn_gaps,
                                  patch_weather = patch_weather,
                                  max_mean_bias = 1,
                                  max_stdev_bias = 2,
                                  time_interval = "2 weeks")

Bonn_gaps[,"Tmin_annual"] <- Bonn_gaps$obs_Tmin - patch_annual$weather$Tmin
Bonn_gaps[,"Tmax_annual"] <- Bonn_gaps$obs_Tmax - patch_annual$weather$Tmax
Bonn_gaps[,"Tmin_month"] <- Bonn_gaps$obs_Tmin - patch_month$weather$Tmin
Bonn_gaps[,"Tmax_month"] <- Bonn_gaps$obs_Tmax - patch_month$weather$Tmax
Bonn_gaps[,"Tmin_2weeks"] <- Bonn_gaps$obs_Tmin - patch_2weeks$weather$Tmin
Bonn_gaps[,"Tmax_2weeks"] <- Bonn_gaps$obs_Tmax - patch_2weeks$weather$Tmax

Interval_eval <- Bonn_gaps %>%
  filter(is.na(Tmin)) %>%
  pivot_longer(Tmin_annual:Tmax_2weeks) %>%
  mutate(Type=factor(name,
                     levels = c("Tmin_annual",
                                "Tmin_month",
                                "Tmin_2weeks",
                                "Tmax_annual",
                                "Tmax_month",
                                "Tmax_2weeks")) )

ggplot(Interval_eval,
       aes(Type,value)) +
  geom_violin(draw_quantiles = c(0.25,0.5,0.75)) +
  xlab("Variable and bias evaluation interval") +
  ylab("Prediction error")
```

It is also possible to evaluate the mean daily error:

```{r}
error_eval <-
  data.frame(Variable = c(rep("Tmin",3),rep("Tmax",3)),
             Interval = rep(c("Year","Month","Two weeks"),2),
             Error = c(
             mean(abs(Bonn_gaps$Tmin_annual[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmin_month[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmin_2weeks[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmax_annual[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmax_month[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE),
             mean(abs(Bonn_gaps$Tmax_2weeks[is.na(Bonn_gaps$Tmin)]),na.rm=TRUE))
             )
```

```{r, eval = FALSE}
error_eval
```

```{r, echo = FALSE}
kable(error_eval, align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

The improvement in the results was not very significant, likely because the stations used are relatively close to each other, and the weather conditions between them do not vary much.

### Saving the data for later

The dataset based on monthly intervals should be saved for later use. Before saving, the remaining missing day needs to be interpolated. Once this gap is filled, the dataset can be stored for future use.

```{r}
monthly_bias_fixed <- fix_weather(patched_monthly)
```

The long-term record for Bonn, covering the period from 1990 to 2020, is now complete with no remaining gaps. This dataset can now be saved for future use.

```{r}
write.csv(monthly_bias_fixed$weather,
          "data/Bonn_weather.csv")
```

Only the weather element, a `data.frame`, was saved, as it can be easily exported as a spreadsheet (`.csv` file). While it is also possible to save lists, such as the `QC` element, this requires a more complex process, so it will be saved at a later time.

## `Exercises` on filling gaps

1.  *Use* `chillR` *functions to find out how many gaps you have in your dataset (even if you have none, please still follow all further steps)*

```{r}
Yakima <- read.csv("Yakima/chillR_weather.csv")
Yakima_QC <- fix_weather(Yakima)$QC

```

```{r, echo = FALSE}
library(kableExtra)
kable(Yakima_QC, align = c("c")) %>%
  kable_styling("striped", position = "left", font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)
```

2.  *Create a list of the 25 closest weather stations using the* `handle_gsod` *function*

```{r}
station_list_Yakima <- handle_gsod(action = "list_stations",
                                   location = c(long = -120.50, lat = 46.60),
                                   time_interval = c(1990, 2020))
```

```{r, echo = FALSE}
library(kableExtra)
kable(station_list_Yakima, align = c("c")) %>%
  kable_styling("striped", position = "left", font_size = 14) %>% 
  scroll_box(width = "100%",
             height = "300px",
             fixed_thead = TRUE)
```

3.  *Identify suitable weather stations for patching gaps*
4.  *Download weather data for promising stations, convert them to* `chillR` *format and compile them in a list*

```{r message=FALSE, warning=FALSE}
patch_weather <-
  handle_gsod(action = "download_weather",
              location = as.character(station_list_Yakima$chillR_code[c(4, 6, 8)]),
              time_interval = c(1990, 2020)) %>%
  handle_gsod()
```

5.  *Use the* `patch_daily_temperatures` *function to fill gaps*

    ```{r}
    patched <- patch_daily_temperatures(weather = Yakima,
                                        patch_weather = patch_weather)
    ```

    ```{r, eval = FALSE}
    # Patch statistics for YRANGE OP 13 /AKIMA TRAINING CENTER
    patched$statistics[[1]]
    ```

    ```{r, echo = FALSE}
    kable(patched$statistics[[1]], align = c("c")) %>%
      kable_styling("striped", 
                    position = "center", 
                    font_size = 14) 
    ```

    ```{r, eval = FALSE}
    # Patch statistics for HANFORD AIRPORT
    patched$statistics[[2]]
    ```

    ```{r, echo = FALSE}
    kable(patched$statistics[[2]], align = c("c")) %>%
      kable_styling("striped", 
                    position = "center", 
                    font_size = 14) 
    ```

    ```{r, eval = FALSE}
    # Patch statistics for BOWERS FIELD AIRPORT
    patched$statistics[[3]]
    ```

    ```{r, echo = FALSE}
    kable(patched$statistics[[3]], align = c("c")) %>%
      kable_styling("striped", 
                    position = "center", 
                    font_size = 14) 
    ```

6.  *Investigate the results - have all gaps been filled?*

```{r}
write.csv(patched$weather,
          "Yakima/Yakima_weather.csv", row.names = FALSE)
```

<!--chapter:end:11-gaps.Rmd-->

# Generating temperature scenarios

## Learning goals

-   Understand what a weather generator is, (roughly) how it works and why it is useful for planning
-   Learn how to run the `temperature_generation` function included in `chillR`
-   Be able to produce chill profiles for a place of interest using histograms and cumulative distributions
-   Understand what Safe Winter Chill is and be able to calculate it

## Chill scenarios

The main goal so far has been to understand chill availability and other agroclimatic conditions at specific sites, which is essential for orchard managers choosing tree species and cultivars. While historical data has been helpful, growers need more tailored, site-specific information. They need a forecast of likely conditions - such as expected chill or heat levels - to make better planting decisions.

## Risk assessment in orchard planning

Because trees are long-lived, they encounter varied temperature patterns throughout their productive years. For optimal yields, trees must meet their climatic requirements every year. Therefore, selecting suitable trees requires understanding the plausible range of weather conditions at a site, helping managers choose trees that reliably meet chill needs or balance early flowering with frost risk. This range is determined by the local climate - essentially, the average of yearly weather patterns, representing all potential weather scenarios from which each year is a random outcome.

Annual weather can be interpreted as a random sample from the full distribution of possible weather conditions. For planning, however, it is more useful to understand this entire distribution rather than relying solely on the randomly observed years. So far, only these samples from historical chill data have been available. In this lesson, the goal is to determine the actual climate pattern to develop temperature profiles for specific locations that support orchard planning.

## Weather generators

The best way to characterize the climate of a specific location is through long-term weather records collected there. Such records allow calculation of average monthly temperatures, rainfall extremes, frost frequency, and other metrics. The detail level for describing local climate can vary, and producing realistic weather data requires a certain sophistication.

Weather generators differ significantly in how they model climate, and understanding their methods would help in choosing the best option. For `chillR`, the weather generator must be R-compatible and align with `chillR`’s structure. Currently, only one generator [RMAWGEN](https://cran.r-project.org/web/packages/RMAWGEN/index.html) meets these requirements, limiting choice. Previously, tools like [LARS-WG](https://sites.google.com/view/lars-wg/), which uses both temperature and rainfall data to model distinct dry and wet spell profiles, were used in analyses ([Luedeling et al., 2011a](https://link.springer.com/article/10.1007/s00484-010-0352-y), [2011b](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0020155)). The `chillR` tool doesn’t need precipitation data but performs well overall. However, evaluating these tools remains challenging, indicating potential for further improvement.

## Weather generation in `chillR`

`chillR` uses the [RMAWGEN](https://cran.r-project.org/web/packages/RMAWGEN/index.html) weather generator, currently the only R-based option found, and has shown to be reliable after initial setup challenges. However, `chillR` currently generates only temperature data, without comprehensive weather simulation capabilities—a limitation that is being addressed by further development, initiated by [Lars Caspersen](https://inresgb-lehre.iaas.uni-bonn.de/author/lars-caspersen/).

The function `temperature_generation` in `chillR` requires calibration using long-term temperature data in `chillR` format, enabling it to calculate parameters that define the local climate. These parameters are then applied to simulate synthetic temperature data over a specified number of years.

For practice, the Klein-Altendorf dataset (`KA_weather`) is used as an example. To use `temperature_generation`, a few key inputs are needed:

-   **years**: a vector specifying the years from observed data to characterize the climate.
-   **sim_years**: the years to simulate, used only for labeling purposes in the output dataset.

An additional option, `temperature_scenario`, will be discussed later; initially, its default zero values trigger a warning, which can be ignored for now. The next step is to run the weather generator to compare simulated data with observed records.

```{r, include = FALSE}
library(chillR)
library(ggplot2)
library(kableExtra)
library(tidyverse)
```

```{r warning=FALSE}
Temp <- KA_weather %>%
  temperature_generation(years = c(1998, 2009),
                         sim_years = c(2001,2100))


Temperatures <- KA_weather %>% filter(Year %in% 1998:2009) %>%
  cbind(Data_source = "observed") %>%
  rbind(
    Temp[[1]] %>% select(c(Year,
                           Month,
                           Day,
                           Tmin,
                           Tmax)) %>%
      cbind(Data_source = "simulated")
    ) %>%
  mutate(Date = as.Date(ISOdate(2000,
                                Month,
                                Day)))
```

```{r message=FALSE}
ggplot(data = Temperatures,
       aes(Date, Tmin)) +
  geom_smooth(aes(colour = factor(Year))) +
  facet_wrap(vars(Data_source)) +
  theme_bw(base_size = 20) +
  theme(legend.position = "none") +
  scale_x_date(date_labels = "%b")
```

```{r message=FALSE}
ggplot(data=Temperatures,
       aes(Date,Tmax)) +
  geom_smooth(aes(colour = factor(Year))) +
  facet_wrap(vars(Data_source)) +
  theme_bw(base_size = 20) +
  theme(legend.position = "none") +
  scale_x_date(date_labels = "%b")
```

The previous plot was created using `geom_smooth` in `ggplot`, rather than `geom_line`, which reduced noise in the dataset and made the figure easier to interpret.

Overall, the temperature trends were captured well, though some local weather nuances may have been missed. The simulated dataset now contains 100 years of data, even though only 8 years were used for calibration. This expanded dataset allows for a clearer understanding of the frequency of specific temperature-related events. Next, the distribution of winter chill for Klein-Altendorf will be examined based on this analysis.

```{r}
chill_observed <- Temperatures %>%
  filter(Data_source == "observed") %>%
  stack_hourly_temps(latitude = 50.4) %>%
  chilling(Start_JDay = 305,
           End_JDay = 59)
  
chill_simulated <- Temperatures %>%
  filter(Data_source == "simulated") %>%
  stack_hourly_temps(latitude = 50.4) %>%
  chilling(Start_JDay = 305,
           End_JDay = 59)
  
chill_comparison <-
  cbind(chill_observed,
        Data_source = "observed") %>%
  rbind(cbind(chill_simulated,
              Data_source = "simulated"))
  
chill_comparison_full_seasons <- 
  chill_comparison %>%
  filter(Perc_complete == 100)
```

The data can now be easily visualized using `ggplot`:

```{r}
ggplot(chill_comparison_full_seasons,
       aes(x = Chill_portions)) + 
  geom_histogram(binwidth = 1,
                 aes(fill = factor(Data_source))) +
  theme_bw(base_size = 20) +
  labs(fill = "Data source") +
  xlab("Chill accumulation (Chill Portions)") +
  ylab("Frequency")
```

In the previous code, incomplete winter seasons were manually removed. The resulting histogram shows the distribution of chill that would have been reasonable to expect in Klein-Altendorf between 1998 and 2005. With more observations, these distributions would improve.

These data can also be plotted as a cumulative distribution function, which helps assess the risk of falling below a certain level of chill accumulation. This can also be directly calculated using the `quantile` function.

```{r}
chill_simulations <-
  chill_comparison_full_seasons %>%
  filter(Data_source == "simulated")
  
ggplot(chill_simulations,
       aes(x = Chill_portions)) +
  stat_ecdf(geom = "step",
            lwd = 1.5,
            col = "blue") +
  ylab("Cumulative probability") +
  xlab("Chill accumulation (in Chill Portions)") +
  theme_bw(base_size = 20)
```

```{r}
# Here's the amount of chill that is exceeded in 90% of all years.
quantile(chill_simulations$Chill_portions,0.1)
```

```{r}
# and here's the 50% confidence interval (25th to 75th percentile)
quantile(chill_simulations$Chill_portions, c(0.25,0.75))
```

The 10% quantile calculated above represents what is referred to as "Safe Winter Chill." This concept is illustrated in the following figure:

![](images/Figure_2_Boxplots_Davis_chilling_hours_a2.jpg)

The process for generating the data for this figure is similar to the one used previously, although different tools were employed at the time. The concept of Safe Winter Chill is based on the idea that a grower might tolerate a small risk of not meeting a tree’s chilling requirement. If a tree's chilling requirement exactly matches the Safe Winter Chill for a site, chill-related issues can be expected roughly one in every ten years. Whether this level of risk is acceptable depends on the grower’s tolerance, but it provides a reasonable threshold. A similar metric can be calculated for growers with different risk tolerances.

## `Exercises` on temperature generation

1.  *For the location you chose for your earlier analyses, use chillR’s weather generator to produce 100 years of synthetic temperature data.*

```{r warning=FALSE}
Yakima <- read.csv("Yakima/Yakima_weather.csv")

Temp <- Yakima %>% 
  temperature_generation(years = c(1998, 2009),
                         sim_years = c(2001, 2100))


Temperatures <- Yakima %>% 
  select(Year, Month, Day, Tmin, Tmax) %>%  
  filter(Year %in% 1998:2009) %>%
  cbind(Data_source = "observed") %>%
  rbind(
    Temp[[1]] %>% select(c(Year, Month, Day, Tmin, Tmax)) %>% 
      cbind(Data_source = "simulated")
  ) %>%
  mutate(Date = as.Date(ISOdate(2000, Month, Day)))
```

```{r message=FALSE}
ggplot(data = Temperatures,
       aes(Date,
           Tmin)) +
  geom_smooth(aes(colour = factor(Year))) +
  facet_wrap(vars(Data_source)) +
  theme_bw(base_size = 20) +
  theme(legend.position = "none") +
  scale_x_date(date_labels = "%b")
```

```{r message=FALSE}
ggplot(data = Temperatures,
       aes(Date,
           Tmax)) +
  geom_smooth(aes(colour = factor(Year))) +
  facet_wrap(vars(Data_source)) +
  theme_bw(base_size = 20) +
  theme(legend.position = "none") +
  scale_x_date(date_labels = "%b")
```

2.  *Calculate winter chill (in Chill Portions) for your synthetic weather, and illustrate your results as histograms and cumulative distributions.*

```{r}
chill_observed <- Temperatures %>%
  filter(Data_source == "observed") %>%
  stack_hourly_temps(latitude = 46.6) %>%
  chilling(Start_JDay = 305,
           End_JDay = 59)
  
chill_simulated <- Temperatures %>%
  filter(Data_source == "simulated") %>%
  stack_hourly_temps(latitude = 46.6) %>%
  chilling(Start_JDay = 305,
           End_JDay = 59)
  
chill_comparison <-
  cbind(chill_observed,
        Data_source = "observed") %>%
  rbind(cbind(chill_simulated,
              Data_source = "simulated"))
  
chill_comparison_full_seasons <- 
  chill_comparison %>%
  filter(Perc_complete == 100)
```

```{r}
ggplot(chill_comparison_full_seasons,
       aes(x = Chill_portions)) + 
  geom_histogram(binwidth = 1,
                 aes(fill = factor(Data_source))) +
  theme_bw(base_size = 20) +
  labs(fill = "Data source") +
  xlab("Chill accumulation (Chill Portions)") +
  ylab("Frequency")
```

```{r}
chill_simulations <-
  chill_comparison_full_seasons %>%
  filter(Data_source == "simulated")
  
ggplot(chill_simulations,
       aes(x = Chill_portions)) +
  stat_ecdf(geom = "step",
            lwd = 1.5,
            col = "blue") +
  ylab("Cumulative probability") +
  xlab("Chill accumulation (in Chill Portions)") +
  theme_bw(base_size = 20)
```

3.  *Produce similar plots for the number of freezing hours (\<0°C) in April (or October, if your site is in the Southern Hemisphere) for your location of interest*

```{r}
df <- data.frame(
  lower =  c(-1000,    0),
  upper =  c(    0, 1000),
  weight = c(    1,    0))

freezing_hours <- function(x) step_model(x,df)

chill_observed <- Temperatures %>%
  filter(Data_source == "observed") %>%
  stack_hourly_temps(latitude = 46.6) %>%
  tempResponse(Start_JDay = 91,
               End_JDay = 120,
               models = list(Frost = freezing_hours,
                             Chill_portions = Dynamic_Model,
                             GDH = GDH))

chill_simulated <- Temperatures %>%
  filter(Data_source == "simulated") %>%
  stack_hourly_temps(latitude = 46.6) %>%
  tempResponse(Start_JDay = 91,
               End_JDay = 120,
               models=list(Frost = freezing_hours,
                           Chill_portions = Dynamic_Model,
                           GDH = GDH))

chill_comparison <-
  cbind(chill_observed,
        Data_source = "observed") %>%
  rbind(cbind(chill_simulated,
              Data_source = "simulated"))

chill_comparison_full_seasons <-
  chill_comparison %>%
  filter(Perc_complete == 100)
```

```{r}
ggplot(chill_comparison_full_seasons,
       aes(x = Frost)) + 
  geom_histogram(binwidth = 25,
                 aes(fill = factor(Data_source))) +
  theme_bw(base_size = 10) +
  labs(fill = "Data source") +
  xlab("Frost incidence during April (hours)") +
  ylab("Frequency")
```

```{r}
chill_simulations <-
  chill_comparison_full_seasons %>%
  filter(Data_source == "simulated")

ggplot(chill_simulations,
       aes(x = Frost)) +
  stat_ecdf(geom = "step",
            lwd = 1.5,
            col = "blue") +
  ylab("Cumulative probability") +
  xlab("Frost incidence during April (hours)") +
  theme_bw(base_size = 20)
```

<!--chapter:end:12-generate-temp.Rmd-->

# Saving and loading data (and hiding this in markdown)

## Learning Goals

-   Learn how to write and read tables
-   Learn how to save and load lists of (not-too-complicated) objects
-   Learn how to hide such writing and saving from the readers of your markdown document

The learning logbook compilation time increases as more code and data processing are added, due to handling a large dataset—100 years of synthetic hourly weather data, totaling about 876,000 hours. Each hourly temperature calculation required extensive computations, including sunrise, sunset, day length, and chill metrics with the Dynamic Model function. Climate change scenario analysis will require even more data, with calculations potentially taking an hour or longer. To manage this, saving results for faster reloading becomes essential.

## Saving and loading data

R provides `save` and `load` functions to store and retrieve data, though simpler formats like CSV are often preferred for easy inspection outside R. For basic `data.frames`, `write.csv` can be used to save data efficiently. For example, to save the `Temperatures` dataset created in the last lesson, `write.csv` is a straightforward solution:

```{r, include = FALSE}
library(chillR)
library(tidyverse)
library(kableExtra)
Temperatures<-read_tab("Yakima/Temperatures.csv")
```

```{r, eval = FALSE}
head(Temperatures)
```

```{r, echo = FALSE}
kable((head(Temperatures)), align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

```{r}
write.csv(Temperatures, file = "Yakima/Temperatures.csv", row.names = FALSE)
```

To save the dataset without row numbering, `row.names=FALSE` is set in `write.csv`. To load the dataset back into R, use `read.tab` as shown below:

```{r}
Temperatures <- read_tab("Yakima/Temperatures.csv")
```

```{r, eval = FALSE}
head(Temperatures)
```

```{r, echo = FALSE}
kable((head(Temperatures)), align = c("c")) %>%
  kable_styling("striped", 
                position = "center", 
                font_size = 14) 
```

In the previous example, instead of using R’s built-in `read.csv` function, the `chillR` package’s `read_tab` function was used, which behaves similarly by reading comma-separated files. However, `.csv` files can cause issues in non-English locales where commas are used as decimal symbols (e.g., in French, Spanish, or German environments). In these cases, values are often separated by semicolons instead. `read_tab` can detect whether commas or semicolons are used as delimiters, ensuring compatibility across different regional settings.

Saving and loading simple `data.frames` is straightforward, but more complex objects, like lists of multiple data frames, require a different approach. For this purpose, `chillR` includes functions specifically for saving and loading lists containing numbers, character strings, and `data frames`. Here’s how to create and save a simple list in the designated `data` folder:

```{r}
test_list <- list(Number = 1,
                  String = "Thanks for using chillR!",
                  DataFrame = data.frame(a = c(1, 2, 3),
                                         b = c(3, 2, 1),
                                         c = c(5, 4, 3)))

save_temperature_scenarios(test_list,
                           path = "data",
                           prefix = "test_list")
```

In the data folder, three files have been created:

-   `test_list_1_Number.csv`

-   `test_list_2_String.csv`

-   `test_list_3_DataFrame.csv`

Each file stores a specific element of the list. To retrieve the list elements, `chillR` provides a function that reads these files and reassembles them into the original list format:

```{r}
test_list <- load_temperature_scenarios(path = "data",
                                        prefix = "test_list")
```

The function's name includes "temperature_scenarios" because it was primarily designed to avoid repeating the time-intensive generation of temperature scenarios, a key step in many `chillR` workflows. Although this function works with other lists as well, it currently has a limitation: single strings and numbers within lists are converted into mini-data frames, an issue that needs addressing.

## Hiding all this from the readers of our markdown file

After completing several lessons, the calculations in the document accumulate, leading to significant processing time, especially as more complex tasks are introduced. To avoid re-running code each time, calculations are typically performed once, and the results are saved. During the document knitting process, the saved data is reloaded.

However, the code can still be visible, even though it may not be executed. This is possible because code chunks in a Markdown document can be configured to either run but remain hidden or be visible without being executed. Key options include:

-   `echo = FALSE`: Runs the code but hides it (and any output) in the final document, while "side effects" like figures are still shown.

-   `eval = FALSE`: Shows the code in the final document but doesn’t execute it.

-   `include = FALSE`: Hides code and results in the final file, but runs the code.

-   `message = FALSE`: Hides messages generated by the code.

-   `warning = FALSE`: Hides warnings generated by the code.

This strategy requires ensuring that all necessary data for later code chunks is preloaded.

<!--chapter:end:13-saving.Rmd-->

# Historic temperature scenarios

```{r load_packages_hist_temp, echo=FALSE, message=FALSE}
require(chillR)
require(tidyverse)
require(kableExtra)
Temperature_scenarios <- read_tab("data/Temperature_scenarios.csv") %>%
  mutate(Date = as.Date(ISOdate(2000,
                                Month,
                                Day)))
Bonn_temps <- 
  read_tab("data/Bonn_temps.csv")
chill_hist_scenario_list <- 
  load_temperature_scenarios("data","Bonn_hist_chill_305_59")

```

## Learning goals for this lesson

-   Understand how we can include temperature scenarios while generating synthetic weather
-   Be able to produce temperature scenarios with arbitrary change scenarios imposed
-   Understand the difference between absolute and relative temperature scenarios and the importance of baselines (for relative scenarios)
-   Learn how to produce temperature scenarios that represent past points in time from historic temperature records
-   Learn how to produce synthetic temperature scenarios for past points in time and efficiently compute agroclimatic metrics for them

## Climate change scenarios

A weather generator accessed through `chillR` can produce agroclimatic profiles for specific locations. Calibration with historical temperature data makes the generated profile representative of the climate during the calibration period. This weather generator can also simulate various climate scenarios using the `temperature_scenario` parameter in the `temperature_generation` function.

The `temperature_scenario` parameter requires a `data.frame` with columns `Tmin` and `Tmax`, each containing 12 values that specify monthly adjustments to the mean minimum and maximum temperatures. Without this parameter, the function defaults to a `data.frame` with zero adjustments, indicating no change.

A simple climate change scenario can be created by adding arbitrary values to each month’s `Tmin` and `Tmax:`

```{r first_arbitrary_change_scenarios, eval=FALSE}
# Here's the call from the earlier lesson. We don't have to run this again.
Temp <- temperature_generation(KA_weather,
                               years = c(1998,2005),
                               sim_years = c(2001,2100))
 
# Now we make a temperature scenario that raises all temperatures by 2°C
```

```{r , eval=FALSE}
change_scenario <- data.frame(Tmin = rep(2,12),
                              Tmax = rep(2,12))

change_scenario
```

```{r load_and_show, echo=FALSE}
change_scenario <- data.frame(Tmin = rep(2,12),
                              Tmax = rep(2,12))

kable(change_scenario, align = c("c"))  %>%
  kable_styling("striped", position = "left",font_size = 14)
```

```{r carry_on, eval=FALSE}
Temp_2 <- temperature_generation(KA_weather,
                                 years = c(1998,2005),
                                 sim_years = c(2001,2100),
                                 temperature_scenario = change_scenario)

Temperature_scenarios <- KA_weather %>%
  filter(Year %in% 1998:2005) %>%
  cbind(Data_source = "observed") %>%
  rbind(Temp[[1]] %>% 
          select(c(Year, Month, Day, Tmin, Tmax)) %>% 
          cbind(Data_source = "simulated")
        ) %>%
  rbind(Temp_2[[1]] %>%
          select(c(Year, Month, Day, Tmin, Tmax)) %>% 
          cbind(Data_source = "Warming_2C")
        ) %>%
  mutate(Date = as.Date(ISOdate(2000,
                                Month,
                                Day)))

```

Due to the identical structure of this dataset to the one used in the previous lesson, the same code can be applied with only the `data.frame` name needing adjustment:

```{r weather_generator_set_up_plot_scenarios, message = FALSE}
ggplot(data = Temperature_scenarios, 
       aes(Date,Tmin)) +
  geom_smooth(aes(colour = factor(Year))) +
  facet_wrap(vars(Data_source)) +
  theme_bw(base_size = 20) +
  theme(legend.position = "none") +
  scale_x_date(date_labels = "%b")

ggplot(data = Temperature_scenarios,
       aes(Date,Tmax)) +
  geom_smooth(aes(colour = factor(Year))) +
  facet_wrap(vars(Data_source)) +
  theme_bw(base_size = 20) +
  theme(legend.position = "none") +
  scale_x_date(date_labels = "%b")

```

The scenario is simplified, with future changes uniformly distributed across all months—an approach that doesn’t reflect historical patterns and is unlikely for the future. However, this method closely resembles an early attempt at modeling climate change scenarios, as illustrated in the figure from the initial publication on chilling.

![Chill scenarios for a mountain oasis in Oman according ([Luedeling et al., 2009b)](https://link.springer.com/article/10.1007/s10584-009-9581-7)(images/Luedeling_JPG_Figure_10_Future_chilling (1).jpg)

To create more realistic scenarios, specific years from historical records are used, considering both the expected typical temperature conditions of the time and actual recorded data. This approach clarifies historical climate trends by focusing on gradual climate shifts, avoiding distortions from random annual variations or extreme outliers that might obscure trend analysis.

## Making historic temperature scenarios

A long-term dataset is essential for this exercise, and the process of obtaining and preparing this data for use in `chillR` has already been covered.

```{r getting_ready_for_historic_scenarios, eval=FALSE}
# download weather station list for the vicinity of Bonn
station_list <- handle_gsod(action = "list_stations",
                            location=c(7.1,50.8))

# download weather data for Cologne/Bonn airport and convert it to chillR format
Bonn_weather <- handle_gsod(action = "download_weather",
                            location = station_list$chillR_code[1],
                            time_interval = c(1973,2019)) %>%
  handle_gsod()

# check record for missing data
fix_weather(Bonn_weather$`KOLN BONN`)$QC

# (incidentally almost all gaps are for years covered by the KA_weather dataset)
Bonn_patched <- patch_daily_temperatures(
  weather = Bonn_weather$`KOLN BONN`,
  patch_weather = list(KA_weather))

fix_weather(Bonn_patched)$QC

# There are still 4 days missing here, out of 47 years -
# let's simply interpolate these gaps now

Bonn <- fix_weather(Bonn_patched)

Bonn_temps <- Bonn$weather

```

To create weather scenarios representing specific years like 1980, 1990, 2000, and 2010, despite using data centered around the median year of 1996 (due to gradual temperature increases from 1973 to 2019), adjustments must be made to account for historical changes. The process involves analyzing the historical temperature record to quantify shifts between the **baseline** year (the reference year of the dataset) and each target **simulation year**.

The `chillR` package provides a function, `temperature_scenario_from_records`, designed for this purpose. This function allows for the creation of temperature scenarios based on past records, facilitating the generation of simulations that accurately reflect historical conditions for specified years:

```{r temp_scen_from_records_1}
scenario_1980 <- temperature_scenario_from_records(weather = Bonn_temps,
                                                   year = 1980)
```

This scenario involves several components:

-   `data`: A data frame containing the minimum (`Tmin`) and maximum (`Tmax`) temperatures that are representative of the year of interest.

-   `scenario_year`: The specific year for which the scenario is being created.

-   `reference_year`: This would be used if the scenario involved temperature changes relative to another year, but in this case, it is set to NA since we are working with absolute temperatures.

-   `scenario_type`: Indicates whether the scenario presents absolute temperatures for the year of interest (true in this case) or changes relative to another year.

-   `labels`: Additional information attached to the scenario, such as "running mean scenario," which provides further context or explanation about the nature of the scenario.

```{r temp_scen_from_records_2, eval=FALSE}
scenario_1980$'1980'$data

```

```{r , echo=FALSE}
kable(scenario_1980$'1980'$data, align = c("c")) %>%
  kable_styling("striped", position = "left",font_size = 14)

```

The warning message received when first running the weather generator highlighted a few important points:

-   *scenario doesn’t contain named elements - consider using the following element names: ‘data’, ‘reference_year’,‘scenario_type’,‘labels’*

-   *setting scenario_type to ‘relative’*

-   *Reference year missing - can’t check if relative temperature scenario is valid*

The warning message previously encountered highlighted the missing elements in the input data frame. Scenarios require more information, and when this is absent, the temperature generation function makes assumptions about how to proceed. Specifically, the **scenario_type** was automatically set to **‘relative’**, as the function had to determine whether the scenario represented absolute or relative temperatures. Additionally, the absence of a **reference_year** meant the function couldn't identify the baseline for the relative temperature scenario.

With the updated scenario, which now includes all necessary information - such as **data**, **scenario_year**, **reference_year**, **scenario_type**, and **labels** - these warnings should no longer occur. The temperature generation function can now be run correctly with this complete set of details:

```{r temp_gen_from_hist_scen, eval=FALSE}
temps_1980 <- temperature_generation(weather = Bonn_temps,
                                     years = c(1973,2019),
                                     sim_years = c(2001,2100),
                                     temperature_scenario = scenario_1980)
```

Despite providing all the necessary information in the updated scenario, a warning message was still encountered:

“Absolute temperature scenario specified - calibration weather record only used for simulating temperature variation, but not for the means”

The weather generator evaluated the calibration dataset by considering the variation of temperatures around the mean values. It then produced a new dataset with mean temperatures (for both **Tmin** and **Tmax**) for each month that correspond to the absolute temperature scenario provided. The variation in this generated dataset was similar to the observed temperature variation, which was the intended outcome.

To convert this absolute temperature scenario into a relative one, a baseline scenario needs to be specified for comparison. In this case, 1996 is chosen as the baseline year, since it represents the median year of the observed record. A scenario for that year can now be created as the reference for comparison:

```{r make_baseline_scenario, eval=FALSE}
scenario_1996 <- temperature_scenario_from_records(weather = Bonn_temps,
                                                   year = 1996)
```

The 1996 scenario is an absolute temperature scenario. To convert it to a relative change scenario, the `temperature_scenario_baseline_adjustment` function is used, which adjusts the temperatures based on the baseline scenario.

```{r baseline_adjustment_demo, eval=FALSE}

relative_scenario <- temperature_scenario_baseline_adjustment(
  baseline = scenario_1996,
  temperature_scenario = scenario_1980)

```

Now, a relative change scenario has been created, which contains the following elements:

-   `data`: A data frame with **Tmin** and **Tmax** columns, representing the relative changes in temperatures between 1996 and 1980 (all negative values, as 1980 was cooler than 1996).

-   `scenario_year`: 1980

-   `reference_year`: 1996

-   `scenario_type`: ‘relative’

-   `labels`: ‘running mean scenario’

The relative change scenario can now be applied similarly to previous scenarios:

```{r temp_gen_rel_scen, eval=FALSE}

temps_1980<-temperature_generation(weather = Bonn_temps,
                                   years = c(1973,2019),
                                   sim_years = c(2001,2100),
                                   temperature_scenario = relative_scenario)

```

This time, no warning message appeared because all the necessary information was provided to the `temperature_generation` function. Now, all the intended scenarios can be created easily, as most functions support vectors and lists, not just single values or data frames.

```{r make_many_past_scenarios, eval=FALSE}
all_past_scenarios <- temperature_scenario_from_records(
  weather = Bonn_temps,
  year = c(1980,
           1990,
           2000,
           2010))

adjusted_scenarios <- temperature_scenario_baseline_adjustment(
  baseline = scenario_1996,
  temperature_scenario = all_past_scenarios)

all_past_scenario_temps <- temperature_generation(
  weather = Bonn_temps,
  years = c(1973,2019),
  sim_years = c(2001,2100),
  temperature_scenario = adjusted_scenarios)

save_temperature_scenarios(all_past_scenario_temps, "data", "Bonn_hist_scenarios")

```

To calculate chill accumulation, instead of manually applying each function to the four generated temperature scenarios, the `tempResponse_daily_list` function from `chillR` can be used, as it automates the process. Additionally, a simple frost model will be created, with the Dynamic Model and GDH Model selected for evaluation.

```{r add_frost_model}

frost_model <- function(x)
  step_model(x,
             data.frame(
               lower=c(-1000,0),
               upper=c(0,1000),
               weight=c(1,0)))

models <- list(Chill_Portions = Dynamic_Model,
               GDH = GDH,
               Frost_H = frost_model)

```

```{r apply_tempResponse_daily_list_1, eval=FALSE}

chill_hist_scenario_list <- tempResponse_daily_list(all_past_scenario_temps,
                                                    latitude = 50.9,
                                                    Start_JDay = 305,
                                                    End_JDay = 59,
                                                    models = models)

```

To ensure the scenarios are available for later use in the lessons on **Making CMPI6 scenarios** and **Making CMIP5 scenarios with the ClimateWizard**, they should be saved with a clear file name that includes the place name and the start and end dates of the considered period. Before saving, any incomplete winters will be removed from the record to ensure the data is accurate and complete.

```{r}

chill_hist_scenario_list <- lapply(chill_hist_scenario_list,
                                   function(x) x %>%
                                     filter(Perc_complete == 100))

save_temperature_scenarios(chill_hist_scenario_list, "data","Bonn_hist_chill_305_59")

```

```{r condense_chill_list, warning=FALSE}

scenarios <- names(chill_hist_scenario_list)[1:4]

all_scenarios <- chill_hist_scenario_list[[scenarios[1]]] %>%
  mutate(scenario = as.numeric(scenarios[1]))

for (sc in scenarios[2:4])
 all_scenarios <- all_scenarios %>%
  rbind(chill_hist_scenario_list[[sc]] %>%
          cbind(
            scenario=as.numeric(sc))
        ) %>%
  filter(Perc_complete == 100)

# Let's compute the actual 'observed' chill for comparison
actual_chill <- tempResponse_daily_list(Bonn_temps,
                                        latitude=50.9,
                                        Start_JDay = 305,
                                        End_JDay = 59,
                                        models)[[1]] %>%
  filter(Perc_complete == 100)

ggplot(data = all_scenarios,
       aes(scenario,
           Chill_Portions,
           fill = factor(scenario))) +
  geom_violin() +
  ylab("Chill accumulation (Chill Portions)") +
  xlab("Scenario year") +
  theme_bw(base_size = 15) +
  ylim(c(0,90)) +
  geom_point(data = actual_chill,
             aes(End_year,
                 Chill_Portions,
                 fill = "blue"),
             col = "blue",
             show.legend = FALSE) +
  scale_fill_discrete(name = "Scenario",
                      breaks = unique(all_scenarios$scenario)) 

```

The observed chill data should also be saved for later use in the lessons on **Making CMPI6 scenarios** and **Making CMIP5 scenarios with the ClimateWizard**. This will allow for easy access and comparison when working with those scenarios in the future.

```{r}
write.csv(actual_chill,"data/Bonn_observed_chill_305_59.csv", row.names = FALSE)

```

The chill distribution scenarios for 1980, 1990, 2000, and 2010 show minimal change, suggesting that chill deficiencies in Bonn are unlikely in the near future.

So far, all annual scenarios were based on running means of **Tmin** and **Tmax**, which is a reasonable approach given the challenges of estimating temperatures in times of accelerating climate change. However, `chillR` also offers the option of using linear regression to determine representative temperatures for a specific historical scenario. A quick comparison will reveal how this method affects the scenarios.

```{r runn_mean_vs_regression}
temperature_means <- 
  data.frame(Year = min(Bonn_temps$Year):max(Bonn_temps$Year),
             Tmin = aggregate(Bonn_temps$Tmin,
                              FUN = "mean",
                              by = list(Bonn_temps$Year))[,2],
             Tmax=aggregate(Bonn_temps$Tmax,
                            FUN = "mean",
                            by = list(Bonn_temps$Year))[,2]) %>%
  mutate(runn_mean_Tmin = runn_mean(Tmin,15),
         runn_mean_Tmax = runn_mean(Tmax,15))


Tmin_regression <- lm(Tmin~Year,
                      temperature_means)

Tmax_regression <- lm(Tmax~Year,
                      temperature_means)

temperature_means <- temperature_means %>%
  mutate(regression_Tmin = Tmin_regression$coefficients[1]+
           Tmin_regression$coefficients[2]*temperature_means$Year,
           regression_Tmax = Tmax_regression$coefficients[1]+
           Tmax_regression$coefficients[2]*temperature_means$Year
         )

ggplot(temperature_means,
       aes(Year,
           Tmin)) + 
  geom_point() + 
  geom_line(data = temperature_means,
            aes(Year,
                runn_mean_Tmin),
            lwd = 2,
            col = "blue") + 
  geom_line(data = temperature_means,
            aes(Year,
                regression_Tmin),
            lwd = 2,
            col = "red") +
  theme_bw(base_size = 15) +
  ylab("Mean monthly minimum temperature (°C)")

ggplot(temperature_means,
       aes(Year,
           Tmax)) + 
  geom_point() + 
  geom_line(data = temperature_means,
            aes(Year,
                runn_mean_Tmax),
            lwd = 2,
            col = "blue") + 
  geom_line(data = temperature_means,
            aes(Year, 
                regression_Tmax),
            lwd = 2,
            col = "red") +
  theme_bw(base_size = 15) +
  ylab("Mean monthly maximum temperature (°C)")

```

There is a noticeable difference between the running mean function (blue) and the linear regression line (red). While the running mean provides a smoother approximation, the linear regression line offers a more direct trend. This difference can be significant, and it is expected to become more pronounced as climate change continues to affect temperature patterns.

## `Exercises` on generating historic temperature scenarios

Please document all results of the following assignments in your `learning logbook`.

1)  *For the location you chose for previous exercises, produce historic temperature scenarios representing several years of the historic record (your choice).*

```{r load_packages, echo=FALSE, message=FALSE, warning=FALSE}
require(chillR)
require(tidyverse)
require(kableExtra)
  read_tab("Yakima/Yakima_temps.csv")
chill_hist_scenario_list <- 
  load_temperature_scenarios("Yakima","Yakima_hist_chill_305_59")
```

```{r message=FALSE, warning=FALSE}
#get a list of close-by weather stations
station_list <- handle_gsod(action = "list_stations",
                            location = c(long = -120.5, lat = 46.6),
                            time_interval = c(1973, 2023))


#download data
Yakima_weather <- handle_gsod(action = "download_weather",
                            location = station_list$chillR_code[1],
                            time_interval = c(1973, 2023)) %>%
  handle_gsod()


# check record for missing data
fix_weather(Yakima_weather$`YAKIMA AIR TERMINAL/MCALSR FIELD AP`)$QC


# filling gaps
patch_weather <-
  handle_gsod(action = "download_weather",
              location = as.character(station_list$chillR_code[c(4, 6)]),
              time_interval = c(1973, 2023)) %>%
  handle_gsod()


Yakima_patched <- patch_daily_temperatures(
  weather = Yakima_weather$`YAKIMA AIR TERMINAL/MCALSR FIELD AP`,
  patch_weather = patch_weather)


fix_weather(Yakima_patched)$QC


Yakima_temps <- Yakima_patched$weather
```

```{r}
temperature_means <- 
  data.frame(Year = min(Yakima_temps$Year):max(Yakima_temps$Year),
             Tmin = aggregate(Yakima_temps$Tmin,
                              FUN = "mean",
                              by = list(Yakima_temps$Year))[,2],
             Tmax=aggregate(Yakima_temps$Tmax,
                            FUN = "mean",
                            by = list(Yakima_temps$Year))[,2]) %>%
  mutate(runn_mean_Tmin = runn_mean(Tmin,15),
         runn_mean_Tmax = runn_mean(Tmax,15))


Tmin_regression <- lm(Tmin~Year,
                      temperature_means)

Tmax_regression <- lm(Tmax~Year,
                      temperature_means)

temperature_means <- temperature_means %>%
  mutate(regression_Tmin = Tmin_regression$coefficients[1]+
           Tmin_regression$coefficients[2]*temperature_means$Year,
         regression_Tmax = Tmax_regression$coefficients[1]+
           Tmax_regression$coefficients[2]*temperature_means$Year
  )
```

```{r}
# plot mean monthly minimum temperature of 1973 to 2023
ggplot(temperature_means,
       aes(Year,
           Tmin)) + 
  geom_point() + 
  geom_line(data = temperature_means,
            aes(Year,
                runn_mean_Tmin),
            lwd = 2,
            col = "blue") + 
  geom_line(data = temperature_means,
            aes(Year,
                regression_Tmin),
            lwd = 2,
            col = "red") +
  theme_bw(base_size = 15) +
  ylab("Mean monthly minimum temperature (°C)")
```

```{r}
# plot mean monthly maximum temperature of 1973 to 2023
ggplot(temperature_means,
       aes(Year,
           Tmax)) + 
  geom_point() + 
  geom_line(data = temperature_means,
            aes(Year,
                runn_mean_Tmax),
            lwd = 2,
            col = "blue") + 
  geom_line(data = temperature_means,
            aes(Year, 
                regression_Tmax),
            lwd = 2,
            col = "red") +
  theme_bw(base_size = 15) +
  ylab("Mean monthly maximum temperature (°C)")
```

2)  *Produce chill distributions for these scenarios and plot them.*

```{r}
scenario_1980 <- temperature_scenario_from_records(weather = Yakima_temps,
                                                   year = 1980)



temps_1980 <- temperature_generation(weather = Yakima_temps,
                                     years = c(1973, 2023),
                                     sim_years = c(2001, 2100),
                                     temperature_scenario = scenario_1980)

scenario_1998 <- temperature_scenario_from_records(weather = Yakima_temps,
                                                   year = 1998)

relative_scenario <- temperature_scenario_baseline_adjustment(
  baseline = scenario_1998,
  temperature_scenario = scenario_1980)

temps_1980 <- temperature_generation(weather = Yakima_temps,
                                   years = c(1973, 2023),
                                   sim_years = c(2001,2100),
                                   temperature_scenario = relative_scenario)

all_past_scenarios <- temperature_scenario_from_records(
  weather = Yakima_temps,
  year = c(1980,
           1990,
           2000,
           2010, 
           2020))

adjusted_scenarios <- temperature_scenario_baseline_adjustment(
  baseline = scenario_1998,
  temperature_scenario = all_past_scenarios)

all_past_scenario_temps <- temperature_generation(
  weather = Yakima_temps,
  years = c(1973, 2023),
  sim_years = c(2001, 2100),
  temperature_scenario = adjusted_scenarios)

save_temperature_scenarios(all_past_scenario_temps, "Yakima", "Yakima_hist_scenarios")

frost_model <- function(x)
  step_model(x,
             data.frame(
               lower = c(-1000,0),
               upper  = c(0,1000),
               weight = c(1,0)))

models <- list(Chill_Portions = Dynamic_Model,
               GDH = GDH,
               Frost_H = frost_model)

chill_hist_scenario_list <- tempResponse_daily_list(all_past_scenario_temps,
                                                    latitude = 46.6,
                                                    Start_JDay = 305,
                                                    End_JDay = 59,
                                                    models = models)

chill_hist_scenario_list <- lapply(chill_hist_scenario_list,
                                   function(x) x %>%
                                     filter(Perc_complete == 100))

save_temperature_scenarios(chill_hist_scenario_list, "Yakima","Yakima_hist_chill_305_59")

scenarios <- names(chill_hist_scenario_list)[1:5]

all_scenarios <- chill_hist_scenario_list[[scenarios[1]]] %>%
  mutate(scenario = as.numeric(scenarios[1]))

for (sc in scenarios[2:5])
  all_scenarios <- all_scenarios %>%
  rbind(chill_hist_scenario_list[[sc]] %>%
          cbind(
            scenario=as.numeric(sc))
  ) %>%
  filter(Perc_complete == 100)

# Let's compute the actual 'observed' chill for comparison
actual_chill <- tempResponse_daily_list(Yakima_temps,
                                        latitude=46.6,
                                        Start_JDay = 305,
                                        End_JDay = 59,
                                        models)[[1]] %>%
  filter(Perc_complete == 100)
```

```{r}
ggplot(data = all_scenarios,
       aes(scenario,
           Chill_Portions,
           fill = factor(scenario))) +
  geom_violin() +
  ylab("Chill accumulation (Chill Portions)") +
  xlab("Scenario year") +
  theme_bw(base_size = 15) +
  ylim(c(0,90)) +
  geom_point(data = actual_chill,
             aes(End_year,
                 Chill_Portions,
                 fill = "blue"),
             col = "blue",
             show.legend = FALSE) +
  scale_fill_discrete(name = "Scenario",
                      breaks = unique(all_scenarios$scenario)) 
```

```{r, saving_observed_chill}
write.csv(actual_chill,"Yakima/Yakima_observed_chill_305_59.csv", row.names = FALSE)
```

<!--chapter:end:14-historic.Rmd-->

# Future temperature scenarios

## Learning goals for this lesson

-   Acquire background knowledge on some important concepts in climate change analysis
-   Understand how future climate projections are generated
-   Learn about the "generations" of climate models and greenhouse gas concentration scenarios
-   Appreciate that it's important to use up-to-date scenarios when you're working in this space

## Impacts of future climate change

Human-induced climate change has already left significant impacts on the climate, ecosystems, and agriculture, as confirmed by numerous studies. The situation is expected to worsen due to historically high greenhouse gas levels and ongoing emissions of about 40 Gt CO2-equivalents annually, with little sign of decline. Widespread ecosystem degradation further erodes the planet's resilience. While the future remains uncertain, including the success of mitigation efforts and ecosystems' capacity to adapt, climate change adaptation research seeks to address these uncertainties by focusing on exposure, sensitivity, and adaptive capacity.

![**Conceptual framework for evaluating vulnerability and adaptation to climate change (Luedeling, 2020)**](images/adaptation_challenge.png)

The framework for assessing climate change vulnerability and adaptation (Luedeling, 2020) includes **exposure** (expected future conditions), **sensitivity** (how the system responds), and **adaptive capacity** (the system's ability to adapt). Combining exposure and sensitivity helps estimate potential impacts, while adaptation aims to reduce sensitivity (e.g., selecting resilient cultivars) or increase adaptive capacity (e.g., implementing flexible management strategies).

Current work focuses mainly on **exposure**, as it develops future scenarios for orchards. While chill models incorporate aspects of tree sensitivity, this is only partial, so the analyses primarily align with exposure.

## Future climate scenarios

Future climate scenarios and methods for creating historical chill scenarios, including relevant technical details, have been covered. The next step involves accessing climate model predictions for the desired location.

Running climate models is a complex process, requiring precise setup of global climate models (GCMs), regional models (RCMs), and downscaling, with no consensus on a single correct approach. `chillR` functions are compatible with both high-quality and less precise scenarios. While advanced climate data or collaboration with experts is ideal, \`chillR\` includes tools to facilitate access to reliable climate databases.

### Some background on climate models and warming pathways

Until November 2023, the `ClimateWizard` database, maintained by Center for Tropical Agriculture (CIAT), was the best source of climate data available in `chillR`. Its main advantage over other sources is the ability to provide point-specific data for climate scenarios from multiple models, avoiding the need to process large-scale gridded datasets. This reduces data transfer bottlenecks, especially when dealing with Tmin and Tmax data for multiple months, models, scenarios, and downscaling methods. `ClimateWizard` simplifies this by delivering data for specified coordinates, making it a practical tool despite its limitations.Originally developed as a web tool by Dr. Evan Girvetz, `ClimateWizard` can also be accessed via an API, streamlining data retrieval without navigating through a web interface. However, the database does not include the latest climate scenarios, such as those from CMIP6, which represents the current state of climate science and incorporates updated Shared Socioeconomic Pathways (SSPs). These SSPs, released in 2021, are now the recommended scenarios for generating state-of-the-art projections. `ClimateWizard` remains based on the older CMIP5 models and the outdated Representative Concentration Pathways (RCPs), offering only two RCP scenarios that fail to encompass the full range of plausible future climates. While these RCP scenarios were useful initially, they are increasingly unsuitable for publishing credible results. Instructions for using `ClimateWizard` with CMIP5 data are still provided, but transitioning to CMIP6 data and SSP scenarios is strongly recommended for up-to-date analyses.

## `Exercises` on future temperature scenarios

Please document all results of the following assignments in your `learning logbook`.

1)  *Briefly describe the differences between the RCPs and the SSPs (you may have to follow some of the links provided above).*

The Representative Concentration Pathways (RCPs) and the Shared Socioeconomic Pathways (SSPs) are both scenario frameworks used for modeling future climate developments, but they differ in their methodology and focus.

-   **RCPs (Representative Concentration Pathways)**:

    -   Developed for the IPCC's 5th Assessment Report (AR5, published in 2014).

    -   Describe different possible levels of radiative forcing (W/m²) by 2100, meaning the direct effect of greenhouse gas emissions on the climate.

    -   Four main pathways: RCP2.6 (strong emissions reduction), RCP4.5 and RCP6.0 (moderate emissions), RCP8.5 (very high emissions, often referred to as "business as usual").

    -   A purely climate-science-based approach without considering socioeconomic developments.

-   **SSPs (Shared Socioeconomic Pathways)**:

    -   Developed for the IPCC's 6th Assessment Report (AR6, published in 2021).

    -   Combine socioeconomic developments with emission pathways to create different future scenarios.

    -   Five main scenarios (SSP1 to SSP5), representing various societal, economic, and political trajectories, such as sustainable development (SSP1) or continued reliance on fossil fuels (SSP5).

    -   Can be combined with different radiative forcing levels (e.g., SSP1-2.6 for sustainable development with low emissions or SSP5-8.5 for high emissions and fossil fuel dependence).

**Summary**:\
RCPs focus solely on emissions and their impact on radiative forcing, while SSPs incorporate socioeconomic factors, making them more comprehensive and flexible for climate projections. SSPs are considered more modern and detailed in representing possible climate futures.

<!--chapter:end:15-future.Rmd-->

# Making CMIP6 scenarios

```{r load_packages_CMIP6, echo=FALSE, message=FALSE, warning=FALSE}
require(chillR)
require(kableExtra)
require(tidyverse)
Bonn_temps<-read_tab("data/Bonn_temps.csv")
chill_hist_scenario_list<-load_temperature_scenarios("data","Bonn_hist_chill_305_59")

```

Running this code requires `chillR` version \>= 0.76, which is available on CRAN. Earlier versions should not be used for CMIP6 projections.

## Learning goals for this lesson

-   Learn how to download future temperature projections for a CMIP6 ensemble from the Copernicus database
-   Be able to produce synthetic temperature scenarios for an ensemble of future climate scenarios
-   Transfer knowledge you gained earlier to compute temperature-based agroclimatic metrics for all the historic temperature records, as well as for past and future temperature scenarios
-   Learn how to plot all your results in a concise manner

## Accessing gridded climate data from the Copernicus climate data store

Most future climate data comes in the form of grids, meaning that to obtain information for a specific location, large files containing data for other locations must often be downloaded. For CMIP5 scenarios, the `ClimateWizard` API allowed access to specific point locations. However, a similar tool is not available for CMIP6 scenarios, which are currently the most advanced. As a result, downloading gridded data may be necessary, though it is more cumbersome than using `ClimateWizard`.

The [Copernicus Climate Data Store (CDS)](https://cds.climate.copernicus.eu/), maintained by the European Centre for Medium-Range Weather Forecasts, offers a reliable source of future climate data. To access this data, an API key and user ID are required. Users must create a free account on the CDS website to obtain these credentials. After registration, navigate to the user profile to retrieve the User ID and Access Token. After agreeing to the Terms of Use, the code will work to download the data.

### Downloading future climate data

To download data using the `download_cmip6_ecmwfr` function, the download area must be specified. The data will be saved locally, so it is important to carefully choose the extent. A small extent can be selected if only data for a single station is needed, while a larger area should be chosen for multiple stations, especially if they are close together.

For example, for Bonn (located at approximately 7.1°E, 50.8°N), a small extent can be selected around this location. The extent is specified as a vector in the format:

`c(maximum latitude,minimum longitude,minimum latitude,maximum longitude)`

This allows for downloading the required data and storing it locally to avoid future downloads.

```{r}

location = c(7.1, 50.8)
area <- c(52, 6, 50, 8)
```

The `download_cmip6_ecmwfr` function includes a default set of models, which can be used for downloading data. In this case, data for SSP1 (ssp126) will be selected for the download. By using this function, it is possible to specify the models and scenarios needed, while focusing on the specific SSP scenario of interest for the analysis.

```{r , eval=FALSE}
download_cmip6_ecmwfr(
  scenarios = 'ssp126', 
  area =  area,
  user = 'write user id here',
  key = 'write key here',
  model = 'default',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 2015,
  year_end = 2100)
```

```{r, echo=FALSE, eval=FALSE}
download_cmip6_ecmwfr(
  scenarios = 'ssp126', 
  area =  area,
  user = 'd78103f2-834f-468c-94f0-8b7064c75df7',
  key = 'ac66d05a-e82b-42d1-9a8d-a94c1afb9fb9',
  model = 'default',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 2015,
  year_end = 2100)
```

After running the `download_cmip6_ecmwfr` function, several files are generated in a subfolder of the working directory, typically named `cmip6_downloaded`, unless the folder name is specified using the `path_download` parameter. If the function encounters models for which no data is available based on the provided specifications, a message about "Dropped models" appears. These models are blacklisted to prevent future download attempts, and a note is added to the download folder.

Once the initial download is successful, additional SSP scenarios can be downloaded by providing a vector of scenarios to the `scenarios` parameter. For example, SSP126, SSP245, SSP370, and SSP585 can be selected. Since the function automatically detects previously downloaded data, it will skip any files that are already present, allowing for efficient downloading without redundancy.

```{r, eval=FALSE}
download_cmip6_ecmwfr(
  scenarios = c("ssp126", "ssp245", "ssp370", "ssp585"), 
  area = area,
  user = 'write user id here'
  key = 'write key here',
  model = 'default',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 2015,
  year_end = 2100)
```

```{r, eval=FALSE, echo=FALSE}
download_cmip6_ecmwfr(
  scenarios = c("ssp126", "ssp245", "ssp370", "ssp585"), 
  area = area,
  user = 'd78103f2-834f-468c-94f0-8b7064c75df7',
  key = 'ac66d05a-e82b-42d1-9a8d-a94c1afb9fb9',
  model = 'default',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 2015,
  year_end = 2100)
```

### Generating change scenarios

The data downloaded so far, stored in the `cmip6_downloaded/52_6_50_8` folder, represents future climate projections starting from 2015. However, these data are based on coarse-resolution climate models, where each pixel averages large areas, potentially including diverse landscapes like rivers, mountains, and coastal regions. As a result, these projections may not accurately represent the local temperature conditions of a specific place.

For site-specific analyses, such coarse data might not be sufficient. Although landscape features significantly influence rainfall patterns, their effect on temperature dynamics is less pronounced. Therefore, while the future temperature data might not be directly usable, the temperature change projected by these models can still be useful. For example, if a model projects a 2°C increase in January temperatures for a future scenario, this change can be added to current temperature observations.

However, it's crucial to determine what "today" refers to when discussing climate change. The baseline temperature used for comparison affects the projected warming. A 2°C increase applied to a cooler climate from 1850 will be different from applying it to today's warmer conditions. In the `chillR` workflow for CMIP6 data, the default baseline period is 1986-2014, as this period is available in the Copernicus database, represents a good median year (2000), and is close to the 30 years commonly used by climate scientists for robust climatology.

To generate accurate change scenarios, it's essential to subtract the conditions during the baseline period from the future projections, ensuring that both the baseline and future data come from the same climate model. Historical data for the same models can be retrieved using the `download_baseline_cmip6_ecmwfr` function.

```{r, eval=FALSE}
download_baseline_cmip6_ecmwfr(
  area = area,
  user = 'write user id here'
  key = 'write key here',
  model = 'match_downloaded',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 1986, 
  year_end = 2014, 
  month = 1:12)
```

```{r, eval=FALSE, echo=FALSE}
download_baseline_cmip6_ecmwfr(
  area = area,
  user = 'd78103f2-834f-468c-94f0-8b7064c75df7',
  key = 'ac66d05a-e82b-42d1-9a8d-a94c1afb9fb9',
  model = 'match_downloaded',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 1986, 
  year_end = 2014, 
  month = 1:12)
```

By specifying `models = match_downloaded`, the function will automatically identify the downloaded models and retrieve the corresponding baseline data.

### Extracting data from the grids

The downloaded data is still in a gridded format, so the data for the specific location needs to be extracted. A small `data.frame` is created containing the station name and coordinates for Bonn. While additional stations could be added, only Bonn is considered in this case. The `extract_cmip6_data` function is then used to extract the data for the specified location.

```{r, eval=FALSE}

station <- data.frame(
  station_name = c("Bonn"),
  longitude = c(7.1),
  latitude = c(50.8))

extracted <- extract_cmip6_data(stations = station)

```

```{r, eval=FALSE, echo=FALSE}

station <- data.frame(
  station_name = c("Bonn"),
  longitude = c(7.1),
  latitude = c(50.8))

extracted <- extract_cmip6_data(stations = station)

write.csv(extracted$`ssp126_AWI-CM-1-1-MR`, "data/extract_example_ssp126_AWI-CM-1-1-MR.csv", row.names = FALSE)

```

Let's look at some of the data for one of the climate models.

```{r, eval=FALSE}
head(extracted$`ssp126_AWI-CM-1-1-MR`)
```

```{r, echo=FALSE, eval=FALSE}

station <- data.frame(
  station_name = c("Bonn"),
  longitude = c(7.1),
  latitude = c(50.8))

extracted <- extract_cmip6_data(stations = station)

```

```{r, echo=FALSE}
show_example<-read.csv("data/extract_example_ssp126_AWI-CM-1-1-MR.csv")

kable(head(show_example), align = c("c"))  %>%
  kable_styling("striped", position = "left",font_size = 14)
```

Since the baseline data is stored in the same folder, a compact call can now be used to generate change scenarios for all the climate projections in that folder.

```{r, eval=FALSE}

change_scenarios <- gen_rel_change_scenario(extracted)
head(change_scenarios)

```

```{r, eval=FALSE, echo=FALSE}

change_scenarios <- gen_rel_change_scenario(extracted)
write.csv(change_scenarios, "data/all_change_scenarios.csv", row.names = FALSE)

```

```{r, echo=FALSE}

change_scenarios <- read.csv("data/all_change_scenarios.csv")

scen_list <- convert_scen_information(change_scenarios)

kable(head(change_scenarios))  %>%
  kable_styling("striped", position = "left",font_size = 10)


```

The data is stored in a single long data frame, which can easily be saved as a `csv` file. While this format isn't directly compatible with the weather generator, `chillR` provides a conversion function called `convert_scen_information`. This function extracts the relevant information and creates scenarios that are compatible with the `temperature_generation` function.

```{r, eval=FALSE}
write.csv(change_scenarios, "data/all_change_scenarios.csv", row.names = FALSE)

scen_list <- convert_scen_information(change_scenarios)

```

The `convert_scen_information` function can also be used to convert the list back into the data.frame format, making it easier to work with for further analysis or processing.

```{r, eval=FALSE}
scen_frame <- convert_scen_information(scen_list)
```

Let's look at one of the elements in the scenario lists.

```{r}
scen_list$Bonn$ssp126$`ACCESS-CM2`$'2050'

```

The scenario includes changes for the mean daily minimum and maximum temperatures for each month, along with several key pieces of information. The `scenario` specifies the SSP scenario, `scenario_type` indicates that this is a relative scenario, and `labels` contain the name of the GCM. The four attributes related to years—`start_year`, `end_year`, and `scenario_year`—define the time slice for the scenario. In this case, the scenario covers a 31-year period (2035-2065), with the median year (2050) being the `scenario_year`, which represents the year most typical for the scenario. The `reference_year` is also crucial, as it specifies the year to which the temperature change is relative—in this case, the changes are relative to conditions in 2000.

### Baseline adjustment

To produce future scenarios, while ensuring that the weather generator simulates temperatures for Bonn specifically, the observed data from Bonn can be used as input. This approach ensures that the temperatures generated reflect local conditions, rather than the average conditions from a GCM pixel. The weather generator can accommodate this, so the next step is to test it with one of the scenarios.

```{r, echo=FALSE}
Bonn_temps<-read_tab("data/Bonn_temps.csv")

```

```{r, eval=FALSE}
Bonn_temps<-read_tab("data/Bonn_temps.csv")

temperature_generation(Bonn_temps, 
                       years = c(1973, 2019),
                       sim_years = c(2001, 2100),
                       scen_list$Bonn$ssp126$`ACCESS-CM2`)

```

The attempt to use the weather generator with the scenario failed due to a mismatch in the reference years. The reference year for the weather station data is 1996, while the future projections have a reference year of 2000. One solution would be to use fewer observation years, such as from 1981 onward, to align the median year with 2000. However, if it's important to include older data, `chillR` provides a tool to adjust the reference year based on temperature trends within the observed dataset.

To adjust the baseline of the observed weather, the first step is to calculate the warming (or cooling) that occurred in Bonn between 1996 and 2000. This can be done by determining typical temperature conditions for both years using the `temperature_scenario_from_records` function.

```{r }
temps_1996 <- temperature_scenario_from_records(Bonn_temps,
                                                1996)

temps_2000 <- temperature_scenario_from_records(Bonn_temps,
                                                2000)

temps_1996

temps_2000

```

The temperature scenarios for 1996 and 2000 are absolute scenarios, describing typical temperature conditions for each year. Using these, a relative change scenario can now be computed to represent the temperature changes that occurred over this period. This is done with the `temperature_scenario_baseline_adjustment` function, which adjusts the baseline based on the observed temperature trends between the two years.

```{r}
base <- temperature_scenario_baseline_adjustment(temps_1996,
                                                 temps_2000)

base
```

The baseline correction can now be applied to the climate scenarios. Since this process currently only works with an unstructured list of scenarios, the scenarios must first be converted using the `give_structure = FALSE` option before applying the correction.

```{r}

scen_list <- convert_scen_information(change_scenarios, 
                                      give_structure = FALSE)

adjusted_list <- 
  temperature_scenario_baseline_adjustment(
    base,
    scen_list,
    temperature_check_args =
      list( scenario_check_thresholds = c(-5, 15)))

```

The temperature generation process can now begin. Given the large number of scenarios, it may take hours to complete. While this might seem like a long time, it’s important to remember that the computer is handling numerous operations that would otherwise be done manually.

```{r, eval = FALSE}
temps <- temperature_generation(Bonn_temps, 
                                years = c(1973, 2019), 
                                sim_years = c(2001, 2100), 
                                adjusted_list,    
                                temperature_check_args =
                                  list( scenario_check_thresholds = c(-5, 15)))

save_temperature_scenarios(temps,
                           "data/future_climate",
                           "Bonn_futuretemps")
```

It's important to save the data now to avoid waiting for the process to run again in the future.

Next, temperature responses can be calculated efficiently using the `tempResponse_daily_list` function. For this calculation, three models are selected: the Dynamic Model for chill accumulation, the GDH model for heat accumulation, and a simple model to compute frost hours.

```{r}
frost_model <- function(x)
  step_model(x,
             data.frame(
               lower = c(-1000, 0),
               upper = c(0, 1000),
               weight = c(1, 0)))

models <- list(Chill_Portions = Dynamic_Model,
               GDH = GDH,
               Frost_H = frost_model)

```

```{r, eval=FALSE}
chill_future_scenario_list <- tempResponse_daily_list(temps,
                                                    latitude = 50.8,
                                                    Start_JDay = 305,
                                                    End_JDay = 59,
                                                    models = models)

chill_future_scenario_list <- lapply(chill_future_scenario_list,
                                     function(x) x %>%
                                       filter(Perc_complete == 100))

save_temperature_scenarios(chill_future_scenario_list,
                           "data/future_climate",
                           "Bonn_futurechill_305_59")

```

```{r, echo=FALSE}
chill_future_scenario_list <- load_temperature_scenarios("data/future_climate",
                                                         "Bonn_futurechill")
```

To facilitate plotting later, climate scenarios are first generated using the `make_climate_scenario` function in chillR. The plotting function will process a list of these scenarios. The starting point is a historic scenario, which includes both distributions for historical years and the observed chill. These records, ideally saved in the "Historic temperature scenarios" lesson, are now loaded for further use.

```{r}

chill_hist_scenario_list<-load_temperature_scenarios("data",
                                                     "Bonn_hist_chill_305_59")

observed_chill <- read_tab("data/Bonn_observed_chill_305_59.csv")


chills <- make_climate_scenario(
  chill_hist_scenario_list,
  caption = "Historical",
  historic_data = observed_chill,
  time_series = TRUE)

plot_climate_scenarios(
  climate_scenario_list = chills,
  metric = "Chill_Portions",
  metric_label = "Chill (Chill Portions)")

```

The function had two outcomes: it generated a plot and returned a brief list with the message "time series labels." While this message may not be of immediate interest, it will contain more relevant content later. To store the information without displaying the plot, the plotting command can be assigned to a new object (e.g., `info <- plot_climate_scenarios(...)`), allowing the function to produce the plot as a side effect.

The same process is repeated for all future climate scenarios. For each scenario, it is added to the chills object using the `make_climate_scenario` function’s `add_to` argument. Before proceeding, the data must first be sorted by the specific combinations of SSP and time.

```{r}

SSPs <- c("ssp126", "ssp245", "ssp370", "ssp585")
Times <- c(2050, 2085)

list_ssp <- 
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(2) %>%
  unlist()

list_gcm <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(3) %>%
  unlist()

list_time <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(4) %>%
  unlist()


for(SSP in SSPs)
  for(Time in Times)
    {
    
    # find all scenarios for the ssp and time
    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]
    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]
    if(SSP == "ssp126") SSPcaption <- "SSP1"
    if(SSP == "ssp245") SSPcaption <- "SSP2"
    if(SSP == "ssp370") SSPcaption <- "SSP3"
    if(SSP == "ssp585") SSPcaption <- "SSP5"    
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- chill %>% 
      make_climate_scenario(
        caption = c(SSPcaption,
                    Time_caption),
        add_to = chills)
}

```

At this point, all the necessary data and steps are in place to plot the results of the climate change analysis:

```{r}

info_chill <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "Chill_Portions",
    metric_label = "Chill (Chill Portions)",
    texcex = 1.5)
```

```{r}
info_heat <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "GDH",
    metric_label = "Heat (Growing Degree Hours)",
    texcex = 1.5)

```

```{r}
info_frost <-
 plot_climate_scenarios(
   climate_scenario_list=chills,
   metric="Frost_H",
   metric_label="Frost hours",
   texcex=1.5)
```

Now that everything is set up, the trends in chill and heat accumulation, as well as in frost hours for Klein-Altendorf, can be visualized. The function not only produces the plot but also returns supplementary information, which was stored in the `info` objects. By inspecting these objects, it is possible to see the names of the climate models for each subplot, along with the time series labels for the historic plot. Since the same models were used for each scenario, this information is repeated multiple times. To avoid redundancy, it’s best to review this data once, focusing on just one of the plots.

```{r , eval=FALSE}
info_chill[[2]]
```

```{r , echo=FALSE}
kable(info_chill[[2]], align = c("c"))  %>%
  kable_styling("striped", position = "left",font_size = 14)

```

The labels are not included directly in the plot to avoid overcrowding the visualization. Instead, each model is represented by a number, with a key provided in a separate table to explain these number codes.

If the design of the figure is not preferred, a customized version can be created later. Additionally, a more modern version of the plot, available in `chillR`, will be explored in later steps.

## `Exercises` on generating CMIP6 temperature scenarios

Please document all results of the following assignments in your `learning logbook`.

1)  *Analyze the historic and future impact of climate change on two agroclimatic metrics of your choice, for the location you've chosen for your earlier analyses.*

```{r, include=FALSE}
require(chillR)
require(tidyverse)
library(ncdf4)
library(PCICt)
Yakima_temps <- read_tab("Yakima/Yakima_temps.csv")
change_scenarios <- read.csv("Yakima/all_change_scenarios.csv")
temps <- load_temperature_scenarios("Yakima/future_climate","Yakima_future_")
```

```{r, eval=FALSE}

# set location
location = c(-120.5, 46.6)
area <- c(48, -122 , 45, -119)


# download scenarios 
download_cmip6_ecmwfr(
  scenarios = c("ssp126", "ssp245", "ssp370", "ssp585"), 
  area = c(49, -122 , 44, -118),
  user = 'd78103f2-834f-468c-94f0-8b7064c75df7',
  key = 'ac66d05a-e82b-42d1-9a8d-a94c1afb9fb9',
  model = 'default',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 2015,
  year_end = 2100)


# download baseline
download_baseline_cmip6_ecmwfr(
  area = c(49, -122 , 44, -118),
  user = 'd78103f2-834f-468c-94f0-8b7064c75df7',
  key = 'ac66d05a-e82b-42d1-9a8d-a94c1afb9fb9',
  model = 'match_downloaded',
  frequency = 'monthly',
  variable = c('Tmin', 'Tmax'),
  year_start = 1986, 
  year_end = 2014, 
  month = 1:12)


#
station <- data.frame(
  station_name = c("Yakima"),
  longitude = c(-120.5),
  latitude = c(46.6))

extracted <- extract_cmip6_data(stations = station,
                                download_path = "cmip6_downloaded/49_-122_44_-118")


change_scenarios <- gen_rel_change_scenario(extracted)

scen_list <- convert_scen_information(change_scenarios)

temps_1996 <- temperature_scenario_from_records(Yakima_temps,
                                                1996)

temps_2000 <- temperature_scenario_from_records(Yakima_temps,
                                                2000)

base <- temperature_scenario_baseline_adjustment(temps_1996,
                                                 temps_2000)

scen_list <- convert_scen_information(change_scenarios, 
                                      give_structure = FALSE)

adjusted_list <- temperature_scenario_baseline_adjustment(base,
                                                          scen_list,
                                                          temperature_check_args =
                                                            list(scenario_check_thresholds = c(-5, 15)))

for(scen in 1:length(adjusted_list))
{
  if(!file.exists(paste0("Yakima/future_climate/Yakima_future_",
                         scen,"_",
                         names(adjusted_list)[scen],".csv")) )
  {temp_temp <- temperature_generation(Yakima_temps,
                                       years = c(1973, 2019),
                                       sim_years = c(2001, 2100),
                                       adjusted_list[scen],  
                                       temperature_check_args = 
                                         list( scenario_check_thresholds = c(-5, 15)))
  write.csv(temp_temp[[1]],paste0("Yakima/future_climate/Yakima_future_",scen,"_",names(adjusted_list)[scen],".csv"),
            row.names=FALSE)
  print(paste("Processed object",scen,"of", length(adjusted_list)))
  
  
  }
  
}


frost_model <- function(x)
  step_model(x,
             data.frame(
               lower=c(-1000,0),
               upper=c(0,1000),
               weight=c(1,0)))

models <- list(Chill_Portions = Dynamic_Model,
               GDH = GDH,
               Frost_H = frost_model)

temps <- load_temperature_scenarios("Yakima/future_climate","Yakima_future_")

chill_future_scenario_list <- tempResponse_daily_list(temps,
                                                      latitude = 46.6,
                                                      Start_JDay = 305,
                                                      End_JDay = 59,
                                                      models = models)

chill_future_scenario_list <- lapply(chill_future_scenario_list,
                                     function(x) x %>%
                                       filter(Perc_complete == 100))

save_temperature_scenarios(chill_future_scenario_list,
                           "Yakima/future_climate",
                           "Yakima_futurechill_305_59")
chill_hist_scenario_list <- load_temperature_scenarios("Yakima",
                                                       "Yakima_hist_chill_305_59")

observed_chill <- read_tab("Yakima/Yakima_observed_chill_305_59.csv")


chills <- make_climate_scenario(
  chill_hist_scenario_list,
  caption = "Historic",
  historic_data = observed_chill,
  time_series = TRUE)

plot_climate_scenarios(
  climate_scenario_list = chills,
  metric = "Chill_Portions",
  metric_label = "Chill (Chill Portions)")

SSPs <- c("ssp126", "ssp245", "ssp370", "ssp585")
Times <- c(2050, 2085)

list_ssp <- 
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(2) %>%
  unlist()

list_gcm <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(3) %>%
  unlist()

list_time <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(4) %>%
  unlist()


for(SSP in SSPs)
  for(Time in Times)
  {
    
    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]
    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]
    if(SSP == "ssp126") SSPcaption <- "SSP1"
    if(SSP == "ssp245") SSPcaption <- "SSP2"
    if(SSP == "ssp370") SSPcaption <- "SSP3"  
    if(SSP == "ssp585") SSPcaption <- "SSP5"    
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- chill %>% 
      make_climate_scenario(
        caption = c(SSPcaption,
                    Time_caption),
        add_to = chills)
  }

# Plot chill hours
info_chill <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "Chill_Portions",
    metric_label = "Chill (Chill Portions)",
    texcex = 1.5)

# Plot Heat (Growing degree hours)
info_heat <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "GDH",
    metric_label = "Heat (Growing Degree Hours)",
    texcex = 1.5)


```

<!--chapter:end:16-cmip6.Rmd-->

# Making CMIP5 scenarios with the `ClimateWizard`


```{r load_packages_ClimateWizard, echo=FALSE, message=FALSE, warning=FALSE}
require(chillR)
require(kableExtra)
require(tidyverse)

Bonn_temps<-read_tab("data/Bonn_temps.csv")
chill_hist_scenario_list<-load_temperature_scenarios("data","Bonn_hist_chill_305_59")

```

***Note that this chapter only deals with CMIP5 climate scenarios. For methods to produce CMIP6 scenarios refer to the chapter on [Making CMIP6 scenarios]*** 

## Learning goals for this lesson {-#goals_ClimateWizard}

- Learn how to download future temperature projections from the ClimateWizard database
- Be able to produce synthetic temperature scenarios for an ensemble of future climate scenarios
- Transfer knowledge you gained earlier to compute temperature-based agroclimatic metrics for all the historic temperature records, as well as for past and future temperature scenarios
- Learn how to plot all your results in a concise manner

## Using the `ClimateWizard`

Here we'll use the `ClimateWizard` tool to download some future data, produced by the CMIP5 models for the RCPs 4.5 and 8.5 (these are the only scenarios that are available there). I should note here that the `ClimateWizard` service has been down from time to time, so the following code doesn't always work.

We'll first look at a small example to familiarize ourselves with the way this function works:

```{r getClimateWizard, eval=FALSE}
getClimateWizardData(coordinates = c(longitude = 10.61,
                                     latitude = 34.93),
                     scenario = "rcp45",
                     start_year = 2020,
                     end_year = 2050,
                     metric = c("CD18", "R02"),
                     GCMs = c("bcc-csm1-1", "BNU-ESM"))

```

This code only downloaded data for two GCMs and one RCP scenario. We're accessing two climate metrics:
  
  - "CD18" - cooling degree days (>18°C)
- "R02" - the annual number of wet days > 0.2 mm/day

We'll get mean values of all years from 2020 to 2050 for a position with longitude=10.61 and latitude=34.93 (somewhere in Tunisia).

These metrics aren't of interest to us, and we'd rather use data from Bonn than Tunisia. Also we'd like to have data for all the climate models in the dataset and for both RCPs (there are only two in the database unfortunately). Finally, since we want to use these data for producing temperature scenarios for our weather generator, we'd like to get information on expected future $T_{min}$ and $T_{max}$ values. Since the latter is a pretty standard objective in the context of `chillR`, the default option for the `metric` parameter is actually the string `monthly_min_max_temps`, which instructs the function to download all the information we need for the weather generator.

There is no automated function for downloading multiple RCPs or multiple points in time, but we can easily implement this using `for` loops. We still have to decide what baseline period we want to use for our analysis. 

The weather record from Bonn goes from 1973 to 2019. Unfortunately, the `ClimateWizard` database requires the baseline period to be a 20-year interval between 1950 and 2005. Since climate has been changing over the 71-year span we have data for, we may want to choose an interval towards the end of this period. Let's therefore select the 31-year interval between 1975 and 2005 (I'm using 31 years, because then the median year is an integer - this isn't necessary, but somehow more aesthetically pleasing than some .5 business). The median year is now 1990.

We'll save the temperature scenarios after download to make sure we don't have to download them again (this takes quite a while). 

```{r download_climate,eval=FALSE}

RCPs <- c("rcp45",
          "rcp85")
Times <- c(2050,
           2085)

for(RCP in RCPs)
  for(Time in Times)
  {start_year <- Time - 15
  end_year <- Time + 15
  clim_scen <-
    getClimateWizardData(
      c(longitude = 7.143,
        latitude = 50.866),
      RCP,
      start_year,
      end_year,
      temperature_generation_scenarios = TRUE,
      baseline =c(1975, 2005),
      metric = "monthly_min_max_temps",
      GCMs = "all")
  save_temperature_scenarios(clim_scen,
                             "data/ClimateWizard",
                             paste0("Bonn_futures_",Time,"_",RCP))}

```

So we selected a period from 1975 to 2005 as the baseline for the future climate data. We could now also restrict the observed weather data used for calibrating the weather generator to the same interval. That would make some sense, but let's say we want to use all of our observed data, which ranges from 1973 to 2019. The median of this period is 1996, which is different from 1990. So we need a baseline adjustment:

```{r baseline_adjustment_future}
scenario_1990 <- Bonn_temps %>%
  temperature_scenario_from_records(1990)
scenario_1996 <- Bonn_temps %>%
  temperature_scenario_from_records(1996)
adjustment_scenario <-
  temperature_scenario_baseline_adjustment(scenario_1996,
                                           scenario_1990)

print(adjustment_scenario)

```

We see that the reference year is now 1996. This is correct, because this scenario will be used for temperature generation, where we'll use the whole observed record for calibration. This adjustment scenario will have to be applied to all downloaded weather scenarios.

We first need to select the RCPs and scenario years for our analysis:
  
```{r RCP_Times}
RCPs <- c("rcp45",
          "rcp85")
Times <- c(2050,
           2085)
```

Now we load the saved scenarios again. Saving and loading wouldn't be absolutely necessary of course, but it really comes in handy when our computer crashes, we accidentally close RStudio etc. The `load_ClimateWizard_scenarios` function was specifically written to load such scenarios.

We'll also add the baseline adjustment and the temperature generation step here, and then save the generated weather:
  
```{r future_temperature_generation, eval=FALSE}

for(RCP in RCPs)
  for(Time in Times)
  {
    clim_scen <- load_ClimateWizard_scenarios(
      "data/climateWizard",
      paste0("Bonn_futures_",
             Time,
             "_",
             RCP))
    
    clim_scen_adjusted <-
      temperature_scenario_baseline_adjustment(
        baseline_temperature_scenario = adjustment_scenario,
        temperature_scenario = clim_scen)
    
    Temps <- temperature_generation(
      weather = Bonn_temps, 
      years = c(1973,
                2019),
      sim_years = c(2001,
                    2101),
      temperature_scenario = clim_scen_adjusted)
    
    save_temperature_scenarios(
      Temps,
      "data/Weather_ClimateWizard",
      paste0("Bonn_",
             Time,
             "_",
             RCP))
  }


``` 

This took quite a while, but now we have all the future temperature data we need for our scenario analysis, and all of this is saved to disk. So we don't have to run this code again.

Let's add some historic scenarios though. This is similar to what we've done before. I'll make scenarios corresponding to 1980, 1990, 2000 and 2010.

```{r future_scenarios_for_simulation, eval=FALSE}
all_past_scenarios <- temperature_scenario_from_records(
  weather = Bonn_temps,
  year = c(1980,
           1990,
           2000,
           2010))

adjusted_scenarios <- temperature_scenario_baseline_adjustment(
  baseline = scenario_1996,
  temperature_scenario = all_past_scenarios)

all_past_scenario_temps <- temperature_generation(
  weather = Bonn_temps,
  years = c(1973,
            2019),
  sim_years = c(2001,
                2101),
  temperature_scenario = adjusted_scenarios)

save_temperature_scenarios(
  all_past_scenario_temps,
  "data/Weather_ClimateWizard",
  "Bonn_historic")

```


Now all we have to do is follow the steps we used to make our historic scenarios. The `tempResponse_daily_list` function makes this reasonably easy. Let's first make a list of models we want to apply - I'm choosing the Dynamic Model for chill, the Growing Degree Hours model for heat, and a frost model here: 
  
```{r}

frost_model <- function(x)
  step_model(x,
             data.frame(
               lower=c(-1000,0),
               upper=c(0,1000),
               weight=c(1,0)))

models <- list(Chill_Portions = Dynamic_Model,
               GDH = GDH,
               Frost_H = frost_model)

```

Now let's first apply these models to the historic data, both the scenarios and the observed temperatures:

```{r make_historic_chill_scenarios, eval=FALSE}

Temps <- load_temperature_scenarios("data/Weather_ClimateWizard",
                                    "Bonn_historic")

chill_past_scenarios <-
  Temps %>%
  tempResponse_daily_list(
    latitude = 50.866,
    Start_JDay = 305,
    End_JDay = 59,
    models = models,
    misstolerance = 10)
chill_observed <- 
  Bonn_temps %>%
  tempResponse_daily_list(
    latitude = 50.866,
    Start_JDay = 305,
    End_JDay = 59,
    models = models,
    misstolerance = 10)

save_temperature_scenarios(chill_past_scenarios,
                           "data/chill_ClimateWizard",
                           "Bonn_historic")
save_temperature_scenarios(chill_observed,
                           "data/chill_ClimateWizard",
                           "Bonn_observed")

```

We'll later want to plot all our data. In `chillR`, this is most conveniently done by producing climate scenarios with the `make_climate_scenario` function. The plotting function we'll be using later then simply processes a list of such climate scenarios. Let's start with a historic scenario that contains both the distributions for historic years and the historically observed chill.


```{r load_and_plot_chill_scens}
chill_past_scenarios <- load_temperature_scenarios(
  "data/chill_ClimateWizard",
  "Bonn_historic")
chill_observed <- load_temperature_scenarios(
  "data/chill_ClimateWizard",
  "Bonn_observed")

chills <- make_climate_scenario(
  chill_past_scenarios,
  caption = "Historic",
  historic_data = chill_observed,
  time_series = TRUE)

plot_climate_scenarios(
  climate_scenario_list = chills,
  metric = "Chill_Portions",
  metric_label = "Chill (Chill Portions)")

```

As you can see, this function had two effects. It produced a plot and it returned a short list containing the message "time series labels". This message isn't too interesting now, but it will later contain more content. If we want to just store this information rather than immediately displaying it in our output, we can assign the plotting command to a new object (as in `info <- plot_climate_scenarios(...`). The function will then just produce its **side effect**, which is the plot itself.

Now we run through the same process for all the future climate scenarios. For each one, we add the climate scenario to the `chills` object (`make_climate_scenario` has an argument `add_to`, where we can specify that):

```{r calculate_future_temp_responses, eval=FALSE}
for(RCP in RCPs)
  for(Time in Times)
    {
    Temps <- load_temperature_scenarios(
      "data/Weather_ClimateWizard",
      paste0("Bonn_",
             Time,
             "_",
             RCP))
    chill <- Temps %>% 
      tempResponse_daily_list(
        latitude = 50.866,
        Start_JDay = 305,
        End_JDay = 59,
        models = models,
        misstolerance = 10)
    save_temperature_scenarios(
      chill,
      "data/chill_ClimateWizard",
      paste0("Bonn_",
             Time,
             "_",
             RCP))
}



```

We now load this again, make climate scenario the same way we did for the historic data, and add them to our `chills` list so that we can easily plot them:

```{r}
for(RCP in RCPs)
  for(Time in Times)
    {
    chill <- load_temperature_scenarios(
      "data/chill_ClimateWizard",
      paste0("Bonn_",
             Time,
             "_",
             RCP))
    if(RCP == "rcp45") RCPcaption <- "RCP4.5"
    if(RCP == "rcp85") RCPcaption <- "RCP8.5"
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- chill %>% 
      make_climate_scenario(
        caption = c(RCPcaption,
                    Time_caption),
        add_to = chills)
}

```

Now we have everything we need to plot the results of our climate change analysis:

```{r plot_full_climate_scenario_analysis}

info_chill <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "Chill_Portions",
    metric_label = "Chill (Chill Portions)",
    texcex = 1.5)

info_heat <-
  plot_climate_scenarios(
    climate_scenario_list = chills,
    metric = "GDH",
    metric_label = "Heat (Growing Degree Hours)",
    texcex = 1.5)

info_frost <- 
  plot_climate_scenarios(  
    climate_scenario_list=chills,
    metric="Frost_H",
    metric_label="Frost hours",
    texcex=1.5)


```

Now we can see the trends in chill and heat accumulation, as well as in frost hours for Klein-Altendorf. As we saw earlier, the function didn't only return the plot, but also some supplementary information, which we stored in the the `info...` objects in the latest code chunk. If you inspect these objects, you'll see that they contain the names of the climate models for each of the subplots (in addition to the **time series labels** for the historic plot). Since we used the same models each time, all information is listed multiple times. Let's only look at this once, and only for one of the plots:
  
```{r , eval=FALSE}
info_chill[[2]]
```

```{r , echo=FALSE}
kable(info_chill[[2]])  %>%
  kable_styling("striped", position = "left",font_size = 10)

```

These labels aren't provided directly in the plot, because they would take up quite a bit of space. Instead, each model is specified by a number, with the key to these number codes given in this table.

If you don't like the design of this figure, don't worry. We'll be making our own version soon (and there's also a more modern version in `chillR` now that we'll get to know later).


## `Exercises` on generating CMIP5 temperature scenarios {-#exercises_ClimateWizard}

Please document all results of the following assignments in your `learning logbook`.

1) Analyze the historic and future impact of climate change on three agroclimatic metrics of your choice, for the location you've chosen for your earlier analyses.

<!--chapter:end:17-cmip5.Rmd-->

# Plotting future scenarios

## Learning goals for this lesson

-   Learn how we can use the `ggplot2` package to make an appealing illustrations of the chill projection results

## Making attractive plots

The default function in `chillR` can generate plots for climate impact projections, but its design may not always meet user preferences. The current version of the `plot_climate_scenarios` function offers limited options for customizing the plot's appearance. This limitation is primarily due to the function being written before extensive use of `ggplot2`.

In this chapter, a similar figure will be recreated using `ggplot`, offering greater flexibility and customization. [Eduardo Fernandez](https://scholar.google.de/citations?hl=de&user=ibSma_AAAAAJ) has already developed a function for this, which is included in `chillR`. However, the process will be built up gradually here to demonstrate how complex plots can be constructed using `ggplot`. Eduardo’s code has been slightly modified for clarity, but the core ideas remain his.

First, all necessary packages will be installed and loaded. New packages introduced here are `ggpmisc` and `patchwork`.

Building on prior work, the outputs produced in the chapters ***Historic Temperature Scenarios*** and ***Making CMIP6 Scenarios*** will also need to be loaded.

```{r}
library(kableExtra)
library(chillR)
library(tidyverse)
library(ggpmisc)
library(patchwork)

chill_hist_scenario_list <- load_temperature_scenarios("data",
                                                       "Bonn_hist_chill_305_59")
actual_chill <- read_tab("data/Bonn_observed_chill_305_59.csv")

chill_future_scenario_list <- load_temperature_scenarios("data/future_climate","Bonn_futurechill_305_59")

chills <- make_climate_scenario(
  chill_hist_scenario_list,
  caption = "Historic",
  historic_data = actual_chill,
  time_series = TRUE)

SSPs <- c("ssp126", "ssp245", "ssp370", "ssp585")
Times <- c(2050, 2085)

list_ssp <- 
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(2) %>%
  unlist()

list_gcm <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(3) %>%
  unlist()

list_time <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(4) %>%
  unlist()


for(SSP in SSPs)
  for(Time in Times)
    {
    
    # find all scenarios for the ssp and time
    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]
    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]
    if(SSP == "ssp126") SSPcaption <- "SSP1"
    if(SSP == "ssp245") SSPcaption <- "SSP2"
    if(SSP == "ssp370") SSPcaption <- "SSP3"
    if(SSP == "ssp585") SSPcaption <- "SSP5"    
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- chill %>% 
      make_climate_scenario(
        caption = c(SSPcaption,
                    Time_caption),
        add_to = chills)
}
```

To work effectively with `ggplot`, data needs to be arranged in a single `data.frame` rather than the list format currently used for storing projection data. Each row of the `data.frame` should include details such as the GCM, SSP, and Year. The most straightforward way to achieve this is by iterating through the elements of the chill projection list, extracting the necessary information, and combining it into a new, long-format `data.frame`.

```{r}
# We'll first process the past scenarios (element 1 of the chills list).
# Within the data element, we have a list of multiple data.frames for
# the various past scenarios.
# Using a 'for' loop, we cycle through all these data.frames.

for(nam in names(chills[[1]]$data))
  {
   # Extract the data frame.
   ch <- chills[[1]]$data[[nam]]
   # Add columns for the new information we have to add and fill them.
   ch[,"GCM"] <- "none"
   ch[,"SSP"] <- "none"
   ch[,"Year"] <- as.numeric(nam)
   
   # Now check if this is the first time we've gone through this loop.
   # If this is the first time, the ch data.frame becomes the output
   # object (past_simulated).
   # If it is not the first time ('else'), we add the current data.frame
   # to the 'past_simulated' object
  if(nam == names(chills[[1]]$data)[1])
    past_simulated <- ch else
      past_simulated <- rbind(past_simulated,
                              ch)
  }

# We add another column called 'Scenario' and label all rows as 'Historical' 
past_simulated["Scenario"] <- "Historical"

kable(head(past_simulated))  %>%
  kable_styling("striped", position = "left",font_size = 14)
```

```{r}
# We'll want to add the historic observation too, so let's simplify the
# pointer to this information for easier use later

past_observed <- chills[[1]][["historic_data"]]
```

```{r, eval=FALSE}
head(past_observed)
```

```{r,  echo=FALSE}
kable(head(past_observed))  %>%
  kable_styling("striped", position = "left",font_size = 14)
```

The same process is now applied to the future data, transforming it into a long-format `data.frame` for further analysis and visualization.

```{r}
# Extract future data
for(i in 2:length(chills))
  for(nam in names(chills[[i]]$data))
    {ch <- chills[[i]]$data[[nam]]
     ch[,"GCM"] <- nam
     ch[,"SSP"] <- chills[[i]]$caption[1]
     ch[,"Year"] <- chills[[i]]$caption[2]
     if(i == 2 & nam == names(chills[[i]]$data)[1])
       future_data <- ch else
         future_data <- rbind(future_data,ch)
  }
```

```{r, eval=FALSE}
head(future_data)
```

```{r,  echo=FALSE}
kable(head(future_data))  %>%
  kable_styling("striped", position = "left",font_size = 14)
```

The data is now formatted for use with `ggplot` and includes three metrics: *Chill_Portions*, *GDH*, and *Frost_H*. While a function could be written to handle this process, the current approach focuses on making the code flexible enough to plot all three metrics without significant modifications. This is achieved by defining variables for the metric (`metric`) and its axis label (`axis_label`) to allow easy adjustments.

The complex plot to be created cannot be produced as a single `ggplot` plot. However, this is manageable since multiple plots can be combined into a compound figure using the `plot_layout` function from the `patchwork` package.

A challenge when combining plots with shared axes, such as a common y-axis, is that the axes may vary due to differing data ranges across plots. To address this, the `range` function is used to determine axis extents that are consistent and reasonable for all plots.

```{r}
metric <- "GDH"
axis_label <- "Heat (in GDH)"

# get extreme values for the axis scale

rng <- range(past_observed[[metric]],
             past_simulated[[metric]],
             future_data[[metric]])  
rng
```

The first plot can now be created, focusing on the past scenarios. This serves as the starting point for visualizing the data:

```{r}
past_plot <- ggplot() +
  geom_boxplot(data = past_simulated,
               aes_string("as.numeric(Year)",
                          metric,group="Year"),
               fill = "skyblue")
```

```{r}
past_plot
```

The general layout of this `ggplot` command should now be familiar. However, a specific detail requires attention: in many `ggplot` commands, variable names can be written directly (e.g., `group = Year`) without quotation marks, which is uncommon in R. Normally, strings must be quoted; otherwise, R interprets them as variable names and raises an error if the variable is undefined.

The developers of `ggplot2`, led by [Hadley Wickham](https://scholar.google.de/citations?hl=en&user=YA43PbsAAAAJ), have removed this requirement, simplifying code in many cases. However, this convenience can cause issues when actual variable names are needed in **ggplot2** calls. To address this, the `aes_string` function can be used instead of `aes`, as demonstrated in the example.

Additionally, the y-axis must accommodate not only the data from this plot but also the data from all future scenarios. This is achieved using the `range` (`rng`) determined earlier. The axis labels will also be customized, with the y-axis label set using the previously defined `axis_label`.

```{r}
past_plot <- past_plot +
  scale_y_continuous(
    limits = c(0, 
               round(rng[2] + rng[2]/10))) +
  labs(x = "Year", 
       y = axis_label)

past_plot
```

The plot for the past scenarios is now complete, but adjustments are needed to ensure consistent formatting with the future data. While the past scenarios involve only one plot, the future scenarios will include multiple plots, created using the `facet_wrap` function. This function automatically adds design elements to the plots.

To maintain a uniform layout across the entire figure, the single plot for the past scenarios will also be converted into a facet layout. Additionally, the black-and-white theme will be applied for consistency.

```{r}
past_plot <- past_plot +
  facet_grid(~ Scenario) +
  theme_bw(base_size = 15) 
  
past_plot
```

The final adjustments involve modifying the facet title and x-axis text. The facet title's background will be removed, and its text will be set to bold for clarity. The x-axis text will be angled to ensure proper display of all year labels, even if the text size is adjusted.

```{r}
past_plot <- past_plot +  
  theme(strip.background = element_blank(),
        strip.text = element_text(face = "bold"),
        axis.text.x = element_text(angle=45,
                                   hjust=1)) 

past_plot
```

The plot of past scenarios is now complete. Next, actual observations will be added to the plot. To distinguish them from the outliers in the box plots, the observations will be colored blue. This step is straightforward and will enhance the clarity of the visualization.

```{r}
# add historic data
past_plot <- past_plot +
  geom_point(data = past_observed,
             aes_string("End_year",
                        metric),
             col = "blue")

past_plot
```

With the past scenarios complete, attention shifts to the future scenarios. These will be organized into two plots: one for the two scenarios in 2050 and another for 2085. These plots will later be displayed as two groups of two plots each.

To maintain an organized structure and allow for potential expansion with additional scenarios, the two plots will be stored in a list. While not strictly necessary for this case, this approach provides flexibility for future modifications.

Before constructing the plots within the list, one will be assembled first as an example, starting with the year 2050.

```{r}
y <- 2050

future_2050 <- ggplot(data= future_data[which(future_data$Year==y),]) +
  geom_boxplot(aes_string("GCM", 
                          metric, 
                          fill = "GCM"))

future_2050
```

At this point, all SSPs are mixed together, and the y-axis is not easily readable. While the information is conveyed through the color scheme, the plot appears cluttered. To improve its appearance, padding will be added to the sides of the plots using the `expand` parameter, ensuring the plot doesn't look too crowded and enhancing its overall readability.

```{r}
future_2050 <- future_2050 +
  facet_wrap(vars(SSP), nrow = 1) +
   scale_x_discrete(labels = NULL,
                    expand = expansion(add = 1)) 
```

In this case, the axis limits also need to be adjusted to ensure consistency between the future and past plots. Additionally, the scenario year will be added to the plots for clarity. This is accomplished using the `geom_text_npc` function from the `ggpmisc` package. This will help in placing the year label appropriately on the plot.

```{r}
future_2050 <- future_2050 +
  scale_y_continuous(limits = c(0, 
                                round(round(1.1*rng[2])))) +
    geom_text_npc(aes(npcx = "center", 
                      npcy = "top",
                      label = Year),
                  size = 5)

future_2050
```

For the final adjustments, the black-and-white theme will be applied once again. To reduce clutter, all axis text and titles will be removed, as they don’t add significant value. The y-axis ticks will also be removed. The legend will be placed at the bottom, and the facet title will be formatted in the same way as in the past plot to maintain visual consistency.

```{r}
future_2050 <- future_2050 +
  theme_bw(base_size = 15) +
  theme(axis.ticks.y = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "bottom",
        legend.margin = margin(0,
                               0,
                               0,
                               0,
                               "cm"),
        legend.background = element_rect(),
        strip.background = element_blank(),
        strip.text = element_text(face = "bold"),
        legend.box.spacing = unit(0, "cm"),
        plot.subtitle = element_text(hjust = 0.5,
                                     vjust = -1,
                                     size = 15 * 1.05,
                                     face = "bold")) 

future_2050
```

The legend is currently too large, but it can be adjusted later.

Next, the procedure will be implemented in a loop to create a list of two plots: one for 2050 and one for 2085. This approach will allow for consistent plotting while maintaining flexibility for future adjustments.

```{r}
future_plot_list <- list()

time_points <- c(2050, 2085)

for(y in time_points)
{
  future_plot_list[[which(y == time_points)]] <-
    ggplot(data = future_data[which(future_data$Year==y),]) +
    geom_boxplot(aes_string("GCM",
                            metric,
                            fill="GCM")) +
    facet_wrap(vars(SSP), nrow = 1) +
    scale_x_discrete(labels = NULL,
                     expand = expansion(add = 1)) +
    scale_y_continuous(limits = c(0, 
                                  round(round(1.1*rng[2])))) +
    geom_text_npc(aes(npcx = "center",
                      npcy = "top", 
                      label = Year),
                  size = 5) +
    theme_bw(base_size = 15) +
    theme(axis.ticks.y = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank(),
          legend.position = "bottom",
          legend.margin = margin(0, 
                                 0, 
                                 0, 
                                 0, 
                                 "cm"),
          legend.background = element_rect(),
          strip.background = element_blank(),
          strip.text = element_text(face = "bold"),
          legend.box.spacing = unit(0, "cm"),
          plot.subtitle = element_text(
            hjust = 0.5,
            vjust = -1,
            size = 15 * 1.05,
            face = "bold")) 
}

future_plot_list
```

Combining the plots is straightforward. The plots can be joined using the `+` sign, which allows for easy layering and combination into a single cohesive figure.

```{r}
both_plots <- past_plot + future_plot_list

both_plots
```

The basic structure of the plots is in place, but a few adjustments are needed, particularly for the legend. The `plot_layout` function from the `patchwork` package is used for this purpose. It allows for the creation of patchwork figures by collecting all legends and removing duplicates, ensuring that only one version of the legend is displayed. Additionally, the width of the individual plots can be adjusted. To achieve this, a vector `c(1, 1.8, 1.8)` is used, specifying that each set of future plots should be 1.8 times the width of the past scenario plot. This provides a more balanced layout.

```{r}
plot <- both_plots +
           plot_layout(guides = "collect",
                       widths = c(1,rep(2,length(future_plot_list))))
```

With the previous adjustments, the plot is no longer clearly visible, so the legend will be moved to the bottom for better visibility. Due to how the `patchwork` package works, the corresponding theme call must be added after an `&` symbol to ensure the legend is positioned correctly at the bottom. This adjustment ensures the plot remains clear and the layout is consistent.

```{r}
plot <- plot & theme(legend.position = "bottom",
                     legend.text = element_text(size=8),
                     legend.title = element_text(size=10),
                     axis.title.x = element_blank())
```

## The results

The adjustments have resulted in clear and consistent `ggplot` figures displaying a heat analysis for Bonn. The figures are well-organized, with properly placed legends and axis labels, making them ready for presentation.

```{r}
plot
```

The process is now complete. The same procedure can be applied to the other metrics. By simply changing the `metric` and `axis_label` variables and running the code again, the same steps can be followed for the other metrics. The full code will not be shown here, but the approach should work seamlessly for these adjustments.

```{r, echo=FALSE}
         
metric <- "Chill_Portions"
axis_label <- "Chill (in CP)"

# get extreme values for the axis scale

rng <- range(past_observed[[metric]],
             past_simulated[[metric]],
             future_data[[metric]])  
past_plot <- ggplot() +
  geom_boxplot(data = past_simulated,
               aes_string("as.numeric(Year)",
                          metric,group="Year"),
               fill="skyblue") +
  scale_y_continuous(limits = c(0, 
                                round(round(1.1*rng[2])))) +
  labs(x = "Year", y = axis_label) +
  facet_grid(~ Scenario) +
  theme_bw(base_size = 15) +  
  theme(strip.background = element_blank(),
           strip.text = element_text(face = "bold"),
           axis.text.x = element_text(angle=45, hjust=1)) +
  geom_point(data = past_observed,
             aes_string("End_year",
                        metric),
             col="blue")

future_plot_list <- list()

for(y in time_points)
{
  future_plot_list[[which(y == time_points)]] <-
    ggplot(data = future_data[which(future_data$Year==y),]) +
    geom_boxplot(aes_string("GCM", 
                            metric, 
                            fill="GCM")) +
  facet_wrap(vars(SSP), nrow = 1) +
   scale_x_discrete(labels = NULL,
                    expand = expansion(add = 1)) +
  scale_y_continuous(limits = c(0,
                                round(round(1.1*rng[2])))) +
    geom_text_npc(aes(npcx = "center", 
                      npcy = "top", 
                      label = Year),
                  size = 5) +
  theme_bw(base_size = 15) +
  theme(axis.ticks.y = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "bottom",
        legend.margin = margin(0,
                               0, 
                               0,
                               0, 
                               "cm"),
        legend.background = element_rect(),
        strip.background = element_blank(),
        strip.text = element_text(face = "bold"),
        legend.box.spacing = unit(0, "cm"),
        plot.subtitle = element_text(hjust = 0.5, 
                                     vjust = -1,
                                     size = 15 * 1.05,
                                     face = "bold")) 
}

plot <- (past_plot +
           future_plot_list +
           plot_layout(guides = "collect",
                       widths = c(1,rep(2,length(future_plot_list))))
         ) & theme(legend.position = "bottom",
                   legend.text = element_text(size = 8),
                   legend.title = element_text(size = 10),
                   axis.title.x=element_blank())
plot

```

```{r, echo=FALSE}
         
metric <- "Frost_H"
axis_label <- "Frost duration (in hours)"

# get extreme values for the axis scale

rng <- range(past_observed[[metric]],
             past_simulated[[metric]],
             future_data[[metric]])  
past_plot <- ggplot() +
  geom_boxplot(data = past_simulated,
               aes_string("as.numeric(Year)",
                          metric,group="Year"),
               fill="skyblue") +
  scale_y_continuous(limits = c(0, 
                                round(round(1.1*rng[2])))) +
  labs(x = "Year", y = axis_label) +
  facet_grid(~ Scenario) +
  theme_bw(base_size = 15) +  
  theme(strip.background = element_blank(),
           strip.text = element_text(face = "bold"),
           axis.text.x = element_text(angle=45, hjust=1)) +
  geom_point(data = past_observed,
             aes_string("End_year",
                        metric),
             col="blue")

future_plot_list <- list()

for(y in time_points)
{
  future_plot_list[[which(y == time_points)]] <-
    ggplot(data = future_data[which(future_data$Year==y),]) +
    geom_boxplot(aes_string("GCM", 
                            metric, 
                            fill="GCM")) +
  facet_wrap(vars(SSP), nrow = 1) +
   scale_x_discrete(labels = NULL,
                    expand = expansion(add = 1)) +
  scale_y_continuous(limits = c(0,
                                round(round(1.1*rng[2])))) +
    geom_text_npc(aes(npcx = "center", 
                      npcy = "top", 
                      label = Year),
                  size = 5) +
  theme_bw(base_size = 15) +
  theme(axis.ticks.y = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "bottom",
        legend.margin = margin(0,
                               0, 
                               0,
                               0, 
                               "cm"),
        legend.background = element_rect(),
        strip.background = element_blank(),
        strip.text = element_text(face = "bold"),
        legend.box.spacing = unit(0, "cm"),
        plot.subtitle = element_text(hjust = 0.5, 
                                     vjust = -1,
                                     size = 15 * 1.05,
                                     face = "bold")) 
}

plot <- (past_plot +
           future_plot_list +
           plot_layout(guides = "collect",
                       widths = c(1,rep(2,length(future_plot_list))))
         ) & theme(legend.position = "bottom",
                   legend.text = element_text(size = 8),
                   legend.title = element_text(size = 10),
                   axis.title.x=element_blank())
plot
```

To create a generally applicable function for the entire processing workflow, some additional effort is required. This involves ensuring that the function produces appropriate warnings or errors when given incorrect inputs, and making it flexible enough to handle different types of data. However, if the function is only intended for use with data structured in the same way as the current dataset, it can be simplified. In this case, all the steps performed so far can be wrapped into a function call, streamlining the process.

```{r}
plot_scenarios_gg <- function(past_observed,
                              past_simulated,
                              future_data,
                              metric,
                              axis_label,
                              time_points)
{
  rng <- range(past_observed[[metric]],
               past_simulated[[metric]],
               future_data[[metric]])  
  past_plot <- ggplot() +
    geom_boxplot(data = past_simulated,
                 aes_string("as.numeric(Year)",
                            metric,
                            group="Year"),
                 fill="skyblue") +
    scale_y_continuous(limits = c(0, 
                                  round(round(1.1*rng[2])))) +
    labs(x = "Year", y = axis_label) +
    facet_grid(~ Scenario) +
    theme_bw(base_size = 15) +  
    theme(strip.background = element_blank(),
          strip.text = element_text(face = "bold"),
          axis.text.x = element_text(angle=45, 
                                     hjust=1)) +
    geom_point(data = past_observed,
               aes_string("End_year",
                          metric),
               col="blue")
  
  future_plot_list <- list()
  
  for(y in time_points)
  {
    future_plot_list[[which(y == time_points)]] <-
      ggplot(data = future_data[which(future_data$Year==y),]) +
      geom_boxplot(aes_string("GCM", 
                              metric, 
                              fill="GCM")) +
      facet_wrap(vars(SSP), nrow = 1) +
      scale_x_discrete(labels = NULL,
                       expand = expansion(add = 1)) +
      scale_y_continuous(limits = c(0, 
                                    round(round(1.1*rng[2])))) +
      geom_text_npc(aes(npcx = "center",
                        npcy = "top",
                        label = Year),
                    size = 5) +
      theme_bw(base_size = 15) +
      theme(axis.ticks.y = element_blank(),
            axis.text = element_blank(),
            axis.title = element_blank(),
            legend.position = "bottom",
            legend.margin = margin(0,
                                   0, 
                                   0, 
                                   0, 
                                   "cm"),
            legend.background = element_rect(),
            strip.background = element_blank(),
            strip.text = element_text(face = "bold"),
            legend.box.spacing = unit(0, "cm"),
            plot.subtitle = element_text(hjust = 0.5,
                                         vjust = -1,
                                         size = 15 * 1.05,
                                         face = "bold")) 
  }
  
  plot <- (past_plot +
             future_plot_list +
             plot_layout(guides = "collect",
                         widths = c(1,rep(2,length(future_plot_list))))
           ) & theme(legend.position = "bottom",
                     legend.text = element_text(size = 8),
                     legend.title = element_text(size = 10),
                     axis.title.x=element_blank())
  plot
  
}
```

By wrapping all the steps into a function, the same outputs can now be generated more quickly. This approach simplifies the process and allows for efficient reuse of the code without repeating all the individual commands.

```{r}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "GDH",
                  axis_label = "Heat (in Growing Degree Hours)",
                  time_points = c(2050, 2085))
```

```{r}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "Chill_Portions",
                  axis_label = "Chill (in Chill Portions)",
                  time_points = c(2050, 2085))
```

```{r}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "Frost_H",
                  axis_label = "Frost duration (in hours)",
                  time_points = c(2050, 2085))
```

## `Exercises` on plotting future projections

1.  Produce similar plots for the weather station you selected for earlier exercises.

```{r}
chill_hist_scenario_list <- load_temperature_scenarios("Yakima",
                                                       "Yakima_hist_chill_305_59")
actual_chill <- read_tab("Yakima/Yakima_observed_chill_305_59.csv")

chill_future_scenario_list <- load_temperature_scenarios("Yakima/future_climate","Yakima_futurechill_305_59")

chills <- make_climate_scenario(
  chill_hist_scenario_list,
  caption = "Historic",
  historic_data = actual_chill,
  time_series = TRUE)

SSPs <- c("ssp126", "ssp245", "ssp370", "ssp585")
Times <- c(2050, 2085)

list_ssp <- 
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(2) %>%
  unlist()

list_gcm <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(3) %>%
  unlist()

list_time <-
  strsplit(names(chill_future_scenario_list), '\\.') %>%
  map(4) %>%
  unlist()


for(SSP in SSPs)
  for(Time in Times)
    {
    
    # find all scenarios for the ssp and time
    chill <- chill_future_scenario_list[list_ssp == SSP & list_time == Time]
    names(chill) <- list_gcm[list_ssp == SSP & list_time == Time]
    if(SSP == "ssp126") SSPcaption <- "SSP1"
    if(SSP == "ssp245") SSPcaption <- "SSP2"
    if(SSP == "ssp370") SSPcaption <- "SSP3"
    if(SSP == "ssp585") SSPcaption <- "SSP5"    
    if(Time == "2050") Time_caption <- "2050"
    if(Time == "2085") Time_caption <- "2085"
    chills <- chill %>% 
      make_climate_scenario(
        caption = c(SSPcaption,
                    Time_caption),
        add_to = chills)
}
```

```{r}
for(nam in names(chills[[1]]$data))
  {
   # Extract the data frame.
   ch <- chills[[1]]$data[[nam]]
   # Add columns for the new information we have to add and fill them.
   ch[,"GCM"] <- "none"
   ch[,"SSP"] <- "none"
   ch[,"Year"] <- as.numeric(nam)
   
   # Now check if this is the first time we've gone through this loop.
   # If this is the first time, the ch data.frame becomes the output
   # object (past_simulated).
   # If it is not the first time ('else'), we add the current data.frame
   # to the 'past_simulated' object
  if(nam == names(chills[[1]]$data)[1])
    past_simulated <- ch else
      past_simulated <- rbind(past_simulated,
                              ch)
  }

# We add another column called 'Scenario' and label all rows as 'Historical' 
past_simulated["Scenario"] <- "Historical"
```

```{r}
for(i in 2:length(chills))
  for(nam in names(chills[[i]]$data))
    {ch <- chills[[i]]$data[[nam]]
     ch[,"GCM"] <- nam
     ch[,"SSP"] <- chills[[i]]$caption[1]
     ch[,"Year"] <- chills[[i]]$caption[2]
     if(i == 2 & nam == names(chills[[i]]$data)[1])
       future_data <- ch else
         future_data <- rbind(future_data,ch)
  }
```

```{r}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "Frost_H",
                  axis_label = "Frost duration (in hours)",
                  time_points = c(2050, 2085))
```

```{r}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "Chill_Portions",
                  axis_label = "Chill (in Chill Portions)",
                  time_points = c(2050, 2085))
```

```{r}
plot_scenarios_gg(past_observed = past_observed,
                  past_simulated = past_simulated,
                  future_data = future_data,
                  metric = "GDH",
                  axis_label = "Heat (in Growing Degree Hours)",
                  time_points = c(2050, 2085))
```

<!--chapter:end:18-plotting.Rmd-->

# Why PLS doesn’t always work

## Learning goals

-   Understand why PLS regression sometimes doesn’t perform as expected
-   Be able to make temperature response plots for agroclimatic metrics
-   Be able to relate temperatures at a study site to temperature responses of models to anticipate whether PLS analysis will be effective

## Disappointing PLS performance

PLS regression makes it difficult to distinguish temperature response phases, especially during the chilling period. Replacing temperatures with agroclimatic metrics did not fully resolve this issue. Expected patterns often appeared only at the beginning and end of the presumed endodormancy phase, with a large gap in between. To understand this, it is important to consider the limitations of the applied analysis method.

## What PLS regression can find

PLS regression identifies relationships between a target variable and variations in input variables, requiring a monotonic response, meaning the correlation must remain consistently positive or negative. Additionally, successful PLS analysis depends on sufficient variation in the independent variables, as meaningful responses cannot be detected without it.

In this case, a constant level of chill effectiveness over a certain period may prevent PLS regression from producing useful results. This could explain its limited performance, particularly in cold regions such as Klein-Altendorf or Beijing, where winter chill accumulation shows little variation.

To investigate this, the model’s response to temperature will be visualized. The analysis is based on the Dynamic Model and examines its reaction to daily temperature patterns. Instead of single-day calculations, the average daily chill accumulation rate over extended periods will be considered, as the model often requires multiple days to accumulate a Chill Portion.

```{r}
library(chillR)

mon <- 1 # Month
ndays <- 31 # Number of days per month
tmin <- 1
tmax <- 8
latitude <- 50


weather <- make_all_day_table(
            data.frame(Year = c(2001, 2001),
                       Month = c(mon, mon),
                       Day = c(1, ndays),
                       Tmin = c(0, 0),
                       Tmax = c(0, 0))) %>%
  mutate(Tmin = tmin,
         Tmax = tmax)

hourly_temps <- stack_hourly_temps(weather,
                                   latitude = latitude)

CPs <- Dynamic_Model(
     hourly_temps$hourtemps$Temp)

daily_CPs <- CPs[length(CPs)] / nrow(weather)

daily_CPs

```

To introduce more flexibility into the code, an alternative way to execute a function is used. Previously, the `Dynamic_Model` function was called directly, but to allow different models to be used dynamically, the `do.call` function is applied. This approach enables passing the model as a parameter, making it possible to switch between models like Dynamic_Model and GDH by assigning them to the `temp_model` variable. The `list(...)` inside `do.call` contains the function arguments, ensuring adaptability.

So far, calculations were performed for a single Tmin/Tmax combination and only for one month. To extend the analysis, data will be generated for all months and a broader range of Tmin/Tmax values. This requires more complex code to handle generalization challenges, such as determining the number of days in each month.

```{r, eval=FALSE}
library(chillR)

latitude <- 50.6

month_range <- c(10, 11, 12, 1, 2, 3)

Tmins <- c(-20:20)
Tmaxs <- c(-15:30)

mins <- NA
maxs <- NA
CP <- NA
month <- NA
temp_model <- Dynamic_Model

for(mon in month_range)
    {days_month <- as.numeric(
      difftime( ISOdate(2002,
                        mon + 1,
                        1),
                ISOdate(2002,
                        mon,
                        1)))
    
    if(mon == 12) days_month <- 31
    
    weather <- make_all_day_table(
                data.frame(Year = c(2001, 2001),
                           Month = c(mon, mon),
                           Day = c(1, days_month),
                           Tmin = c(0, 0),
                           Tmax = c(0, 0)))
    
    for(tmin in Tmins)
      for(tmax in Tmaxs)
        if(tmax >= tmin)
          {
          hourtemps <- weather %>%
            mutate(Tmin = tmin,
                   Tmax = tmax) %>%
            stack_hourly_temps(latitude = latitude) %>% 
            pluck("hourtemps", "Temp")

          CP <- c(CP,
                  tail(do.call(temp_model,
                               list(hourtemps)), 1) /
                    days_month)
          mins <- c(mins, tmin)
          maxs <- c(maxs, tmax)
          month <- c(month, mon)
        }
}

results <- data.frame(Month = month,
                      Tmin = mins,
                      Tmax = maxs,
                      CP)

results <- results[!is.na(results$Month), ]


write.csv(results,
          "data/model_sensitivity_development.csv",
          row.names = FALSE)
```

```{r, eval=FALSE}
head(results)
```

```{r, echo=FALSE }
results <- read.csv("data/model_sensitivity_development.csv")
month_range <- c(10, 11, 12, 1, 2, 3)
library(kableExtra)
kable(head(results), align = c("c")) %>%
  kable_styling("striped", position = "left", font_size=14)
```

```{r}
latitude <- 50.6

month_range <- c(10, 11, 12, 1, 2, 3)

Tmins <- c(-20:20)
Tmaxs <- c(-15:30)
```

With the generated data, the temperature response curve can now be plotted, visualizing how the model reacts to different Tmin/Tmax combinations over time.

```{r, warning=FALSE, message=FALSE}

library(ggplot2)
library(colorRamps)

results$Month_names <- factor(results$Month,
                              levels = month_range,
                              labels = month.name[month_range])  

DM_sensitivity <- ggplot(results,
                         aes(x = Tmin,
                             y = Tmax,
                             fill = CP)) +
  geom_tile() +
  scale_fill_gradientn(colours = alpha(matlab.like(15),
                                       alpha = .5),
                       name = "Chill/day (CP)") +
  ylim(min(results$Tmax),
       max(results$Tmax)) +
  ylim(min(results$Tmin),
       max(results$Tmin))

DM_sensitivity
```

```{r}
DM_sensitivity <- DM_sensitivity +
  facet_wrap(vars(Month_names)) +
  ylim(min(results$Tmax),
       max(results$Tmax)) +
  ylim(min(results$Tmin),
       max(results$Tmin))

DM_sensitivity
```

The vibrant colors in the upper plot result from the `alpha` function in the `ggplot` call, which makes colors transparent. Since data points for six months are layered on top of each other, the overlapping transparency creates more intense colors than those in the legend.

This plot is specific to the given latitude, as daily temperature curves are influenced by sunrise and sunset times. The visualization reveals clear variations in chill effectiveness across the temperature spectrum, with no accumulation at extreme temperatures and a peak in the middle. A steep gradient in chill accumulation rates is also visible near the edges of the effective temperature range.

To assess the likelihood of PLS regression performing well under these climatic conditions, temperatures can be plotted over this diagram. Since the plot corresponds to latitude 50.6°N, temperature data for Klein-Altendorf can be overlaid for further analysis.

```{r, warning=FALSE}
temperatures <- read_tab("data/TMaxTMin1958-2019_patched.csv") %>%
  filter(Month %in% month_range) %>%
  mutate(Month_names =
           factor(Month,
                  levels = c(10, 11, 12, 1, 2, 3),
                  labels = c("October", "November", "December",
                             "January", "February", "March")))

temperatures[which(temperatures$Tmax < temperatures$Tmin),
             c("Tmax", "Tmin")] <- NA

DM_sensitivity +
  geom_point(data = temperatures,
             aes(x = Tmin,
                 y = Tmax,
                 fill = NULL,
                 color = "Temperature"),
             size = 0.2) +
  facet_wrap(vars(Month_names)) +
  scale_color_manual(values = "black",
                     labels = "Daily temperature \nextremes (°C)",
                     name = "Observed at site" ) +
  guides(fill = guide_colorbar(order = 1),
         color = guide_legend(order = 2)) +
  ylab("Tmax (°C)") +
  xlab("Tmin (°C)") + 
  theme_bw(base_size = 15) 
```

Before analyzing the results in detail, the process will be automated to generate data for additional locations where PLS results have been examined. This requires two functions: one for computing model sensitivity data, as this step is time-intensive, and another for plotting the results. This approach ensures consistency and efficiency in comparing temperature response patterns across different locations.

```{r}
Chill_model_sensitivity <-
  function(latitude,
           temp_models = list(Dynamic_Model = Dynamic_Model,
                              GDH = GDH),
           month_range = c(10, 11, 12, 1, 2, 3),
           Tmins = c(-10:20),
           Tmaxs = c(-5:30))
  {
  mins <- NA
  maxs <- NA
  metrics <- as.list(rep(NA,
                         length(temp_models)))
  names(metrics) <- names(temp_models)
  month <- NA
 
  for(mon in month_range)
    {
    days_month <-
      as.numeric(difftime(ISOdate(2002,
                                  mon + 1,
                                  1),
                          ISOdate(2002,
                                  mon,
                                  1) ))
    if(mon == 12) days_month <- 31
    weather <- 
      make_all_day_table(data.frame(Year = c(2001, 2001),
                                    Month = c(mon, mon),
                                    Day = c(1, days_month),
                                    Tmin = c(0, 0),
                                    Tmax = c(0, 0)))

    
    for(tmin in Tmins)
      for(tmax in Tmaxs)
        if(tmax >= tmin)
          {
          hourtemps <- weather %>%
            mutate(Tmin = tmin,
                   Tmax = tmax) %>%
            stack_hourly_temps(
              latitude = latitude) %>%
            pluck("hourtemps",
                  "Temp")
          
          for(tm in 1:length(temp_models))
            metrics[[tm]] <- 
              c(metrics[[tm]],
                tail(do.call(temp_models[[tm]],
                        list(hourtemps)),1)/
                              days_month)
          
          mins <- c(mins, tmin)
          maxs <- c(maxs, tmax)
          month <- c(month, mon)
        }
    }
  results <- cbind(data.frame(Month = month,
                              Tmin = mins,
                              Tmax = maxs),
                   as.data.frame(metrics))
  
  results <- results[!is.na(results$Month),]
}


Chill_sensitivity_temps <-
  function(chill_model_sensitivity_table,
           temperatures,
           temp_model,
           month_range = c(10, 11, 12, 1, 2, 3),
           Tmins = c(-10:20),
           Tmaxs = c(-5:30),
           legend_label = "Chill/day (CP)")
{
  library(ggplot2)
  library(colorRamps)

  cmst <- chill_model_sensitivity_table
  cmst <- cmst[which(cmst$Month %in% month_range),]
  cmst$Month_names <- factor(cmst$Month,
                             levels = month_range,
                             labels = month.name[month_range])  
  
  DM_sensitivity<-
    ggplot(cmst,
           aes_string(x = "Tmin",
                      y = "Tmax",
                      fill = temp_model)) +
    geom_tile() +
    scale_fill_gradientn(colours = alpha(matlab.like(15),
                                         alpha = .5),
                         name = legend_label) +
    xlim(Tmins[1],
         Tmins[length(Tmins)]) +
    ylim(Tmaxs[1],
         Tmaxs[length(Tmaxs)])
  
  temperatures<-
    temperatures[which(temperatures$Month %in% month_range),]
  
  temperatures[which(temperatures$Tmax < temperatures$Tmin),
               c("Tmax", 
                 "Tmin")] <- NA
  
  temperatures$Month_names <-
    factor(temperatures$Month,
           levels = month_range,
           labels = month.name[month_range])  
  
  DM_sensitivity +
    geom_point(data = temperatures,
               aes(x = Tmin,
                   y = Tmax,
                   fill = NULL,
                   color = "Temperature"),
               size = 0.2) +
    facet_wrap(vars(Month_names)) +
    scale_color_manual(values = "black",
                       labels = "Daily temperature \nextremes (°C)",
                       name = "Observed at site" ) +
    guides(fill = guide_colorbar(order = 1),
           color = guide_legend(order = 2)) +
    ylab("Tmax (°C)") +
    xlab("Tmin (°C)") + 
    theme_bw(base_size = 15)

}
  
```

To compare different locations, the model sensitivity data will first be generated for each site. These datasets will be saved to a file to avoid redundant computations, ensuring efficiency by allowing reuse of the data without reprocessing. This will facilitate a direct comparison of temperature response patterns across locations.

```{r, eval=FALSE}

Model_sensitivities_CKA <-
  Chill_model_sensitivity(latitude = 50,
                          temp_models = list(Dynamic_Model = Dynamic_Model,
                                             GDH = GDH),
                          month_range = c(10:12, 1:5))

write.csv(Model_sensitivities_CKA,
          "data/Model_sensitivities_CKA.csv",
          row.names = FALSE)

Model_sensitivities_Davis <-
  Chill_model_sensitivity(latitude = 38.5,
                          temp_models = list(Dynamic_Model = Dynamic_Model,
                                             GDH = GDH),
                          month_range=c(10:12, 1:5))

write.csv(Model_sensitivities_Davis,
          "data/Model_sensitivities_Davis.csv",
          row.names = FALSE)

Model_sensitivities_Beijing <-
  Chill_model_sensitivity(latitude = 39.9,
                          temp_models = list(Dynamic_Model = Dynamic_Model, 
                                             GDH = GDH),
                          month_range = c(10:12, 1:5))

write.csv(Model_sensitivities_Beijing,
          "data/Model_sensitivities_Beijing.csv",
          row.names = FALSE)

Model_sensitivities_Sfax <-
  Chill_model_sensitivity(latitude = 35,
                          temp_models = list(Dynamic_Model = Dynamic_Model,
                                             GDH = GDH),
                          month_range = c(10:12, 1:5))

write.csv(Model_sensitivities_Sfax,
          "data/Model_sensitivities_Sfax.csv",
          row.names = FALSE)

```

```{r, echo=FALSE}
Model_sensitivities_CKA <- read.csv("data/Model_sensitivities_CKA.csv")
Model_sensitivities_Davis <- read.csv("data/Model_sensitivities_Davis.csv")
Model_sensitivities_Beijing <- read.csv("data/Model_sensitivities_Beijing.csv")
Model_sensitivities_Sfax <- read.csv("data/Model_sensitivities_Sfax.csv")
```

Now, temperature patterns for specific months can be compared to the effective ranges for chill and heat accumulation at the four locations. By analyzing long-term weather data, it becomes possible to assess how well each site's climate aligns with the conditions necessary for dormancy processes.

```{r, echo=FALSE, warning=FALSE}
Beijing_weather <- read_tab("data/Beijing_weather.csv")
CKA_temperatures <- read_tab("data/TMaxTMin1958-2019_patched.csv")
Davis_weather <- read_tab("data/Davis_weather.csv")
Sfax_weather <- read_tab("data/Sfax_weather.csv")
```

To proceed, the long-term weather data for the four locations should be downloaded and saved in a `data` subfolder within the working directory.

```{r, echo=FALSE}
library(downloadthis)
Beijing_weather %>% download_this(
    output_name = "Beijing_weather",
    output_extension = ".csv",
    button_label = "Download weather data for Beijing",
    button_type = "warning",
    has_icon = TRUE,
    icon = "fa fa-save"
  )

CKA_temperatures %>% download_this(
    output_name = "TMaxTMin1958-2019_patched",
    output_extension = ".csv",
    button_label = "Download weather data for Klein-Altendorf",
    button_type = "warning",
    has_icon = TRUE,
    icon = "fa fa-save"
  )

Davis_weather %>% download_this(
    output_name = "Davis_weather",
    output_extension = ".csv",
    button_label = "Download weather data for Davis",
    button_type = "warning",
    has_icon = TRUE,
    icon = "fa fa-save"
  )

Sfax_weather %>% download_this(
    output_name = "Sfax_weather",
    output_extension = ".csv",
    button_label = "Download weather data for Sfax",
    button_type = "warning",
    has_icon = TRUE,
    icon = "fa fa-save"
  )
```

The analysis will begin with Beijing, the coldest location, and then progress through Klein-Altendorf, Davis, and Sfax, representing increasingly warmer growing regions. This step-by-step approach allows for a clear comparison of chill accumulation across different climates.

Since `Chill_sensitivity_temps` outputs a `ggplot` object, it can be modified using the `+`notation in ggplot2. Titles will be added to each plot using the `ggtitle` function to improve clarity. Below are the generated plots for chill accumulation, quantified using the Dynamic Model, illustrating how effective temperature ranges vary across these locations.

```{r, warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_Beijing,
                        Beijing_weather,
                        temp_model = "Dynamic_Model",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        legend_label = "Chill per day \n(Chill Portions)") +
  ggtitle("Chill model sensitivity at Beijing, China")

```

```{r warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_CKA,
                        CKA_temperatures,
                        temp_model = "Dynamic_Model",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        legend_label = "Chill per day \n(Chill Portions)") +
  ggtitle("Chill model sensitivity at Klein-Altendorf, Germany")
```

```{r warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_Davis,
                        Davis_weather,
                        temp_model = "Dynamic_Model",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        legend_label = "Chill per day \n(Chill Portions)") +
  ggtitle("Chill model sensitivity at Davis, California")
```

```{r warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_Sfax,
                        Sfax_weather,
                        temp_model = "Dynamic_Model",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        legend_label = "Chill per day \n(Chill Portions)") +
  ggtitle("Chill model sensitivity near Sfax, Tunisia")
```

Below are the plots for heat accumulation:

```{r, warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_Beijing,
                        Beijing_weather,
                        temp_model = "GDH",
                        month_range = c(12, 1:5),
                        legend_label = "Heat per day \n(GDH)") +
  ggtitle("Heat model sensitivity at Beijing, China")
```

```{r warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_CKA,
                        CKA_temperatures,
                        temp_model = "GDH",
                        month_range = c(12, 1:5),
                        legend_label = "Heat per day \n(GDH)") +
  ggtitle("Heat model sensitivity at Klein-Altendorf, Germany")
```

```{r warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_Davis,
                        Davis_weather,
                        temp_model = "GDH",
                        month_range = c(12, 1:5),
                        legend_label = "Heat per day \n(GDH)") +
  ggtitle("Heat model sensitivity at Davis, California")
```

```{r warning=FALSE}
Chill_sensitivity_temps(Model_sensitivities_Sfax,
                        Sfax_weather,
                        temp_model = "GDH",
                        month_range = c(12, 1:5),
                        legend_label = "Heat per day \n(GDH)") +
  ggtitle("Heat model sensitivity near Sfax, Tunisia")
```

Now, the response patterns will be analyzed in relation to the sensitivity of the chill model. This will help identify how different locations align with the effective temperature ranges for chill accumulation and how variations in climate influence dormancy processes.

## Chill model sensitivity vs. observed temperature

Although chill model sensitivity is influenced by hourly temperatures, which depend on sunrise and sunset times, the overall sensitivity patterns appear nearly identical across all locations. While minor differences exist, they are not significant enough to impact the broader interpretation.

What does vary, however, is how observed hourly temperatures align with the sensitive periods for chill accumulation. This alignment shifts not only between locations but also across different months, highlighting the importance of local climatic conditions in determining the effectiveness of chill accumulation.

### Beijing, China

In Beijing, the relationship between observed temperatures and effective chill accumulation varies significantly across months:

-   **October:** Temperatures are evenly distributed between effective chill accumulation conditions and values that are too warm. This variation may generate **chill-related signals**, potentially influencing **bloom dates** in a detectable way.

-   **November:** Almost all temperatures fall within the highly effective chill accumulation range. Despite considerable temperature fluctuations, chill accumulation rates remain similar for most days. Without meaningful variation, PLS regression is unlikely to produce useful results.

-   **December–February:** Temperatures drop further, leading to many hours below the effective chill accumulation range. If the Dynamic Model correctly represents chill accumulation at low temperatures (which remains uncertain), a response might still be observed.

-   **March:** Similar to **November**, temperatures remain almost always optimal for chill accumulation, resulting in limited variation.

**Summary:**

Beijing's conditions should allow for clear identification of chill effects in most months, except for November, where the lack of variation may reduce the effectiveness of PLS regression.

### Klein-Altendorf, Germany

Compared to Beijing, Klein-Altendorf appears to be less suitable for using PLS regression to delineate the chilling period. While both locations experience a mix of optimal and suboptimal temperatures, the proportion of days with suboptimal chill conditions is much lower in Klein-Altendorf.

-   **October, December, January, February:** Most days have near-optimal chill conditions, with only a few too warm days (mainly in October) or too cold days for effective chill accumulation.

-   **November and March:** Almost all days fall within the optimal chill accumulation range, leading to minimal variation.

Summary:

Due to the lack of strong temperature fluctuations, Klein-Altendorf may not be well-suited for PLS regression analysis, as the method relies on meaningful variation to detect temperature response phases.

### Davis, California

Davis provides more favorable conditions for PLS regression compared to the colder locations, particularly due to the distribution of daily temperatures across chill model sensitivity levels during certain months.

-   **November and March:** Daily temperatures are evenly distributed between effective and ineffective chill accumulation conditions, creating meaningful variation that could help detect temperature response phases.

-   **February:** Most days have optimal chill accumulation conditions, with relatively few days experiencing low or zero chill accumulation.

-   **January and December:** Conditions are mostly optimal for chill accumulation, with little variation, which may limit PLS regression’s ability to detect meaningful signals.

Summary:

While Davis offers better conditions than colder locations for PLS-based analysis, it may still face limitations, particularly in months where chill accumulation remains consistently high, reducing variability in the data.

### Sfax, Tunisia

In Sfax, temperature conditions are generally more favorable for PLS regression, particularly between December and February:

-   **December–February:** Temperatures vary enough to create meaningful chill signals, making these months well-suited for PLS-based analysis.

-   **November and March:** Some chill accumulation occurs, but the proportion of effective chill days is much lower compared to peak winter months.

-   **October:** Temperatures remain consistently too high for chill accumulation, meaning no significant chill-related signal is expected.

Summary:

Among the four locations, Sfax presents the best conditions for PLS regression, especially in mid-winter. However, October shows no variation, and November/March may produce only weak signals, limiting their usefulness in identifying temperature response phases.

## Heat model sensitivity vs. observed temperature

Compared to the Dynamic Model, the Growing Degree Hours (GDH) model exhibits a much simpler temperature response. Any day with a minimum temperature above 4°C contributes to heat accumulation, with warmer temperatures leading to a stronger response.

-   Beijing (December–May): Heat accumulation days are rare in December, January, and February, but become frequent from March onward.

-   Other locations: Heat accumulation conditions are met in all months, making it easier to detect response phases.

Across the four sites, there are few months where PLS regression would struggle to identify heat accumulation phases. This likely explains why PLS regression has been more effective at detecting ecodormancy (heat accumulation) phases than endodormancy (chill accumulation) periods, where temperature conditions are often more stable and less variable.

## `Exercises` on expected PLS responsiveness

1.  *Produce chill and heat model sensitivity plots for the location you focused on in previous exercises.*

```{r}
# Generate model sensitivity data
Model_sensitivities_Yakima <-
  Chill_model_sensitivity(latitude = 46.6,
                          temp_models = list(Dynamic_Model = Dynamic_Model,
                                             GDH = GDH),
                          month_range = c(10:12, 1:5))

write.csv(Model_sensitivities_Sfax,
          "Yakima/Model_sensitivities_Yakima.csv",
          row.names = FALSE)

```

```{r warning=FALSE}
# Chill sensitivity model for Yakima
Yakima_weather <- read.csv("Yakima/Yakima_weather.csv")
Model_sensitivities_Yakima <- read.csv("Yakima/Model_sensitivities_Yakima.csv")

Chill_sensitivity_temps(Model_sensitivities_Yakima,
                        Yakima_weather,
                        temp_model = "Dynamic_Model",
                        month_range = c(10, 11, 12, 1, 2, 3),
                        legend_label = "Chill per day \n(Chill Portions)") +
  ggtitle("Chill model sensitivity at Yakima, USA") 
  
```

```{r warning=FALSE}
# Heat sensitivity model for Yakima

Chill_sensitivity_temps(Model_sensitivities_Yakima,
                        Yakima_weather,
                        temp_model = "GDH",
                        month_range = c(12, 1:5),
                        legend_label = "Heat per day \n(GDH)") +
  ggtitle("Heat model sensitivity at Yakima, USA") 
  
```

<!--chapter:end:25-PLS_not_work.Rmd-->

